{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"apm/","text":"APM Architecture Overview \u00b6 Enabling APM If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed! An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from SignalFx to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to SignalFx and check that you have the APM tab on the top navbar next to Dashboards. SignalFx APM captures end-to-end distributed transactions from your applications, with trace spans sent directly to SignalFx or via the SignalFx Smart Agent deployed on each host (recommended). Optionally, you can deploy an OpenTelemetry Collector to act as a central aggregation point prior to sending trace spans to SignalFx. In addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment. The following illustration shows the recommended deployment model: SignalFx auto-instrumentation libraries send spans to the Smart Agent; the Smart Agent can send the spans to SignalFx directly or via an optional OpenTelemetry Collector.","title":"APM Architecture Overview"},{"location":"apm/#apm-architecture-overview","text":"Enabling APM If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed! An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from SignalFx to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to SignalFx and check that you have the APM tab on the top navbar next to Dashboards. SignalFx APM captures end-to-end distributed transactions from your applications, with trace spans sent directly to SignalFx or via the SignalFx Smart Agent deployed on each host (recommended). Optionally, you can deploy an OpenTelemetry Collector to act as a central aggregation point prior to sending trace spans to SignalFx. In addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment. The following illustration shows the recommended deployment model: SignalFx auto-instrumentation libraries send spans to the Smart Agent; the Smart Agent can send the spans to SignalFx directly or via an optional OpenTelemetry Collector.","title":"APM Architecture Overview"},{"location":"apm/hotrod-original/","text":"Deploying Hot R.O.D. in K3s - Lab Summary \u00b6 Deploy the Hot R.O.D. application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic See APM data in the UI Ensure you have a running instance The setup part is already documented in the Preparation and Deploy the Smart Agent in K3s steps. If you are using an AWS/EC2 instance, make sure it is available and skip to Step 1 , otherwise ensure your Multipass instance is available and running before continuing. Shell Command multipass list Example Output Name State IPv4 Image vmpe-k3s Running 192.168.64.17 Ubuntu 18.04 LTS 1. Deploy the Hot R.O.D. application into K3s \u00b6 To deploy the Hot R.O.D. application into K3s apply the deployment. Shell Command cd ~/workshop sudo kubectl apply -f apm/hotrod/k8s/deployment.yaml Output deployment.apps/hotrod created service/hotrod created To ensure the Hot R.O.D. application is running: Shell Command sudo kubectl get pods Example Output NAME READY STATUS RESTARTS AGE signalfx-agent-mmzxk 1/1 Running 0 110s hotrod-7cc9fc85b7-n765r 1/1 Running 0 41s 2. Viewing the Hot R.O.D. application in your browser \u00b6 AWC/EC2 Users If you are using an AWS/EC2 instance, the application will be available on port 8080 of the EC2 instance's IP address. Open your web browser and go to http:// EC2-IP :8080/ , you will then be able to see the application running. Then continue with the next section on how to Generate Traffic . In order to view the application in your web browser we need to find the LoadBalancer IP address and the port the application is listening on. Shell Command sudo kubectl get svc Example Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 43m hotrod LoadBalancer 10.43.32.97 192.168.64.35 8080:31521/TCP 40m Make note of the EXTERNAL-IP (in the example above this is 192.168.64.35 ). Open your web browser and type in http:// EXTERNAL-IP :8080 , you will then be able to see the application running. Click on customer name to order a car: 3. Generate some traffic to the application using Siege Benchmark \u00b6 Return to your shell and create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(sudo kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') Confirm the environment variable is set Shell Command curl $HOTROD_ENDPOINT Then run the following command(s) to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number. Preparation For a follow up lab, make note of the current time. Shell Command siege -r1 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=391&nonse=0.17041229755366172\" 4. Validating the Hot R.O.D. application in Splunk Observability Suite \u00b6 Open the SignalFx UI, and go to you cluster in the Kubernetes Navigator. You should see the new Pod being started and containers being deployed. Usually it should only take around 20 seconds for the pod to transition into a Running state. When you click on the new pod in the SignalFx UI you should have a cluster that looks like below: If you select the WORKLOADS tab again you should now see that there is a new replica set and a deployment added for hotrod: Next, we want to validate that you are seeing the APM metrics in the UI. For this we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env . To find the hostname, check the prompt of you instance, please go to your instance (Multipass or AWS/EC2) and run the following command. Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example Output Your APM environment is: ip-172-31-30-133-apm-env Now go to Dashboards \u2192 APM \u2192 Service . Please select your environment you found in the previous task then select the frontend service and set time to -15m () No Data in charts if no data is visible, check that you have the right service frontend , and not front-end. To load the dashboard with more data run the following command a few times to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" With this automatically generated dashboard you can keep an eye out for the health of your service, it provides various performance related charts as well as relevant information on the underlying host and Kubernetes platform if applicable. Take some time to explore the various charts in this dashboard 5. Verify that APM traces are reaching SignalFx \u00b6 Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency Map for the Hot R.O.D. application. It should look similar to the screenshot below: Warning If the screen looks very different you may by accident have selected the Previous Generation of APM ( APM PG ) from the menu bar. To rectify this, go back and select the APM tab. The legend at the bottom of the page explains the different visualizations in the Dependency Map. Service requests, error rate and root error rate. Request rate, latency and error rate Also in this view you can see the overall Error and Latency rates over time charts.","title":"Deploying Hot R.O.D. in K3s - Lab Summary"},{"location":"apm/hotrod-original/#deploying-hot-rod-in-k3s-lab-summary","text":"Deploy the Hot R.O.D. application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic See APM data in the UI Ensure you have a running instance The setup part is already documented in the Preparation and Deploy the Smart Agent in K3s steps. If you are using an AWS/EC2 instance, make sure it is available and skip to Step 1 , otherwise ensure your Multipass instance is available and running before continuing. Shell Command multipass list Example Output Name State IPv4 Image vmpe-k3s Running 192.168.64.17 Ubuntu 18.04 LTS","title":"Deploying Hot R.O.D. in K3s - Lab Summary"},{"location":"apm/hotrod-original/#1-deploy-the-hot-rod-application-into-k3s","text":"To deploy the Hot R.O.D. application into K3s apply the deployment. Shell Command cd ~/workshop sudo kubectl apply -f apm/hotrod/k8s/deployment.yaml Output deployment.apps/hotrod created service/hotrod created To ensure the Hot R.O.D. application is running: Shell Command sudo kubectl get pods Example Output NAME READY STATUS RESTARTS AGE signalfx-agent-mmzxk 1/1 Running 0 110s hotrod-7cc9fc85b7-n765r 1/1 Running 0 41s","title":"1. Deploy the Hot R.O.D. application into K3s"},{"location":"apm/hotrod-original/#2-viewing-the-hot-rod-application-in-your-browser","text":"AWC/EC2 Users If you are using an AWS/EC2 instance, the application will be available on port 8080 of the EC2 instance's IP address. Open your web browser and go to http:// EC2-IP :8080/ , you will then be able to see the application running. Then continue with the next section on how to Generate Traffic . In order to view the application in your web browser we need to find the LoadBalancer IP address and the port the application is listening on. Shell Command sudo kubectl get svc Example Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 43m hotrod LoadBalancer 10.43.32.97 192.168.64.35 8080:31521/TCP 40m Make note of the EXTERNAL-IP (in the example above this is 192.168.64.35 ). Open your web browser and type in http:// EXTERNAL-IP :8080 , you will then be able to see the application running. Click on customer name to order a car:","title":"2. Viewing the Hot R.O.D. application in your browser"},{"location":"apm/hotrod-original/#3-generate-some-traffic-to-the-application-using-siege-benchmark","text":"Return to your shell and create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(sudo kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') Confirm the environment variable is set Shell Command curl $HOTROD_ENDPOINT Then run the following command(s) to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number. Preparation For a follow up lab, make note of the current time. Shell Command siege -r1 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=391&nonse=0.17041229755366172\"","title":"3. Generate some traffic to the application using Siege Benchmark"},{"location":"apm/hotrod-original/#4-validating-the-hot-rod-application-in-splunk-observability-suite","text":"Open the SignalFx UI, and go to you cluster in the Kubernetes Navigator. You should see the new Pod being started and containers being deployed. Usually it should only take around 20 seconds for the pod to transition into a Running state. When you click on the new pod in the SignalFx UI you should have a cluster that looks like below: If you select the WORKLOADS tab again you should now see that there is a new replica set and a deployment added for hotrod: Next, we want to validate that you are seeing the APM metrics in the UI. For this we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env . To find the hostname, check the prompt of you instance, please go to your instance (Multipass or AWS/EC2) and run the following command. Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example Output Your APM environment is: ip-172-31-30-133-apm-env Now go to Dashboards \u2192 APM \u2192 Service . Please select your environment you found in the previous task then select the frontend service and set time to -15m () No Data in charts if no data is visible, check that you have the right service frontend , and not front-end. To load the dashboard with more data run the following command a few times to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" With this automatically generated dashboard you can keep an eye out for the health of your service, it provides various performance related charts as well as relevant information on the underlying host and Kubernetes platform if applicable. Take some time to explore the various charts in this dashboard","title":"4. Validating the Hot R.O.D. application in Splunk Observability Suite"},{"location":"apm/hotrod-original/#5-verify-that-apm-traces-are-reaching-signalfx","text":"Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency Map for the Hot R.O.D. application. It should look similar to the screenshot below: Warning If the screen looks very different you may by accident have selected the Previous Generation of APM ( APM PG ) from the menu bar. To rectify this, go back and select the APM tab. The legend at the bottom of the page explains the different visualizations in the Dependency Map. Service requests, error rate and root error rate. Request rate, latency and error rate Also in this view you can see the overall Error and Latency rates over time charts.","title":"5. Verify that APM traces are reaching SignalFx"},{"location":"apm/hotrod-trouble-depend/","text":"uAPM troubleshooting - Hot R.O.D.- Lab Summary \u00b6 Explore troubleshooting and dependencies interface in uAPM Use the Breakdown feature to enrich the troubleshooting info Examine a trace in waterfall mode Ensure you have a running instance that has the hot R.O.D app running The setup part is already documented in the Preparation , Deploy the Smart Agent in K3s and Deploying hot-rod in K3s steps. If you are using an AWS/EC2 instance, make sure it is available and skip to Step 1 , otherwise ensure your Multipass instance is available and running before continuing. Shell Command multipass list Example Output Name State IPv4 Image vmpe-k3s Running 192.168.64.17 Ubuntu 18.04 LTS 1. Find a problematic traces using dependencies and/or tags \u00b6 For this use case we assume there is an intermittent problem but we do not have a clear time frame for the occurrence. This is the very the case when a working on a performance problem. or edge cases, Using the Analytic engine of SignalFx, combined with using tags and/or services selections, we can find relevant traces, and dive into them to explore. 1.1 Select Application Environment \u00b6 First, we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env To find the hostname, check the prompt of you instance, please go to your instance (Multipass or EC2) and run the following command. Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example Multipass Output Your APM environment is: vmpe-apm-env Example AWS/EC2 Output Your APM environment is: ip-172-31-30-133-apm-env Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found and set the time to 1 hour. This will show you the automatically generated Dependency Map for the Hot R.O.D. application. In the scenario we are working with, we have been told that we get intermittent errors in the front end service, but not much more , so we need to discover what is going on. So the first activity for identifying the issue, is to pre-select both the front-end service and follow the dependencies. 2. Explore the troubleshooting and dependencies view \u00b6 3. Use the breakdown feature to enrich troubleshooting info \u00b6","title":"uAPM troubleshooting - Hot R.O.D.- Lab Summary"},{"location":"apm/hotrod-trouble-depend/#uapm-troubleshooting-hot-rod-lab-summary","text":"Explore troubleshooting and dependencies interface in uAPM Use the Breakdown feature to enrich the troubleshooting info Examine a trace in waterfall mode Ensure you have a running instance that has the hot R.O.D app running The setup part is already documented in the Preparation , Deploy the Smart Agent in K3s and Deploying hot-rod in K3s steps. If you are using an AWS/EC2 instance, make sure it is available and skip to Step 1 , otherwise ensure your Multipass instance is available and running before continuing. Shell Command multipass list Example Output Name State IPv4 Image vmpe-k3s Running 192.168.64.17 Ubuntu 18.04 LTS","title":"uAPM troubleshooting - Hot R.O.D.- Lab Summary"},{"location":"apm/hotrod-trouble-depend/#1-find-a-problematic-traces-using-dependencies-andor-tags","text":"For this use case we assume there is an intermittent problem but we do not have a clear time frame for the occurrence. This is the very the case when a working on a performance problem. or edge cases, Using the Analytic engine of SignalFx, combined with using tags and/or services selections, we can find relevant traces, and dive into them to explore.","title":"1. Find a problematic traces using dependencies and/or tags"},{"location":"apm/hotrod-trouble-depend/#11-select-application-environment","text":"First, we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env To find the hostname, check the prompt of you instance, please go to your instance (Multipass or EC2) and run the following command. Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example Multipass Output Your APM environment is: vmpe-apm-env Example AWS/EC2 Output Your APM environment is: ip-172-31-30-133-apm-env Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found and set the time to 1 hour. This will show you the automatically generated Dependency Map for the Hot R.O.D. application. In the scenario we are working with, we have been told that we get intermittent errors in the front end service, but not much more , so we need to discover what is going on. So the first activity for identifying the issue, is to pre-select both the front-end service and follow the dependencies.","title":"1.1 Select Application Environment"},{"location":"apm/hotrod-trouble-depend/#2-explore-the-troubleshooting-and-dependencies-view","text":"","title":"2. Explore the troubleshooting and dependencies view"},{"location":"apm/hotrod-trouble-depend/#3-use-the-breakdown-feature-to-enrich-troubleshooting-info","text":"","title":"3. Use the breakdown feature to enrich troubleshooting info"},{"location":"apm/hotrod-trouble-timebased/","text":"Troubleshooting Hot R.O.D. - Lab Summary \u00b6 Find APM Traces for a specific time period or by service Examine traces in the waterfall Discover the cause of the errors 1. Traces and Spans explained \u00b6 A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services. Each span has a name, representing the operation captured by this span, and a service name, representing within which service the operation took place. Additionally, spans may reference another span as their parent, defining the relationships between the operations captured in the trace that were performed to process that transaction. Each span contains a lot of information about the method, operation, or block of code that it captures, including: the operation name the start time of the operation with microsecond precision how long the operation took to execute, also with microsecond precision the logical name of the service on which the operation took place the IP address of the service instance on which the operation took place 2. Find a specific trace using time slots and/or tags \u00b6 For this use case we assume there is a problem reported with a clear time frame for an error. This is the case when a customer reports problems, for example at 15:05 yesterday, or when a log reports an issue at an exact time. Using the All Traces functionality, combined with using tags and/or services selections, we can find relevant traces, and dive into them to explore. 2.1 Select Application Environment \u00b6 First, we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env To find the hostname, check the prompt of you instance, please go to your instance and run the following command: Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example AWS/EC2 Output Your APM environment is: whul-apm-env Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found and set the time to -15m . This will show you the automatically generated Dependency Map for the Hot R.O.D. application. Please note the Show Traces button at the bottom of the page: 2.2 View traces in the Trace List \u00b6 Now click the Show Traces button at the bottom of the page. This will start loading all the traces view, and if there are more then 25 it will show a selection. Enable Errors Only in the table header as indicated in the screenshot below. In the scenario we are working with, the reported error that we are looking for is seen in the frontend service, when trying to call to the customer service to update customer details. So the first activity for identifying the issue, is to pre-select both those services as a filter for the trace list. This will just show the traces that have both these services in the trace. 2.3 Filtering traces in the Trace List \u00b6 To do this, click on the services drop down at the top of the page and first select the frontend service. Then click on the button to add a new line, then select the customer service. On first review this step has not brought any visual changes, but there was a 3rd item we can use, we can filter on a specific time frame. 2.4 Filtering traces by time \u00b6 To do this, click on the duration drop down (it should show -15m ) and create a Custom Time slot. In the From field you can either enter a date and time stamp or you can set minutes or hours to look back e.g. -10m or -1d Keep the range as short as possible for the best result then click the Apply to search for traces matching the filter in this time frame. To view a trace with an error, click on the blue linked TraceID in the TraceID column This will bring you to the Trace Waterfall view, allowing you to inspect the trace in detail. 3. Examine traces in the waterfall view \u00b6 You should now be in the Trace Waterfall view and your screen should like the view below: The waterfall shows you the trace or route a request/interaction has taken though your application. 4. Exploring the waterfall view \u00b6 As you can see from this trace, we have 3 front end spans, a customer span and a mysql span. The top frontend service has a deep red icon , this indicates that this service has returned with an error. The light red icon on the last frontend span, indicates that the it has received an error from an underlying service. The deep red icon on the customer service indicates that an error originates from that service. Now lets explore the waterfall view. First open the mysql span by clicking on the operation in the span label (SQL SELECT) to see what information is available in the tags that are part of this span. The TAGS section will help you to identify or search further for specific problems. There is information on your environment, your host and Kubernetes related information, you can use to validate the health of theses platforms. You can also see information about the actual SQL query being performed. In this case \"SELECT * FROM customer WHERE customer_id=391\" Close the mysql span by clicking on the Operation in the span label again. Now click on the customer span, this is the span with a deep red icon and lets see if we can find the core of the problem. This will open the span info and look like this: You can see the various actions that are done by this service in the LOGS section. It is trying to load customer 391, and as we can see the request failed due an invalid customer ID.","title":"Troubleshooting Hot R.O.D."},{"location":"apm/hotrod-trouble-timebased/#troubleshooting-hot-rod-lab-summary","text":"Find APM Traces for a specific time period or by service Examine traces in the waterfall Discover the cause of the errors","title":"Troubleshooting Hot R.O.D. - Lab Summary"},{"location":"apm/hotrod-trouble-timebased/#1-traces-and-spans-explained","text":"A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services. Each span has a name, representing the operation captured by this span, and a service name, representing within which service the operation took place. Additionally, spans may reference another span as their parent, defining the relationships between the operations captured in the trace that were performed to process that transaction. Each span contains a lot of information about the method, operation, or block of code that it captures, including: the operation name the start time of the operation with microsecond precision how long the operation took to execute, also with microsecond precision the logical name of the service on which the operation took place the IP address of the service instance on which the operation took place","title":"1. Traces and Spans explained"},{"location":"apm/hotrod-trouble-timebased/#2-find-a-specific-trace-using-time-slots-andor-tags","text":"For this use case we assume there is a problem reported with a clear time frame for an error. This is the case when a customer reports problems, for example at 15:05 yesterday, or when a log reports an issue at an exact time. Using the All Traces functionality, combined with using tags and/or services selections, we can find relevant traces, and dive into them to explore.","title":"2. Find a specific trace using time slots and/or tags"},{"location":"apm/hotrod-trouble-timebased/#21-select-application-environment","text":"First, we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env To find the hostname, check the prompt of you instance, please go to your instance and run the following command: Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example AWS/EC2 Output Your APM environment is: whul-apm-env Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found and set the time to -15m . This will show you the automatically generated Dependency Map for the Hot R.O.D. application. Please note the Show Traces button at the bottom of the page:","title":"2.1 Select Application Environment"},{"location":"apm/hotrod-trouble-timebased/#22-view-traces-in-the-trace-list","text":"Now click the Show Traces button at the bottom of the page. This will start loading all the traces view, and if there are more then 25 it will show a selection. Enable Errors Only in the table header as indicated in the screenshot below. In the scenario we are working with, the reported error that we are looking for is seen in the frontend service, when trying to call to the customer service to update customer details. So the first activity for identifying the issue, is to pre-select both those services as a filter for the trace list. This will just show the traces that have both these services in the trace.","title":"2.2 View traces in the Trace List"},{"location":"apm/hotrod-trouble-timebased/#23-filtering-traces-in-the-trace-list","text":"To do this, click on the services drop down at the top of the page and first select the frontend service. Then click on the button to add a new line, then select the customer service. On first review this step has not brought any visual changes, but there was a 3rd item we can use, we can filter on a specific time frame.","title":"2.3 Filtering traces in the Trace List"},{"location":"apm/hotrod-trouble-timebased/#24-filtering-traces-by-time","text":"To do this, click on the duration drop down (it should show -15m ) and create a Custom Time slot. In the From field you can either enter a date and time stamp or you can set minutes or hours to look back e.g. -10m or -1d Keep the range as short as possible for the best result then click the Apply to search for traces matching the filter in this time frame. To view a trace with an error, click on the blue linked TraceID in the TraceID column This will bring you to the Trace Waterfall view, allowing you to inspect the trace in detail.","title":"2.4 Filtering traces by time"},{"location":"apm/hotrod-trouble-timebased/#3-examine-traces-in-the-waterfall-view","text":"You should now be in the Trace Waterfall view and your screen should like the view below: The waterfall shows you the trace or route a request/interaction has taken though your application.","title":"3. Examine traces in the waterfall view"},{"location":"apm/hotrod-trouble-timebased/#4-exploring-the-waterfall-view","text":"As you can see from this trace, we have 3 front end spans, a customer span and a mysql span. The top frontend service has a deep red icon , this indicates that this service has returned with an error. The light red icon on the last frontend span, indicates that the it has received an error from an underlying service. The deep red icon on the customer service indicates that an error originates from that service. Now lets explore the waterfall view. First open the mysql span by clicking on the operation in the span label (SQL SELECT) to see what information is available in the tags that are part of this span. The TAGS section will help you to identify or search further for specific problems. There is information on your environment, your host and Kubernetes related information, you can use to validate the health of theses platforms. You can also see information about the actual SQL query being performed. In this case \"SELECT * FROM customer WHERE customer_id=391\" Close the mysql span by clicking on the Operation in the span label again. Now click on the customer span, this is the span with a deep red icon and lets see if we can find the core of the problem. This will open the span info and look like this: You can see the various actions that are done by this service in the LOGS section. It is trying to load customer 391, and as we can see the request failed due an invalid customer ID.","title":"4. Exploring the waterfall view"},{"location":"apm/hotrod/","text":"Deploying Hot R.O.D. in K3s - Lab Summary \u00b6 Deploy the Hot R.O.D. application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic See APM data in the UI 1. Deploy the Hot R.O.D. application into K3s \u00b6 To deploy the Hot R.O.D. application into K3s apply the deployment. Shell Command cd ~/workshop sudo kubectl apply -f apm/hotrod/k8s/deployment.yaml Output deployment.apps/hotrod created service/hotrod created To ensure the Hot R.O.D. application is running: Shell Command sudo kubectl get pods Example Output NAME READY STATUS RESTARTS AGE signalfx-agent-mmzxk 1/1 Running 0 110s hotrod-7cc9fc85b7-n765r 1/1 Running 0 41s 2. Viewing the Hot R.O.D. application in your browser \u00b6 The application is viewable on port 8080 of the EC2 instance's IP address. The IP address is the one you used to SSH into the instance at the beginning of the workshop. Open your web browser and go to http:// EC2-IP :8080/ , you will then be able to see the application running. 3. Generate some traffic to the application using Siege Benchmark \u00b6 Return to your shell and create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(sudo kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') Confirm the environment variable is set: Shell Command curl $HOTROD_ENDPOINT Then run the following command(s) to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number: Shell Command siege -r1 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=391&nonse=0.17041229755366172\" 4. Validating the Hot R.O.D. application in SignalFx \u00b6 Open the SignalFx UI, and go to you cluster in the Kubernetes Navigator. You should see the new Pod being started and container being deployed. Usually it should only take around 20 seconds for the pod to transition into a Running state. When you click on the new pod in the SignalFx UI you should have a cluster that looks like below: If you select the WORKLOADS tab again you should now see that there is a new replica set and a deployment added for hotrod: Next, we want to validate that you are seeing the APM metrics in the UI. For this we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env . To find the hostname, on the AWS/EC2 instance run the following command: Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example Output Your APM environment is: whul-apm-env Now go to Dashboards \u2192 APM \u2192 Service . Please select your environment you found in the previous task then select the frontend service and set time to -15m () No Data in charts If no data is visible, check that you have the right service frontend , and not front-end. To load the dashboard with more data run the following command a few times to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" With this automatically generated dashboard you can keep an eye out for the health of your service, it provides various performance related charts as well as relevant information on the underlying host and Kubernetes platform if applicable. Take some time to explore the various charts in this dashboard 5. Verify that APM traces are reaching SignalFx \u00b6 Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency Map for the Hot R.O.D. application. It should look similar to the screenshot below: The legend at the bottom of the page explains the different visualizations in the Dependency Map. Service requests, error rate and root error rate. Request rate, latency and error rate Also in this view you can see the overall Error and Latency rates over time charts.","title":"Deploying Hot R.O.D. in K3s"},{"location":"apm/hotrod/#deploying-hot-rod-in-k3s-lab-summary","text":"Deploy the Hot R.O.D. application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic See APM data in the UI","title":"Deploying Hot R.O.D. in K3s - Lab Summary"},{"location":"apm/hotrod/#1-deploy-the-hot-rod-application-into-k3s","text":"To deploy the Hot R.O.D. application into K3s apply the deployment. Shell Command cd ~/workshop sudo kubectl apply -f apm/hotrod/k8s/deployment.yaml Output deployment.apps/hotrod created service/hotrod created To ensure the Hot R.O.D. application is running: Shell Command sudo kubectl get pods Example Output NAME READY STATUS RESTARTS AGE signalfx-agent-mmzxk 1/1 Running 0 110s hotrod-7cc9fc85b7-n765r 1/1 Running 0 41s","title":"1. Deploy the Hot R.O.D. application into K3s"},{"location":"apm/hotrod/#2-viewing-the-hot-rod-application-in-your-browser","text":"The application is viewable on port 8080 of the EC2 instance's IP address. The IP address is the one you used to SSH into the instance at the beginning of the workshop. Open your web browser and go to http:// EC2-IP :8080/ , you will then be able to see the application running.","title":"2. Viewing the Hot R.O.D. application in your browser"},{"location":"apm/hotrod/#3-generate-some-traffic-to-the-application-using-siege-benchmark","text":"Return to your shell and create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(sudo kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') Confirm the environment variable is set: Shell Command curl $HOTROD_ENDPOINT Then run the following command(s) to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number: Shell Command siege -r1 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=391&nonse=0.17041229755366172\"","title":"3. Generate some traffic to the application using Siege Benchmark"},{"location":"apm/hotrod/#4-validating-the-hot-rod-application-in-signalfx","text":"Open the SignalFx UI, and go to you cluster in the Kubernetes Navigator. You should see the new Pod being started and container being deployed. Usually it should only take around 20 seconds for the pod to transition into a Running state. When you click on the new pod in the SignalFx UI you should have a cluster that looks like below: If you select the WORKLOADS tab again you should now see that there is a new replica set and a deployment added for hotrod: Next, we want to validate that you are seeing the APM metrics in the UI. For this we need to know the name of your application environment. In this workshop all the environments use your hostname -apm-env . To find the hostname, on the AWS/EC2 instance run the following command: Shell Command echo \"Your APM environment is: $(hostname)-apm-env\" Example Output Your APM environment is: whul-apm-env Now go to Dashboards \u2192 APM \u2192 Service . Please select your environment you found in the previous task then select the frontend service and set time to -15m () No Data in charts If no data is visible, check that you have the right service frontend , and not front-end. To load the dashboard with more data run the following command a few times to create load on the service: Shell Command siege -r2 -c20 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" With this automatically generated dashboard you can keep an eye out for the health of your service, it provides various performance related charts as well as relevant information on the underlying host and Kubernetes platform if applicable. Take some time to explore the various charts in this dashboard","title":"4. Validating the Hot R.O.D. application in SignalFx"},{"location":"apm/hotrod/#5-verify-that-apm-traces-are-reaching-signalfx","text":"Open SignalFx in your browser and select the APM tab. Select the Troubleshooting tab, and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency Map for the Hot R.O.D. application. It should look similar to the screenshot below: The legend at the bottom of the page explains the different visualizations in the Dependency Map. Service requests, error rate and root error rate. Request rate, latency and error rate Also in this view you can see the overall Error and Latency rates over time charts.","title":"5. Verify that APM traces are reaching SignalFx"},{"location":"apm/otel/","text":"Open Telemetry Collector \u00b6 The collector is an optional component between the smart agent and SaaS ingest. In this configuration we are using the sapm endpoint to receive traces. 1. Install OpenTelemetry Collector with helm \u00b6 Add the repository with Shell Command helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts Then Shell Command helm install \\ --set standaloneCollector.configOverride.exporters.signalfx.realm=$REALM \\ --set standaloneCollector.configOverride.exporters.signalfx.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.endpoint=https://ingest.$REALM.signalfx.com/v2/trace \\ opentelemetry-collector open-telemetry/opentelemetry-collector \\ -f ~/workshop/otel/collector.yaml 2. Validate the OpenTelemetry Collector installation \u00b6 Review the OpenTelemetry Collector logs: Shell Command sudo kubectl logs -l app.kubernetes.io/name = opentelemetry-collector Look for a log entry with Example Output ... \"msg\":\"Everything is ready. Begin running and processing data.\"} Validate that the service is running and has a sapm endpoint on port 7276. Shell Command kubectl get svc opentelemetry-collector Example Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE opentelemetry-collector ClusterIP 10.43.119.140 <none> 14250/TCP,14268/TCP,55680/TCP, 7276 /TCP,9411/TCP 13m Use the healthcheck endpoint to confirm: Shell Command OTEL_ENDPOINT=$(sudo kubectl get svc opentelemetry-collector -n default -o jsonpath='{.spec.clusterIP}') curl http://$OTEL_ENDPOINT:13133/; echo Example Output {\"status\":\"Server available\",\"upSince\":\"2020-10-22T08:07:33.656859114Z\",\"uptime\":\"8m33.548333561s\"} 3. Reconfigure the agent to use OpenTelemetry Collector \u00b6 We want to send traces in sapm format and point it to the OpenTelemetry Collector trace endpoint. Uninstall the agent: Shell Command helm uninstall signalfx-agent Then reinstall it with traceEndpointUrl set to point to OpenTelemetry Collector and using sapm as trace format: Shell Command helm install \\ --set writer.traceExportFormat=sapm \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$(hostname)-k3s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=http://opentelemetry-collector:7276/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml Check the OpenTelemetry Collector dashboards and validate metrics and spans are being sent.","title":"Open Telemetry Collector"},{"location":"apm/otel/#open-telemetry-collector","text":"The collector is an optional component between the smart agent and SaaS ingest. In this configuration we are using the sapm endpoint to receive traces.","title":"Open Telemetry Collector"},{"location":"apm/otel/#1-install-opentelemetry-collector-with-helm","text":"Add the repository with Shell Command helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts Then Shell Command helm install \\ --set standaloneCollector.configOverride.exporters.signalfx.realm=$REALM \\ --set standaloneCollector.configOverride.exporters.signalfx.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.access_token=$ACCESS_TOKEN \\ --set standaloneCollector.configOverride.exporters.sapm.endpoint=https://ingest.$REALM.signalfx.com/v2/trace \\ opentelemetry-collector open-telemetry/opentelemetry-collector \\ -f ~/workshop/otel/collector.yaml","title":"1. Install OpenTelemetry Collector with helm"},{"location":"apm/otel/#2-validate-the-opentelemetry-collector-installation","text":"Review the OpenTelemetry Collector logs: Shell Command sudo kubectl logs -l app.kubernetes.io/name = opentelemetry-collector Look for a log entry with Example Output ... \"msg\":\"Everything is ready. Begin running and processing data.\"} Validate that the service is running and has a sapm endpoint on port 7276. Shell Command kubectl get svc opentelemetry-collector Example Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE opentelemetry-collector ClusterIP 10.43.119.140 <none> 14250/TCP,14268/TCP,55680/TCP, 7276 /TCP,9411/TCP 13m Use the healthcheck endpoint to confirm: Shell Command OTEL_ENDPOINT=$(sudo kubectl get svc opentelemetry-collector -n default -o jsonpath='{.spec.clusterIP}') curl http://$OTEL_ENDPOINT:13133/; echo Example Output {\"status\":\"Server available\",\"upSince\":\"2020-10-22T08:07:33.656859114Z\",\"uptime\":\"8m33.548333561s\"}","title":"2. Validate the OpenTelemetry Collector installation"},{"location":"apm/otel/#3-reconfigure-the-agent-to-use-opentelemetry-collector","text":"We want to send traces in sapm format and point it to the OpenTelemetry Collector trace endpoint. Uninstall the agent: Shell Command helm uninstall signalfx-agent Then reinstall it with traceEndpointUrl set to point to OpenTelemetry Collector and using sapm as trace format: Shell Command helm install \\ --set writer.traceExportFormat=sapm \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$(hostname)-k3s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=http://opentelemetry-collector:7276/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml Check the OpenTelemetry Collector dashboards and validate metrics and spans are being sent.","title":"3. Reconfigure the agent to use OpenTelemetry Collector"},{"location":"apm/sockshop/","text":"Deploying Sock Shop in K3s \u00b6 Note The setup part is already documented in the Preparation and Deploy the Smart Agent in K3s steps. Please ensure this instance is available and running before continuing. Shell Command multipass list Output Name State IPv4 Image rwc-k3s Running 192.168.64.17 Ubuntu 18.04 LTS Warning The Sock Shop application requires some horse power to run it, please ensure you are running a Multipass or AWS/EC2 instance that can handle it. Sock Shop Multipass min. requirements: 4 vCPU, 15Gb Disk, 8Gb Memory Sock Shop AWS/EC2 min. requirements: t2.large 2 vCPU, 15Gb Disk, 8Gb Memory 1. Deploy the Sock Shop application into K3s \u00b6 To deploy the Sock Shop application into K3s apply the deployment Shell Command cd ~/workshop/apm/sockshop sudo kubectl create namespace sock-shop sudo kubectl apply -f k8s/deployment.yaml Output namespace/sock-shop created deployment.apps/carts-db created service/carts-db created deployment.apps/carts created service/carts created deployment.apps/catalogue-db created service/catalogue-db created deployment.apps/catalogue created service/catalogue created deployment.apps/front-end created service/front-end created deployment.apps/orders-db created service/orders-db created deployment.apps/orders created service/orders created deployment.apps/payment created service/payment created deployment.apps/queue-master created service/queue-master created deployment.apps/rabbitmq created service/rabbitmq created deployment.apps/shipping created service/shipping created deployment.apps/user-db created service/user-db created deployment.apps/user created service/user created 2. Ensure Sock Shop is fully deployed \u00b6 To monitor the deployment of Sock Shop using k9s to monitor: Shell Command k9s Once in k9s press 0 to show all namespaces: 3. Take Sock Shop for a test drive \u00b6 Sock Shop should be running in your cluster and exposes services via cluster IP and port. Obtain the ip address for the front-end service. Shell Command export SOCKS_ENDPOINT=$(sudo kubectl get svc front-end -n sock-shop -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') Then confirm the SOCKS_ENDPOINT environment variable has been set: Shell Command curl http://$SOCKS_ENDPOINT Output ... </ script > </ body > </ html > 4. Viewing the SockShop application in your browser \u00b6 (If you are using an AWS/EC2 instance, please skip to this section.) To view the application in your web browser we need to find the LoadBalancer IP address and the port the application is listening on. Shell Command sudo kubectl get svc -n sock-shop front-end Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE front-end LoadBalancer 10.43.247.97 192.168.64.35 8081:30001/TCP 34m Make note of the EXTERNAL-IP (in the example above this is 192.168.64.35 ). Then head over to your web browser and type in http://[EXTERNAL-IP]:8081 , you should then be able to see the application running. Happy Shopping! 5. Apply load on Sock Shop \u00b6 A load testing scenario is available for the Sock Shop application. To generate some load run the following command: Shell Command ./loadgen.sh -c 50 -r 3m The parameter -c controls the amount of concurrent clients and -r the runtime of the load test. To apply continuous load set -r to the desired runtime. The load test runs as a job in the K8S cluster. Observe the progress: Shell Command sudo kubectl -n sock-shop logs -f jobs/loadgen If you want to abort a load test, delete the job: Shell Command sudo kubectl -n sock-shop delete jobs/loadgen 6. Visualize and analyze trace data \u00b6 Navigate to APM ( not APM PG) and select Monitoring, then ensure you have selected your environment from the dropdown at the top, you should see something like this: 6.1. Explore the User Interface \u00b6 Review an automatically generated Service Dashboard. How do you correlate Service performance with Infrastructure? 6.2. Troubleshoot a service \u00b6 Let's stress the sock shop a bit. Increase the amount of clients running for the load test to something ludicrous, e.g.: Shell Command ./loadgen.sh -c 1000 -a 100 -r 5m While the load test is running observe in SignalFx what happens with the services. Troubleshoot a service with a higher error rate. Also review the service dependencies. Look at individual traces and span performance.","title":"Deploying Sock Shop in K3s"},{"location":"apm/sockshop/#deploying-sock-shop-in-k3s","text":"Note The setup part is already documented in the Preparation and Deploy the Smart Agent in K3s steps. Please ensure this instance is available and running before continuing. Shell Command multipass list Output Name State IPv4 Image rwc-k3s Running 192.168.64.17 Ubuntu 18.04 LTS Warning The Sock Shop application requires some horse power to run it, please ensure you are running a Multipass or AWS/EC2 instance that can handle it. Sock Shop Multipass min. requirements: 4 vCPU, 15Gb Disk, 8Gb Memory Sock Shop AWS/EC2 min. requirements: t2.large 2 vCPU, 15Gb Disk, 8Gb Memory","title":"Deploying Sock Shop in K3s"},{"location":"apm/sockshop/#1-deploy-the-sock-shop-application-into-k3s","text":"To deploy the Sock Shop application into K3s apply the deployment Shell Command cd ~/workshop/apm/sockshop sudo kubectl create namespace sock-shop sudo kubectl apply -f k8s/deployment.yaml Output namespace/sock-shop created deployment.apps/carts-db created service/carts-db created deployment.apps/carts created service/carts created deployment.apps/catalogue-db created service/catalogue-db created deployment.apps/catalogue created service/catalogue created deployment.apps/front-end created service/front-end created deployment.apps/orders-db created service/orders-db created deployment.apps/orders created service/orders created deployment.apps/payment created service/payment created deployment.apps/queue-master created service/queue-master created deployment.apps/rabbitmq created service/rabbitmq created deployment.apps/shipping created service/shipping created deployment.apps/user-db created service/user-db created deployment.apps/user created service/user created","title":"1. Deploy the Sock Shop application into K3s"},{"location":"apm/sockshop/#2-ensure-sock-shop-is-fully-deployed","text":"To monitor the deployment of Sock Shop using k9s to monitor: Shell Command k9s Once in k9s press 0 to show all namespaces:","title":"2. Ensure Sock Shop is fully deployed"},{"location":"apm/sockshop/#3-take-sock-shop-for-a-test-drive","text":"Sock Shop should be running in your cluster and exposes services via cluster IP and port. Obtain the ip address for the front-end service. Shell Command export SOCKS_ENDPOINT=$(sudo kubectl get svc front-end -n sock-shop -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') Then confirm the SOCKS_ENDPOINT environment variable has been set: Shell Command curl http://$SOCKS_ENDPOINT Output ... </ script > </ body > </ html >","title":"3. Take Sock Shop for a test drive"},{"location":"apm/sockshop/#4-viewing-the-sockshop-application-in-your-browser","text":"(If you are using an AWS/EC2 instance, please skip to this section.) To view the application in your web browser we need to find the LoadBalancer IP address and the port the application is listening on. Shell Command sudo kubectl get svc -n sock-shop front-end Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE front-end LoadBalancer 10.43.247.97 192.168.64.35 8081:30001/TCP 34m Make note of the EXTERNAL-IP (in the example above this is 192.168.64.35 ). Then head over to your web browser and type in http://[EXTERNAL-IP]:8081 , you should then be able to see the application running. Happy Shopping!","title":"4. Viewing the SockShop application in your browser"},{"location":"apm/sockshop/#5-apply-load-on-sock-shop","text":"A load testing scenario is available for the Sock Shop application. To generate some load run the following command: Shell Command ./loadgen.sh -c 50 -r 3m The parameter -c controls the amount of concurrent clients and -r the runtime of the load test. To apply continuous load set -r to the desired runtime. The load test runs as a job in the K8S cluster. Observe the progress: Shell Command sudo kubectl -n sock-shop logs -f jobs/loadgen If you want to abort a load test, delete the job: Shell Command sudo kubectl -n sock-shop delete jobs/loadgen","title":"5. Apply load on Sock Shop"},{"location":"apm/sockshop/#6-visualize-and-analyze-trace-data","text":"Navigate to APM ( not APM PG) and select Monitoring, then ensure you have selected your environment from the dropdown at the top, you should see something like this:","title":"6. Visualize and analyze trace data"},{"location":"apm/sockshop/#61-explore-the-user-interface","text":"Review an automatically generated Service Dashboard. How do you correlate Service performance with Infrastructure?","title":"6.1. Explore the User Interface"},{"location":"apm/sockshop/#62-troubleshoot-a-service","text":"Let's stress the sock shop a bit. Increase the amount of clients running for the load test to something ludicrous, e.g.: Shell Command ./loadgen.sh -c 1000 -a 100 -r 5m While the load test is running observe in SignalFx what happens with the services. Troubleshoot a service with a higher error rate. Also review the service dependencies. Look at individual traces and span performance.","title":"6.2. Troubleshoot a service"},{"location":"dashboards/","text":"Working with Dashboards, Charts and Metrics \u00b6 Introduction to the Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Introduction to SignalFlow 1. Introduction to the Observability Suite UI \u00b6 Logon to the Organization you have been invited to. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of prebuilt dashboards are provided for you in your default view. If you are already receiving metrics through a Cloud API integration or the Smart Agent you will see relevant dashboards for these services. Among the dashboards you will see a Dashboard group called Sample Data . Let's take a closer look at it! 2. Inspecting the Sample Data \u00b6 In this dashboard view expand the Sample Data dashboard group by clicking on it, and then click on the Intro to SignalFx dashboard. You will see a selection of sample charts. To learn more about charts you can click on the other sample dashboards ( PART 1 , PART 2 and PART 3 ). Let's take a look at the Sample charts. Click on the SAMPLE CHARTS dashboard name. In the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards.","title":"Introduction"},{"location":"dashboards/#working-with-dashboards-charts-and-metrics","text":"Introduction to the Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Introduction to SignalFlow","title":"Working with Dashboards, Charts and Metrics"},{"location":"dashboards/#1-introduction-to-the-observability-suite-ui","text":"Logon to the Organization you have been invited to. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of prebuilt dashboards are provided for you in your default view. If you are already receiving metrics through a Cloud API integration or the Smart Agent you will see relevant dashboards for these services. Among the dashboards you will see a Dashboard group called Sample Data . Let's take a closer look at it!","title":"1. Introduction to the Observability Suite UI"},{"location":"dashboards/#2-inspecting-the-sample-data","text":"In this dashboard view expand the Sample Data dashboard group by clicking on it, and then click on the Intro to SignalFx dashboard. You will see a selection of sample charts. To learn more about charts you can click on the other sample dashboards ( PART 1 , PART 2 and PART 3 ). Let's take a look at the Sample charts. Click on the SAMPLE CHARTS dashboard name. In the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards.","title":"2. Inspecting the Sample Data"},{"location":"dashboards/adding-charts/","text":"Adding charts to dashboards \u00b6 Let's now save our chart. Click on Save as... and enter a name for your chart; use your initials like [YOUR INITIALS] Latency Chart and click OK . In the next window, find your email address in the list and select it, then click Ok . You will immediately be transported to the dashboard created under your selected group (make sure the group name on the top left is your email address). Last but not least, change the dashboard's name, by clicking the ... icon on the top right and selecting Rename . Enter a new name for your dashboard and click on Done . Congratulations! You have created your first chart and dashboard!","title":"Adding charts"},{"location":"dashboards/adding-charts/#adding-charts-to-dashboards","text":"Let's now save our chart. Click on Save as... and enter a name for your chart; use your initials like [YOUR INITIALS] Latency Chart and click OK . In the next window, find your email address in the list and select it, then click Ok . You will immediately be transported to the dashboard created under your selected group (make sure the group name on the top left is your email address). Last but not least, change the dashboard's name, by clicking the ... icon on the top right and selecting Rename . Enter a new name for your dashboard and click on Done . Congratulations! You have created your first chart and dashboard!","title":"Adding charts to dashboards"},{"location":"dashboards/datalinks/","text":"Integration with Splunk \u00b6 1. Introduction to Data links \u00b6 Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards. 2. Configuring an integration with Splunk \u00b6 Informational exercise only This module is purely informational and serves only to educate how to link from SignalFx into Splunk Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Splunk. For the Link to use the dropdown and select Splunk . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The URL value will be the your Splunk instance. The Minimum Time Window will be the time interval between values assigned to start_time and end_time variables and is equal to either this window or, when linking from a data table, the chart resolution if that is wider. For Property mapping this allows you define mappings between terms found in SignalFx and the external system. So, for example, SignalFx might have a property set called container_id but the external system uses container . 3. Using Data Links \u00b6 Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . Note Please note that because we configured Any Metadata value not all Data Links will yield results as there might not be corresponding logs for the property value.","title":"Integration to Splunk"},{"location":"dashboards/datalinks/#integration-with-splunk","text":"","title":"Integration with Splunk"},{"location":"dashboards/datalinks/#1-introduction-to-data-links","text":"Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards.","title":"1. Introduction to Data links"},{"location":"dashboards/datalinks/#2-configuring-an-integration-with-splunk","text":"Informational exercise only This module is purely informational and serves only to educate how to link from SignalFx into Splunk Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Splunk. For the Link to use the dropdown and select Splunk . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The URL value will be the your Splunk instance. The Minimum Time Window will be the time interval between values assigned to start_time and end_time variables and is equal to either this window or, when linking from a data table, the chart resolution if that is wider. For Property mapping this allows you define mappings between terms found in SignalFx and the external system. So, for example, SignalFx might have a property set called container_id but the external system uses container .","title":"2. Configuring an integration with Splunk"},{"location":"dashboards/datalinks/#3-using-data-links","text":"Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . Note Please note that because we configured Any Metadata value not all Data Links will yield results as there might not be corresponding logs for the property value.","title":"3. Using Data Links"},{"location":"dashboards/editing/","text":"Editing charts \u00b6 1. Edit Histogram Chart \u00b6 Click on the three dots ... on the Latency histogram chart in the Sample Data dashboard and then on Open (or you can click on the name of the chart which here is Latency histogram ). You will see the plot options, current plot and signal (metric) for the Latency histogram chart. Click on the different chart type icons to explore each of the visualizations. Notice their name while you click on or swipe over them. See how the chart changes. Note You can use different ways to visualize your metrics - you choose which chart type fits best for the visualization you want to have. For more info on the different chart types see Choosing a chart type . Click on the Line chart type and you will see the line plot. In the Plot Editor tab under Signal you see the metric demo.trans.latency we are currently plotting. 2. Creating a new chart \u00b6 Let's now create a new chart and save it in a new dashboard! Click on the plus icon (top right of the UI) and from the drop down, click on Chart . You will now see a chart template like the following. Let's enter a metric to plot. We are going to use the metric demo.trans.latency . In the Plot Editor tab under Signal enter demo.trans.latency . You will instantly see a number of Line plots, like below. The number 18 ts indicates that we are plotting 18 metric time series in the chart. Click on the DATA TABLE tab. You see now 18 rows, each representing a metics time series with a number of columns. If you swipe over the plot horizontally you will see the metrics in these columns at different times. In the demo_datacenter column you see that there are two data centers, Paris and Tokyo , for which we are getting metrics.","title":"Editing charts"},{"location":"dashboards/editing/#editing-charts","text":"","title":"Editing charts"},{"location":"dashboards/editing/#1-edit-histogram-chart","text":"Click on the three dots ... on the Latency histogram chart in the Sample Data dashboard and then on Open (or you can click on the name of the chart which here is Latency histogram ). You will see the plot options, current plot and signal (metric) for the Latency histogram chart. Click on the different chart type icons to explore each of the visualizations. Notice their name while you click on or swipe over them. See how the chart changes. Note You can use different ways to visualize your metrics - you choose which chart type fits best for the visualization you want to have. For more info on the different chart types see Choosing a chart type . Click on the Line chart type and you will see the line plot. In the Plot Editor tab under Signal you see the metric demo.trans.latency we are currently plotting.","title":"1. Edit Histogram Chart"},{"location":"dashboards/editing/#2-creating-a-new-chart","text":"Let's now create a new chart and save it in a new dashboard! Click on the plus icon (top right of the UI) and from the drop down, click on Chart . You will now see a chart template like the following. Let's enter a metric to plot. We are going to use the metric demo.trans.latency . In the Plot Editor tab under Signal enter demo.trans.latency . You will instantly see a number of Line plots, like below. The number 18 ts indicates that we are plotting 18 metric time series in the chart. Click on the DATA TABLE tab. You see now 18 rows, each representing a metics time series with a number of columns. If you swipe over the plot horizontally you will see the metrics in these columns at different times. In the demo_datacenter column you see that there are two data centers, Paris and Tokyo , for which we are getting metrics.","title":"2. Creating a new chart"},{"location":"dashboards/filtering/","text":"Using Filters \u00b6 1. Filtering and Analytics \u00b6 Let's now select the Paris datacenter to do some analytics - for that we will use a filter. Let's go back to the Plot Editor tab and click on Add Filter , wait until it automatically populates, choose demo_datacenter , and then Paris . In the F(x) column, add the analytic function Percentile:Aggregation , and leave the value to 95 (click outside to confirm). For info on the Percentile function and the other functions see Analytics reference . 2. Using Timeshift analytical function \u00b6 Let's now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A . You will see a new row identical to A , called B , both visible and plotted. For Signal B , in the F(x) column add the analytic function Timeshift and enter 7d (7 days = 1 week), and click outside to confirm. Click on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B . Click on Close . Next, click into the field next to Time on the Override bar and choose Past Day from the dropdown. We now see plots for Signal A (the last day) as a blue plot, and 7 days ago in pink. In order to make this clearer we can click on the Area chart icon to change the visualization. We now have a better view of our two plots!","title":"Using filters"},{"location":"dashboards/filtering/#using-filters","text":"","title":"Using Filters"},{"location":"dashboards/filtering/#1-filtering-and-analytics","text":"Let's now select the Paris datacenter to do some analytics - for that we will use a filter. Let's go back to the Plot Editor tab and click on Add Filter , wait until it automatically populates, choose demo_datacenter , and then Paris . In the F(x) column, add the analytic function Percentile:Aggregation , and leave the value to 95 (click outside to confirm). For info on the Percentile function and the other functions see Analytics reference .","title":"1. Filtering and Analytics"},{"location":"dashboards/filtering/#2-using-timeshift-analytical-function","text":"Let's now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A . You will see a new row identical to A , called B , both visible and plotted. For Signal B , in the F(x) column add the analytic function Timeshift and enter 7d (7 days = 1 week), and click outside to confirm. Click on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B . Click on Close . Next, click into the field next to Time on the Override bar and choose Past Day from the dropdown. We now see plots for Signal A (the last day) as a blue plot, and 7 days ago in pink. In order to make this clearer we can click on the Area chart icon to change the visualization. We now have a better view of our two plots!","title":"2. Using Timeshift analytical function"},{"location":"dashboards/formulas/","text":"Using Formulas \u00b6 1. Plotting differences \u00b6 Let's now plot the difference of all metric values for a day with 7 days in between. Click on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C . We now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time. Click on DATA TABLE and in that view swipe horizontally along the X axis to see the metric values at different times. 2. Using Absolute Value \u00b6 Click on Plot Editor to get back to the Plot Editor view. Let's apply another function to get the values of C to positive values. Note By doing so we will see the difference between the metric values for a period of 24 hours with 7 days between. This difference can be used to see an alarming trend if we consider last week to be a baseline (the bigger the number - the more we deviate from the baseline) - but mainly we do this for you to get a bit more training on using analytical functions! In the Plot Editor for C , under F(x) , click on Add Analytics and choose Absolute Value . You will see the C plot now having only positive values.","title":"Using formulas"},{"location":"dashboards/formulas/#using-formulas","text":"","title":"Using Formulas"},{"location":"dashboards/formulas/#1-plotting-differences","text":"Let's now plot the difference of all metric values for a day with 7 days in between. Click on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C . We now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time. Click on DATA TABLE and in that view swipe horizontally along the X axis to see the metric values at different times.","title":"1. Plotting differences"},{"location":"dashboards/formulas/#2-using-absolute-value","text":"Click on Plot Editor to get back to the Plot Editor view. Let's apply another function to get the values of C to positive values. Note By doing so we will see the difference between the metric values for a period of 24 hours with 7 days between. This difference can be used to see an alarming trend if we consider last week to be a baseline (the bigger the number - the more we deviate from the baseline) - but mainly we do this for you to get a bit more training on using analytical functions! In the Plot Editor for C , under F(x) , click on Add Analytics and choose Absolute Value . You will see the C plot now having only positive values.","title":"2. Using Absolute Value"},{"location":"dashboards/overlay/","text":"Using Overlays \u00b6 Let's overlay metrics and events to our initial plot to see if there is any correlation with high latency. To discover and add new metrics to the chart from the ones that are being sent to SignalFx already, click on Browse as highlighted in the screenshot below. In the METRICS sidebar on the right, enter demo and click on the search icon to search. Observe that the Find Metrics option is pre-selected. The metrics search is showing 3 metrics with demo in the name. Select demo.trans.count and click on the Add Plot button. Click on the blue eye icon next to C to hide that Signal, and on the greyed eye icon for Signal A to show it. On plot D , apply the Percentile:Aggregation function and set to 95 . Click on Delta Rollup and from the Rollup dropdown select Rate/Sec . Enter -1h in the Time frame for the entire chart. We see that there is a correlation between latency and number of transactions. Note Likewise we could check Find Events and find events like deployment events etc. to correlate with. Click on the the greater than sign icon to collapse the METRICS sidebar.","title":"Using overlays"},{"location":"dashboards/overlay/#using-overlays","text":"Let's overlay metrics and events to our initial plot to see if there is any correlation with high latency. To discover and add new metrics to the chart from the ones that are being sent to SignalFx already, click on Browse as highlighted in the screenshot below. In the METRICS sidebar on the right, enter demo and click on the search icon to search. Observe that the Find Metrics option is pre-selected. The metrics search is showing 3 metrics with demo in the name. Select demo.trans.count and click on the Add Plot button. Click on the blue eye icon next to C to hide that Signal, and on the greyed eye icon for Signal A to show it. On plot D , apply the Percentile:Aggregation function and set to 95 . Click on Delta Rollup and from the Rollup dropdown select Rate/Sec . Enter -1h in the Time frame for the entire chart. We see that there is a correlation between latency and number of transactions. Note Likewise we could check Find Events and find events like deployment events etc. to correlate with. Click on the the greater than sign icon to collapse the METRICS sidebar.","title":"Using Overlays"},{"location":"dashboards/signalflow/","text":"SignalFlow \u00b6 Let's take a look at SignalFlow - the analytics language of SignalFx that can be used to setup monitoring as code. Click on View SignalFlow . You will see the SignalFlow code that composes the chart we were working on. SignalFlow A = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . publish ( label = 'A' ) B = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . timeshift ( '1w' ) . publish ( label = 'B' , enable = False ) C = ( A - B ) . abs () . publish ( label = 'C' , enable = False ) D = data ( 'demo.trans.count' ) . percentile ( pct = 95 ) . publish ( label = 'D' ) SignalFlow is the analytics language of SignalFx. Among other benefits, it can be used to setup monitoring as code. For more info on SignalFlow see Getting started with SignalFlow . Click on View Builder to go back to the Chart Builder UI.","title":"SignalFlow"},{"location":"dashboards/signalflow/#signalflow","text":"Let's take a look at SignalFlow - the analytics language of SignalFx that can be used to setup monitoring as code. Click on View SignalFlow . You will see the SignalFlow code that composes the chart we were working on. SignalFlow A = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . publish ( label = 'A' ) B = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . timeshift ( '1w' ) . publish ( label = 'B' , enable = False ) C = ( A - B ) . abs () . publish ( label = 'C' , enable = False ) D = data ( 'demo.trans.count' ) . percentile ( pct = 95 ) . publish ( label = 'D' ) SignalFlow is the analytics language of SignalFx. Among other benefits, it can be used to setup monitoring as code. For more info on SignalFlow see Getting started with SignalFlow . Click on View Builder to go back to the Chart Builder UI.","title":"SignalFlow"},{"location":"detectors/detectors/","text":"Working with Detectors - Lab Summary \u00b6 Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction \u00b6 A detector monitors a signal for conditions or issues that you care about. Those conditions or issues are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical. For example, a detector that monitors the latency of an API call may go into a critical state when the latency is significantly higher than normal, as defined in the detector rules. 2. Create a detector from one of your charts \u00b6 In Dashboards click on your dashboard group (the one with your email address) and then on the dashboard name where the chart you created in the previous lab resides or search for your previously created dashboard's name, and click on that dashboard's name in the results. We are now going to create a new detector from this chart. Once you see the chart, click on the bell icon on your chart and then on New Detector From Chart . In the text field next to Detector Name , ADD YOUR INITIALS before the proposed detector name. Naming the detector It's important that you add your initials in front of the proposed detector name. It should be something like this: LI's Latency Chart Detector . Click on Create Alert Rule In the Detector window, inside Alert signal , the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert. Click on Proceed to Alert Condition 3. Setting Alert condition \u00b6 In Alert condition , click on Static Threshold and then on Proceed to Alert Settings In Alert Settings , enter the value 290 in the Threshold field. In the same window change Time on top right to past day ( -1d ). 4. Alert pre-flight check \u00b6 A pre-flight check will take place after 5 seconds. See the Estimated alert count . Based on the current alert settings, the amount of alerts we would\u2019ve received in 1 day would have been approx. 18 . About pre-flight checks Once you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day. Immediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Suite. To read more about detector previewing, please visit this link Setting up detectors . Click on Proceed to Alert Message 5. Configuring the alert message \u00b6 In Alert message , under Severity choose Major . Click on Proceed to Alert Recipients Click on Add Recipient and then on your email address displayed as the first option. Notification Services That's the same as entering that email address OR you can enter another email address by clicking on E-mail... . This is just one example of the many Notification Services the suite has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services . 6. Activating the alert \u00b6 Click on Proceed to Alert Activation In Activate... click on Activate Alert Rule If you want to get alerts quicker you can click back on Alert Settings and lower the value from 290 to say 280 . If you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metics from the last 1 hour. Hover over Alerts in the top menu and then click on Detectors . You will see you detector listed here. If you don't then please refresh your browser. Congratulations ! You have created your first detector and activated it!","title":"Creating a Detector"},{"location":"detectors/detectors/#working-with-detectors-lab-summary","text":"Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules","title":"Working with Detectors - Lab Summary"},{"location":"detectors/detectors/#1-introduction","text":"A detector monitors a signal for conditions or issues that you care about. Those conditions or issues are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical. For example, a detector that monitors the latency of an API call may go into a critical state when the latency is significantly higher than normal, as defined in the detector rules.","title":"1. Introduction"},{"location":"detectors/detectors/#2-create-a-detector-from-one-of-your-charts","text":"In Dashboards click on your dashboard group (the one with your email address) and then on the dashboard name where the chart you created in the previous lab resides or search for your previously created dashboard's name, and click on that dashboard's name in the results. We are now going to create a new detector from this chart. Once you see the chart, click on the bell icon on your chart and then on New Detector From Chart . In the text field next to Detector Name , ADD YOUR INITIALS before the proposed detector name. Naming the detector It's important that you add your initials in front of the proposed detector name. It should be something like this: LI's Latency Chart Detector . Click on Create Alert Rule In the Detector window, inside Alert signal , the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert. Click on Proceed to Alert Condition","title":"2. Create a detector from one of your charts"},{"location":"detectors/detectors/#3-setting-alert-condition","text":"In Alert condition , click on Static Threshold and then on Proceed to Alert Settings In Alert Settings , enter the value 290 in the Threshold field. In the same window change Time on top right to past day ( -1d ).","title":"3. Setting Alert condition"},{"location":"detectors/detectors/#4-alert-pre-flight-check","text":"A pre-flight check will take place after 5 seconds. See the Estimated alert count . Based on the current alert settings, the amount of alerts we would\u2019ve received in 1 day would have been approx. 18 . About pre-flight checks Once you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day. Immediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Suite. To read more about detector previewing, please visit this link Setting up detectors . Click on Proceed to Alert Message","title":"4. Alert pre-flight check"},{"location":"detectors/detectors/#5-configuring-the-alert-message","text":"In Alert message , under Severity choose Major . Click on Proceed to Alert Recipients Click on Add Recipient and then on your email address displayed as the first option. Notification Services That's the same as entering that email address OR you can enter another email address by clicking on E-mail... . This is just one example of the many Notification Services the suite has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services .","title":"5. Configuring the alert message"},{"location":"detectors/detectors/#6-activating-the-alert","text":"Click on Proceed to Alert Activation In Activate... click on Activate Alert Rule If you want to get alerts quicker you can click back on Alert Settings and lower the value from 290 to say 280 . If you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metics from the last 1 hour. Hover over Alerts in the top menu and then click on Detectors . You will see you detector listed here. If you don't then please refresh your browser. Congratulations ! You have created your first detector and activated it!","title":"6. Activating the alert"},{"location":"detectors/muting/","text":"Working with Muting Rules - Lab Summary \u00b6 Learn how to configure how to mute Alerts 1. Learn how to configure muting your alerts \u00b6 There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in SignalFx. Let's create one! Hover over Alerts in the menu and from the drop down click on Detectors . You will see a list of active detectors. If you created an detector in Working with Detectors you can click on the three dots ... on the far right for that detector; if not, do that for another detector. From the drop-down click on Create Muting Rule... . In the Muting Rule window check Mute Indefinitely and enter a reason. Note This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector. Click Next and in the new modal window confirm the muting rule setup. Click on Mute Indefinitely to confirm. You won't be receiving any email notifications from you detector until you resume notifications again. Let's now see how to do that! 2. Resuming notifications \u00b6 To Resume notifications, hover over Alerts in the top menu and click on Muting Rules . You will see the name of the detector you muted notifications for under Detector . Click on the thee dots ... on the far right. Click on Resume Notifications . Click on Resume to confirm and resume notifications for this detector. Congratulations! You have now resumed your alert notifications!","title":"Creating a Muting Rule"},{"location":"detectors/muting/#working-with-muting-rules-lab-summary","text":"Learn how to configure how to mute Alerts","title":"Working with Muting Rules - Lab Summary"},{"location":"detectors/muting/#1-learn-how-to-configure-muting-your-alerts","text":"There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in SignalFx. Let's create one! Hover over Alerts in the menu and from the drop down click on Detectors . You will see a list of active detectors. If you created an detector in Working with Detectors you can click on the three dots ... on the far right for that detector; if not, do that for another detector. From the drop-down click on Create Muting Rule... . In the Muting Rule window check Mute Indefinitely and enter a reason. Note This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector. Click Next and in the new modal window confirm the muting rule setup. Click on Mute Indefinitely to confirm. You won't be receiving any email notifications from you detector until you resume notifications again. Let's now see how to do that!","title":"1. Learn how to configure muting your alerts"},{"location":"detectors/muting/#2-resuming-notifications","text":"To Resume notifications, hover over Alerts in the top menu and click on Muting Rules . You will see the name of the detector you muted notifications for under Detector . Click on the thee dots ... on the far right. Click on Resume Notifications . Click on Resume to confirm and resume notifications for this detector. Congratulations! You have now resumed your alert notifications!","title":"2. Resuming notifications"},{"location":"lambda/","text":"APM with AWS Lambda (Developer Focused) \u00b6 Enabling APM If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed! An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from Splunk's Observability team to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to Splunk's observability suite and check that you have the APM tab on the top navbar next to Dashboards. 1. AWS Lambda exercise & APM overview \u00b6 This workshop section is focused on developer's of serverless/Lambda application/functions. This workshop is going to guide them through the steps to add Tracing to Python and Node-Js Lambda Functions, and see traces flow from an on-prem Java application though the various Python and Node-Js Lambda Functions in Splunk APM. Splunk APM captures end-to-end distributed transactions from your applications, including serverless apps (Lambda's) with trace spans sent directly to Splunk or via an optional OpenTelemetry Collector that act as a central aggregation point prior to sending trace spans to Splunk. (recommended, and show in the workshop). In addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment. The following illustration shows the recommended deployment model: Splunk's auto-instrumentation libraries send spans to the OpenTelemetry Collector; the OpenTelemetry Collector forwards the spans to SignalFx. 2. AWS Lambda exercise requirements flow \u00b6 During this workshop you will perform the following activities: Perform a test run of the Splunk Mobile Phone Web shop and its services Enable Tracing on local SpringBoot App Enable Tracing on First Python & Node-js Lambda Functions Enable Tracing on the other functions Enrich the spans Look at configuring options for the OpenTelemetry Collector 3. AWS Lambda exercise requirements \u00b6 This workshop section assumes that you have access to the following features as they are required for the workshop: AWS account: Access to EC2 Instances Ability to create/run AWS Lambda's","title":"APM with AWS Lambda (Developer Focused)"},{"location":"lambda/#apm-with-aws-lambda-developer-focused","text":"Enabling APM If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed! An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from Splunk's Observability team to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to Splunk's observability suite and check that you have the APM tab on the top navbar next to Dashboards.","title":"APM with AWS Lambda (Developer Focused)"},{"location":"lambda/#1-aws-lambda-exercise-apm-overview","text":"This workshop section is focused on developer's of serverless/Lambda application/functions. This workshop is going to guide them through the steps to add Tracing to Python and Node-Js Lambda Functions, and see traces flow from an on-prem Java application though the various Python and Node-Js Lambda Functions in Splunk APM. Splunk APM captures end-to-end distributed transactions from your applications, including serverless apps (Lambda's) with trace spans sent directly to Splunk or via an optional OpenTelemetry Collector that act as a central aggregation point prior to sending trace spans to Splunk. (recommended, and show in the workshop). In addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment. The following illustration shows the recommended deployment model: Splunk's auto-instrumentation libraries send spans to the OpenTelemetry Collector; the OpenTelemetry Collector forwards the spans to SignalFx.","title":"1. AWS Lambda exercise &amp; APM overview"},{"location":"lambda/#2-aws-lambda-exercise-requirements-flow","text":"During this workshop you will perform the following activities: Perform a test run of the Splunk Mobile Phone Web shop and its services Enable Tracing on local SpringBoot App Enable Tracing on First Python & Node-js Lambda Functions Enable Tracing on the other functions Enrich the spans Look at configuring options for the OpenTelemetry Collector","title":"2. AWS Lambda exercise requirements flow"},{"location":"lambda/#3-aws-lambda-exercise-requirements","text":"This workshop section assumes that you have access to the following features as they are required for the workshop: AWS account: Access to EC2 Instances Ability to create/run AWS Lambda's","title":"3. AWS Lambda exercise requirements"},{"location":"lambda/initial_run_env/","text":"Initial run of Splunk Mobile Shop Application \u00b6 The goal of this session is to make you familiar with the various components that are used in the workshop. Required information You should have the following information at hand as you will need this in various places throughout the workshop Access to the AWS account that is used to setup the workshop The region the Workshop is using to run the workshop (i.e. Ireland, Frankfurt, Ohio, Tokio ..) A unique UID string allowing you to identify your services IP address & password of the EC2 instance assigned to you 1. Validate availability of Lambda Functions \u00b6 Please log into the AWS account that has been used to create the workshop and select the region that is used by the workshop - the Splunk SE or your organisations lead will confirm this at the start of the Workshop. In the example below the Region is Frankfurt, but we may be using a different one for this Workshop. Once the Lambda service have been selected you should see a list of available Lambda Functions. To find the ones that are assigned to you, use the filter option with the the Unique ID you have been allocated, you should have something similar to: UID _RetailOrder UID _RetailOrderLine UID _RetailOrderPrice UID _RetailOrderDiscount (Where UID represents the Unique ID you have been provided - ACME in our example) 2. Verify CloudWatch Logs Location \u00b6 To investigate issues that may occur during the run, we need to check the CloudWatch logs. Open CloudWatch Log Groups in a new browser tab so you can easily switch between the Lambda Functions and the Logs. The activity to pick in CloudWatch is the Log Group section. If there are logs present, you can filter on your preset like before. The result should be that there are no logs visible (see below). If there are logs, even after filtering on your preset, make sure they are not related to the 4 service above, or delete them if possible). Once you have deleted the logs, the filter option will be disabled until there are new logs. 3. Connect to your EC2 instance \u00b6 Next open a Terminal window and log into the EC2 instance you have been assigned. Note If you need help with this, here are the instructions how to access you pre-configured AWS/EC2 instance . If you need help with this, here are the instructions on how to access you pre-configured AWS/EC2 instance . (If you can, open a second ssh window to you EC2 instance, as this will be useful later in the workshop) Please return here after you have successfully connected to your instance. Once connected move into the correct directory to run the Java SpringBoot application by running the following command within your instances shell session: Shell Command cd ~/SplunkLambdaAPM/MobileShop/Base From here we will start the Java SpringBoot application that contains our simple web shop application. Run the application by issuing the following command: Shell Command mvn spring-boot:run On the first run SpringBoot will download a lot of packages like in the image below, be patience! The next runs will be much faster. but as soon as everything is loaded, you should see the SpringBoot logo. We are now ready to test the app. 4. Test the Splunk Mobile Phone Shop App \u00b6 Open another new browser tab and navigate to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) URL http://[ec2_ip]:8080/order To test your environment enter the following information: Name of a phone: for example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver And hit submit to run a test though your system. The result should be similar to this : You can use The Submit another order to go back and reorder a new phone! 5. Verify Logs in CloudWatch \u00b6 Even if there are no errors and you have the above result, check the Cloudwatch logs to verify they have been created. Go to the CloudWatch tab you opened in step 2 and refresh, confirm that all four logs have been generated. Assuming you have all four logs groups listed, you are now ready to add Traces and Spans, however if there are not four logs listed then we have a problem, so please bring this to the attention of the Splunk SE running the Workshop.","title":"Initial run of Splunk Mobile Shop Application"},{"location":"lambda/initial_run_env/#initial-run-of-splunk-mobile-shop-application","text":"The goal of this session is to make you familiar with the various components that are used in the workshop. Required information You should have the following information at hand as you will need this in various places throughout the workshop Access to the AWS account that is used to setup the workshop The region the Workshop is using to run the workshop (i.e. Ireland, Frankfurt, Ohio, Tokio ..) A unique UID string allowing you to identify your services IP address & password of the EC2 instance assigned to you","title":"Initial run of Splunk Mobile Shop Application"},{"location":"lambda/initial_run_env/#1-validate-availability-of-lambda-functions","text":"Please log into the AWS account that has been used to create the workshop and select the region that is used by the workshop - the Splunk SE or your organisations lead will confirm this at the start of the Workshop. In the example below the Region is Frankfurt, but we may be using a different one for this Workshop. Once the Lambda service have been selected you should see a list of available Lambda Functions. To find the ones that are assigned to you, use the filter option with the the Unique ID you have been allocated, you should have something similar to: UID _RetailOrder UID _RetailOrderLine UID _RetailOrderPrice UID _RetailOrderDiscount (Where UID represents the Unique ID you have been provided - ACME in our example)","title":"1. Validate availability of Lambda Functions"},{"location":"lambda/initial_run_env/#2-verify-cloudwatch-logs-location","text":"To investigate issues that may occur during the run, we need to check the CloudWatch logs. Open CloudWatch Log Groups in a new browser tab so you can easily switch between the Lambda Functions and the Logs. The activity to pick in CloudWatch is the Log Group section. If there are logs present, you can filter on your preset like before. The result should be that there are no logs visible (see below). If there are logs, even after filtering on your preset, make sure they are not related to the 4 service above, or delete them if possible). Once you have deleted the logs, the filter option will be disabled until there are new logs.","title":"2. Verify CloudWatch Logs Location"},{"location":"lambda/initial_run_env/#3-connect-to-your-ec2-instance","text":"Next open a Terminal window and log into the EC2 instance you have been assigned. Note If you need help with this, here are the instructions how to access you pre-configured AWS/EC2 instance . If you need help with this, here are the instructions on how to access you pre-configured AWS/EC2 instance . (If you can, open a second ssh window to you EC2 instance, as this will be useful later in the workshop) Please return here after you have successfully connected to your instance. Once connected move into the correct directory to run the Java SpringBoot application by running the following command within your instances shell session: Shell Command cd ~/SplunkLambdaAPM/MobileShop/Base From here we will start the Java SpringBoot application that contains our simple web shop application. Run the application by issuing the following command: Shell Command mvn spring-boot:run On the first run SpringBoot will download a lot of packages like in the image below, be patience! The next runs will be much faster. but as soon as everything is loaded, you should see the SpringBoot logo. We are now ready to test the app.","title":"3. Connect to your EC2 instance"},{"location":"lambda/initial_run_env/#4-test-the-splunk-mobile-phone-shop-app","text":"Open another new browser tab and navigate to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) URL http://[ec2_ip]:8080/order To test your environment enter the following information: Name of a phone: for example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver And hit submit to run a test though your system. The result should be similar to this : You can use The Submit another order to go back and reorder a new phone!","title":"4. Test the Splunk Mobile Phone Shop App"},{"location":"lambda/initial_run_env/#5-verify-logs-in-cloudwatch","text":"Even if there are no errors and you have the above result, check the Cloudwatch logs to verify they have been created. Go to the CloudWatch tab you opened in step 2 and refresh, confirm that all four logs have been generated. Assuming you have all four logs groups listed, you are now ready to add Traces and Spans, however if there are not four logs listed then we have a problem, so please bring this to the attention of the Splunk SE running the Workshop.","title":"5. Verify Logs in CloudWatch"},{"location":"lambda/retail-order-I/","text":"Enable APM in the RetailOrder Function \u00b6 1. Edit UID_RetailOrder Lambda in your AWS environment \u00b6 To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: Now select the Lambda Function UID _RetailOrder to open the browser based editor environment for Lambda functions. Now scroll down so you have the full editor visible. To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow two lines at the top of the file and add an empty line. RetailOrder Updates import signalfx_lambda import opentracing Add the following two lines above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrder Updates @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() Verify that the top of the file now looks like this: RetailOrder Updates import signalfx_lambda import opentracing import os import json import boto3 import requests # The Environment Tag is used by Splunk APM to filter Environments in UI APM_ENVIRONMENT = os.environ['SIGNALFX_APM_ENVIRONMENT'] PRICE_URL = os.environ['PRICE_URL'] ORDER_LINE = os.environ['ORDER_LINE'] # Define the client to interact with AWS Lambda client = boto3.client('lambda') @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() def lambda_handler(event,context): print(\"event received :\", event) To save your work, you must press on the Deploy button above the editor as show here. 3. Run a case and find both the Service Dashboard for your lambda function & your trace \u00b6 Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this: 3.1 Find your Service Dashboard for the RetailOrder Lambda Function in Splunk APM \u00b6 Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Lambda Function will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes. 3.1 Look at trace info in splunk APM \u00b6 Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application.","title":"Enable APM in the RetailOrder Function"},{"location":"lambda/retail-order-I/#enable-apm-in-the-retailorder-function","text":"","title":"Enable APM in the RetailOrder Function"},{"location":"lambda/retail-order-I/#1-edit-uid_retailorder-lambda-in-your-aws-environment","text":"To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: Now select the Lambda Function UID _RetailOrder to open the browser based editor environment for Lambda functions. Now scroll down so you have the full editor visible. To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow two lines at the top of the file and add an empty line. RetailOrder Updates import signalfx_lambda import opentracing Add the following two lines above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrder Updates @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() Verify that the top of the file now looks like this: RetailOrder Updates import signalfx_lambda import opentracing import os import json import boto3 import requests # The Environment Tag is used by Splunk APM to filter Environments in UI APM_ENVIRONMENT = os.environ['SIGNALFX_APM_ENVIRONMENT'] PRICE_URL = os.environ['PRICE_URL'] ORDER_LINE = os.environ['ORDER_LINE'] # Define the client to interact with AWS Lambda client = boto3.client('lambda') @signalfx_lambda.emits_metrics() @signalfx_lambda.is_traced() def lambda_handler(event,context): print(\"event received :\", event) To save your work, you must press on the Deploy button above the editor as show here.","title":"1. Edit UID_RetailOrder Lambda in your AWS environment"},{"location":"lambda/retail-order-I/#3-run-a-case-and-find-both-the-service-dashboard-for-your-lambda-function-your-trace","text":"Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this:","title":"3. Run a case and find both the Service Dashboard for your lambda function &amp; your trace"},{"location":"lambda/retail-order-I/#31-find-your-service-dashboard-for-the-retailorder-lambda-function-in-splunk-apm","text":"Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Lambda Function will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes.","title":"3.1 Find your Service Dashboard for the RetailOrder Lambda Function in Splunk APM"},{"location":"lambda/retail-order-I/#31-look-at-trace-info-in-splunk-apm","text":"Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application.","title":"3.1 Look at trace info in splunk APM"},{"location":"lambda/retail-orderline-I/","text":"Enable APM in the RetailOrderLine Function \u00b6 1. Edit UID_RetailOderLine Lambda in your AWS Environment \u00b6 To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: INSERT SCREENSHOT Now select the Lambda Function UID _RetailOrderLine to open the browser based editor environment for Lambda functions. INSERT SCREENSHOT Enable the Full Screen Editor and then close the 'Execution Result' tab to give you a clean working area INSERT SCREENSHOT To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow lines at the top of the file and add an empty line. RetailOrderLine Updates import signalfx_lambda import opentracing from opentracing.ext import tags from opentracing.propagation import Format Add the following line above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrderLine Updates @signalfx_lambda.is_traced(with_span=False) Verify that the file now looks like this: INSERT SCREENSHOT","title":"Enable APM in the RetailOrderLine Function"},{"location":"lambda/retail-orderline-I/#enable-apm-in-the-retailorderline-function","text":"","title":"Enable APM in the RetailOrderLine Function"},{"location":"lambda/retail-orderline-I/#1-edit-uid_retailoderline-lambda-in-your-aws-environment","text":"To add APM to our lambda function, you need to got to the browser tab with your lambda functions selected in the first exercise, or follow the original Validate Lambda Functions instructions. Once filtered with your UID you should have something like this: INSERT SCREENSHOT Now select the Lambda Function UID _RetailOrderLine to open the browser based editor environment for Lambda functions. INSERT SCREENSHOT Enable the Full Screen Editor and then close the 'Execution Result' tab to give you a clean working area INSERT SCREENSHOT To enable APM, we need to import the Splunk SignalFx Lambda wrapper. We already included this for you, but you can find the details on how to set this up yourself here Secondly we also import the opentracing library, we will use this later to customize the span information we send to Splunk's APM. Add the follow lines at the top of the file and add an empty line. RetailOrderLine Updates import signalfx_lambda import opentracing from opentracing.ext import tags from opentracing.propagation import Format Add the following line above the line stating def lambda_handler(event,context): This will enable the wrapper. RetailOrderLine Updates @signalfx_lambda.is_traced(with_span=False) Verify that the file now looks like this: INSERT SCREENSHOT","title":"1. Edit UID_RetailOderLine Lambda in your AWS Environment"},{"location":"lambda/setup/","text":"Workshop Setup \u00b6 This setup module is used to prepare a set of AWS Lambda Functions and EC2 Instances to be used for the workshop, these will need to be created by an AWS Admin who has access to a suitable account that can be used for the workshop. There will be 4 Lambda Functions and 1 EC2 instance deployed for each participant. Every resource will be prefixed with a Unique ID (UID) to identify which attendee they belong to. Terraform is used to deploy all of the resources and this module details the steps required to install and configure terraform, configure it to authenticate with AWS, and then deploy the resources. 1. Install Terraform \u00b6 Terraform is used to deploy all of the AWS infrastructure for the workshop, so needs to be installed on your machine. Instructions on how to install Terraform can be found here . Terraform AWS Authentication \u00b6 Once Terraform is installed, you need to configure it to authenticate with your AWS Account. Details on AWS Authentication can be found here . The AWS Authentication consists of two files, config and credentials which are typically located in the ~/.ssh folder. The config file details different profiles, and works in conjunction with the credentials file which contains your access and secret keys. config [default] region = us-east-1 output = json [profile splunk] region = us-east-1 output = json authentication [default] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key} [splunk] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key} 2. Clone Workshop Content - WILL NEED TO UPDATE URL ONCE PUBLISHED TO SPLUNK REPO \u00b6 The Workshop Content needs to be pulled down from Github to your local machine, then updated with your specific settings for AWS Authentication. 2.1 Download the Workshop Repo \u00b6 Git Clone git clone git clone https://github.com/geoffhigginbottom/tflambdatestv2.git example result Workshop git clone https://github.com/geoffhigginbottom/tflambdatestv2.git cloning into 'tflambdatestv2'... remote: Enumerating objects: 276, done. remote: Counting objects: 100% (276/276), done. remote: Compressing objects: 100% (186/186), done. remote: Total 276 (delta 159), reused 206 (delta 89), pack-reused 0 Receiving objects: 100% (276/276), 269.48 KiB | 1.13 MiB/s, done. Resolving deltas: 100% (159/159), done. 2.2 Create terraform.tfvars \u00b6 A file called terraform.tfvars needs to be created and populated with your specific settings. This file contains all of the settings required to enable Terraform to connect to both your AWS and Splunk Environments. An example version of the file is included in the repo named terraform.tfvars.example , which you should copy and rename to terraform.tfvars . Run the following command from within the directory where the workshop content was download. crete terraform.tfvars cp terraform.tfvars.example terraform.tfvars Then update the newly created terraform.tfvars starting with the AWS Variables Section. 2.2.1 AWS Variables Section \u00b6 profile should match the profile name used in your aws authentication file which Terraform will use to authenticate with AWS key_name is the name of the ssh_key you wish to use to access the EC2 instances (note password login is also enabled on the Instances) private_key_path is the path to your private ssh key, such as ~/.ssh/xxx.pem or ~/.ssh/id_rsa instance_type is the AWS Instance Type used for the EC2 Instances - this defaults to the free tier \"t2.micro\" region is an optional parameter, normally left commented out. It enables you to override the region prompt during the Terraform deployment terraform.tfvars - AWS Variables ### AWS Variables ### profile = \"xxx\" key_name = \"yyy\" private_key_path = \"~/.ssh/xxx.pem or ~/.ssh/id_rsa etc\" instance_type = \"t2.micro\" ## Terraform will prompt you to select an AWS Region to deploy the resources into ## Setting a region here removes the region prompt (default is to have it prompt you) ## List of supported regions can be found in the variables.tf file #region = \"2\" 2.2.2 Splunk Variables Section \u00b6 function_version is an optional parameter, normally left commented out. It enables you to override the version prompt during the Terraform deployment access_token is the token you wish to use to authenticate with the Splunk Monitoring backend realm specifies which Realm your Splunk Monitoring backend is deployed in collector_image specifies the contributor version of the otel collector, and the latest version can be found here and will need updating as new versions are released terraform.tfvars - Splunk Variables ### Splunk Variables ### ## Terraform will prompt you to select a version to be deployed, ## \"a\" = apm version, \"b\" = base version. ## Base version is typically deployed, but the apm version can be deployed ## for testing and comparison purposes #function_version = \"b\" access_token = \"xxxxxxxxxx\" realm = \"xxx\" # smart_agent_version = \"5.6.0-1\" # Optional - If left blank, latest will be installed smart_agent_version = \"\" # Optional - If left blank, latest will be installed ## Latest otel collector releases can be found at ## https://github.com/open-telemetry/opentelemetry-collector-contrib/releases collector_image = \"otel/opentelemetry-collector-contrib:0.15.0\" 2.3 Generate the Unique IDs \u00b6 A file named quantity.auto.tfvars needs to be created and populated with your specific settings. This file contains the Unique IDs which will be appended to each AWS Resource to identify which workshop participant they are allocated to. There is an example file in the repo called quantity.auto.tfvars.example which needs to be copied and renamed to quantity.auto.tfvars . Run the following command from within the directory where the workshop content was download. crete quantity.auto.tfvars cp quantity.auto.tfvars.example quantity.auto.tfvars Edit quantity.auto.tfvars and populate the list of participants, ensuring each value is unique and has no spaces. Ensure the function_count value equals the total number of names, and that each entry ends with a comma, apart from the last one, as per the example below. quantity.auto.tfvars function_count = \"3\" function_ids = [ \"John\", \"Sarah\", \"Amir\" ] 3. Initialize Terraform \u00b6 Once you have finished creating and updating xxx and yyy you now need to initialize terraform, so run the following command: Terraform init teraform init Example output Initializing the backend... Initializing provider plugins... - Finding latest version of terraform-providers/docker... - Finding latest version of hashicorp/null... - Finding latest version of hashicorp/archive... - Finding latest version of hashicorp/aws... - Installing hashicorp/archive v2.0.0... - Installed hashicorp/archive v2.0.0 (signed by HashiCorp) - Installing hashicorp/aws v3.18.0... - Installed hashicorp/aws v3.18.0 (signed by HashiCorp) - Installing terraform-providers/docker v2.7.2... - Installed terraform-providers/docker v2.7.2 (signed by HashiCorp) - Installing hashicorp/null v3.0.0... - Installed hashicorp/null v3.0.0 (signed by HashiCorp) The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, we recommend adding version constraints in a required_providers block in your configuration, with the constraint strings suggested below. * hashicorp/archive: version = \"~> 2.0.0\" * hashicorp/aws: version = \"~> 3.18.0\" * hashicorp/null: version = \"~> 3.0.0\" * terraform-providers/docker: version = \"~> 2.7.2\" Warning: Additional provider information from registry The remote registry returned warnings for registry.terraform.io/terraform-providers/docker: - For users on Terraform 0.13 or greater, this provider has moved to kreuzwerker/docker. Please update your source in required_providers. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. 4. Deploy the Workshop \u00b6 4.1 Terraform Plan \u00b6 You can now deploy the workshop using Terraform. It is always best practice to run a plan so you can check what changes Terraform is going to make. When executing Terraform will prompt you to select a version of the workshop (select b for base), and also an AWS Region (choose an appropriate one from the list) Terraform plan terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Example output terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.aws_ami.latest-ubuntu: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create <= read (data resources) Terraform will perform the following actions: # data.archive_file.retailorder_lambda_zip will be read during apply # (config refers to values not yet known) <= data \"archive_file\" \"retailorder_lambda_zip\" { + id = (known after apply) + output_base64sha256 = (known after apply) + output_md5 = (known after apply) + output_path = \"retailorder_lambda.zip\" + output_sha = (known after apply) + output_size = (known after apply) + source_file = \"retailorder_lambda_function.py\" + type = \"zip\" } ........ EXTRA LINES REMOVED ........ Plan: 106 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. 4.3 Terraform Apply \u00b6 After checking the plan output looks OK, you can now apply the deployment, using the same options as when you ran the plan , and entering yes when prompted: Terraform apply terraform apply var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Plan: 106 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Example output Apply complete! Resources: 106 added, 0 changed, 0 destroyed. Outputs: OTC_Instances = [ \"john_otc, 52.47.138.166\", \"sarah_otc, 15.188.51.119\", \"amir_otc, 35.180.230.215\", ] retaildiscount_invoke_url = [ \"John_RetailOrderDiscount_api_gateway, o91dr4o3c6.execute-api.eu-west-3.amazonaws.com\", \"Sarah_RetailOrderDiscount_api_gateway, leelg1e0y9.execute-api.eu-west-3.amazonaws.com\", \"Amir_RetailOrderDiscount_api_gateway, 0gdlkj8ceb.execute-api.eu-west-3.amazonaws.com\", ] retailorder_arns = [ \"John_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrder\", \"Sarah_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrder\", \"Amir_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrder\", ] retailorder_invoke_url = [ \"John_RetailOrder_api_gateway, https://286kkhj9k1.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrder_api_gateway, https://rgv6lwdf81.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrder_api_gateway, https://5lnuuesqz3.execute-api.eu-west-3.amazonaws.com/default\", ] retailorderdiscount_arns = [ \"John_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderDiscount\", ] retailorderdiscount_path = [ \"John_RetailOrderDiscount, /default/John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, /default/Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, /default/Amir_RetailOrderDiscount\", ] retailorderline_arns = [ \"John_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderLine\", \"Sarah_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderLine\", \"Amir_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderLine\", ] retailorderprice_arns = [ \"John_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderPrice\", \"Sarah_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderPrice\", \"Amir_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderPrice\", ] retailorderprice_invoke_url = [ \"John_RetailOrderPrice_api_gateway, https://gohem8pwib.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrderPrice_api_gateway, https://dv23py0xo0.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrderPrice_api_gateway, https://dv1rn91g83.execute-api.eu-west-3.amazonaws.com/default\", ] 4.3 What Did We Deploy? \u00b6 Assuming a successful deployment, if you check in your AWS Console you should find the following (with multiple versions where UID matches the user names from quantity.auto.tfvars ): Lambda Functions UID_RetailOrder UID_RetailOrderDiscount UID_RetailOrderLine UID_RetailOrderPrice Lambda Layers request-opentracing_2_0 Instances UID_otc API Gateways UID_RetailOrder_api_gateway UID_RetailOrderDiscount_api_gateway UID_RetailOrderPrice_api_gateway IAM Roles splunk_lambda_role_xxxxxxxxxx IAM Policies lambda_initiate_lambda_policy_xxxxxxxxxxx","title":"Workshop Setup"},{"location":"lambda/setup/#workshop-setup","text":"This setup module is used to prepare a set of AWS Lambda Functions and EC2 Instances to be used for the workshop, these will need to be created by an AWS Admin who has access to a suitable account that can be used for the workshop. There will be 4 Lambda Functions and 1 EC2 instance deployed for each participant. Every resource will be prefixed with a Unique ID (UID) to identify which attendee they belong to. Terraform is used to deploy all of the resources and this module details the steps required to install and configure terraform, configure it to authenticate with AWS, and then deploy the resources.","title":"Workshop Setup"},{"location":"lambda/setup/#1-install-terraform","text":"Terraform is used to deploy all of the AWS infrastructure for the workshop, so needs to be installed on your machine. Instructions on how to install Terraform can be found here .","title":"1. Install Terraform"},{"location":"lambda/setup/#terraform-aws-authentication","text":"Once Terraform is installed, you need to configure it to authenticate with your AWS Account. Details on AWS Authentication can be found here . The AWS Authentication consists of two files, config and credentials which are typically located in the ~/.ssh folder. The config file details different profiles, and works in conjunction with the credentials file which contains your access and secret keys. config [default] region = us-east-1 output = json [profile splunk] region = us-east-1 output = json authentication [default] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key} [splunk] aws_access_key_id = {your access key} aws_secret_access_key = {your secret key}","title":"Terraform AWS Authentication"},{"location":"lambda/setup/#2-clone-workshop-content-will-need-to-update-url-once-published-to-splunk-repo","text":"The Workshop Content needs to be pulled down from Github to your local machine, then updated with your specific settings for AWS Authentication.","title":"2. Clone Workshop Content - WILL NEED TO UPDATE URL ONCE PUBLISHED TO SPLUNK REPO"},{"location":"lambda/setup/#21-download-the-workshop-repo","text":"Git Clone git clone git clone https://github.com/geoffhigginbottom/tflambdatestv2.git example result Workshop git clone https://github.com/geoffhigginbottom/tflambdatestv2.git cloning into 'tflambdatestv2'... remote: Enumerating objects: 276, done. remote: Counting objects: 100% (276/276), done. remote: Compressing objects: 100% (186/186), done. remote: Total 276 (delta 159), reused 206 (delta 89), pack-reused 0 Receiving objects: 100% (276/276), 269.48 KiB | 1.13 MiB/s, done. Resolving deltas: 100% (159/159), done.","title":"2.1 Download the Workshop Repo"},{"location":"lambda/setup/#22-create-terraformtfvars","text":"A file called terraform.tfvars needs to be created and populated with your specific settings. This file contains all of the settings required to enable Terraform to connect to both your AWS and Splunk Environments. An example version of the file is included in the repo named terraform.tfvars.example , which you should copy and rename to terraform.tfvars . Run the following command from within the directory where the workshop content was download. crete terraform.tfvars cp terraform.tfvars.example terraform.tfvars Then update the newly created terraform.tfvars starting with the AWS Variables Section.","title":"2.2 Create terraform.tfvars"},{"location":"lambda/setup/#221-aws-variables-section","text":"profile should match the profile name used in your aws authentication file which Terraform will use to authenticate with AWS key_name is the name of the ssh_key you wish to use to access the EC2 instances (note password login is also enabled on the Instances) private_key_path is the path to your private ssh key, such as ~/.ssh/xxx.pem or ~/.ssh/id_rsa instance_type is the AWS Instance Type used for the EC2 Instances - this defaults to the free tier \"t2.micro\" region is an optional parameter, normally left commented out. It enables you to override the region prompt during the Terraform deployment terraform.tfvars - AWS Variables ### AWS Variables ### profile = \"xxx\" key_name = \"yyy\" private_key_path = \"~/.ssh/xxx.pem or ~/.ssh/id_rsa etc\" instance_type = \"t2.micro\" ## Terraform will prompt you to select an AWS Region to deploy the resources into ## Setting a region here removes the region prompt (default is to have it prompt you) ## List of supported regions can be found in the variables.tf file #region = \"2\"","title":"2.2.1 AWS Variables Section"},{"location":"lambda/setup/#222-splunk-variables-section","text":"function_version is an optional parameter, normally left commented out. It enables you to override the version prompt during the Terraform deployment access_token is the token you wish to use to authenticate with the Splunk Monitoring backend realm specifies which Realm your Splunk Monitoring backend is deployed in collector_image specifies the contributor version of the otel collector, and the latest version can be found here and will need updating as new versions are released terraform.tfvars - Splunk Variables ### Splunk Variables ### ## Terraform will prompt you to select a version to be deployed, ## \"a\" = apm version, \"b\" = base version. ## Base version is typically deployed, but the apm version can be deployed ## for testing and comparison purposes #function_version = \"b\" access_token = \"xxxxxxxxxx\" realm = \"xxx\" # smart_agent_version = \"5.6.0-1\" # Optional - If left blank, latest will be installed smart_agent_version = \"\" # Optional - If left blank, latest will be installed ## Latest otel collector releases can be found at ## https://github.com/open-telemetry/opentelemetry-collector-contrib/releases collector_image = \"otel/opentelemetry-collector-contrib:0.15.0\"","title":"2.2.2 Splunk Variables Section"},{"location":"lambda/setup/#23-generate-the-unique-ids","text":"A file named quantity.auto.tfvars needs to be created and populated with your specific settings. This file contains the Unique IDs which will be appended to each AWS Resource to identify which workshop participant they are allocated to. There is an example file in the repo called quantity.auto.tfvars.example which needs to be copied and renamed to quantity.auto.tfvars . Run the following command from within the directory where the workshop content was download. crete quantity.auto.tfvars cp quantity.auto.tfvars.example quantity.auto.tfvars Edit quantity.auto.tfvars and populate the list of participants, ensuring each value is unique and has no spaces. Ensure the function_count value equals the total number of names, and that each entry ends with a comma, apart from the last one, as per the example below. quantity.auto.tfvars function_count = \"3\" function_ids = [ \"John\", \"Sarah\", \"Amir\" ]","title":"2.3 Generate the Unique IDs"},{"location":"lambda/setup/#3-initialize-terraform","text":"Once you have finished creating and updating xxx and yyy you now need to initialize terraform, so run the following command: Terraform init teraform init Example output Initializing the backend... Initializing provider plugins... - Finding latest version of terraform-providers/docker... - Finding latest version of hashicorp/null... - Finding latest version of hashicorp/archive... - Finding latest version of hashicorp/aws... - Installing hashicorp/archive v2.0.0... - Installed hashicorp/archive v2.0.0 (signed by HashiCorp) - Installing hashicorp/aws v3.18.0... - Installed hashicorp/aws v3.18.0 (signed by HashiCorp) - Installing terraform-providers/docker v2.7.2... - Installed terraform-providers/docker v2.7.2 (signed by HashiCorp) - Installing hashicorp/null v3.0.0... - Installed hashicorp/null v3.0.0 (signed by HashiCorp) The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, we recommend adding version constraints in a required_providers block in your configuration, with the constraint strings suggested below. * hashicorp/archive: version = \"~> 2.0.0\" * hashicorp/aws: version = \"~> 3.18.0\" * hashicorp/null: version = \"~> 3.0.0\" * terraform-providers/docker: version = \"~> 2.7.2\" Warning: Additional provider information from registry The remote registry returned warnings for registry.terraform.io/terraform-providers/docker: - For users on Terraform 0.13 or greater, this provider has moved to kreuzwerker/docker. Please update your source in required_providers. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"3. Initialize Terraform"},{"location":"lambda/setup/#4-deploy-the-workshop","text":"","title":"4. Deploy the Workshop"},{"location":"lambda/setup/#41-terraform-plan","text":"You can now deploy the workshop using Terraform. It is always best practice to run a plan so you can check what changes Terraform is going to make. When executing Terraform will prompt you to select a version of the workshop (select b for base), and also an AWS Region (choose an appropriate one from the list) Terraform plan terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Example output terraform plan var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.aws_ami.latest-ubuntu: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create <= read (data resources) Terraform will perform the following actions: # data.archive_file.retailorder_lambda_zip will be read during apply # (config refers to values not yet known) <= data \"archive_file\" \"retailorder_lambda_zip\" { + id = (known after apply) + output_base64sha256 = (known after apply) + output_md5 = (known after apply) + output_path = \"retailorder_lambda.zip\" + output_sha = (known after apply) + output_size = (known after apply) + source_file = \"retailorder_lambda_function.py\" + type = \"zip\" } ........ EXTRA LINES REMOVED ........ Plan: 106 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn't specify an \"-out\" parameter to save this plan, so Terraform can't guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run.","title":"4.1 Terraform Plan"},{"location":"lambda/setup/#43-terraform-apply","text":"After checking the plan output looks OK, you can now apply the deployment, using the same options as when you ran the plan , and entering yes when prompted: Terraform apply terraform apply var.function_version Select Function Version (a:apm, b:base) Enter a value: b var.region Select region (1:eu-west-1, 2:eu-west-3, 3:eu-central-1, 4:us-east-1, 5:us-east-2, 6:us-west-1, 7:us-west-2, 8:ap-southeast-1, 9:ap-southeast-2, 10:sa-east-1 ) Enter a value: 2 Plan: 106 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Example output Apply complete! Resources: 106 added, 0 changed, 0 destroyed. Outputs: OTC_Instances = [ \"john_otc, 52.47.138.166\", \"sarah_otc, 15.188.51.119\", \"amir_otc, 35.180.230.215\", ] retaildiscount_invoke_url = [ \"John_RetailOrderDiscount_api_gateway, o91dr4o3c6.execute-api.eu-west-3.amazonaws.com\", \"Sarah_RetailOrderDiscount_api_gateway, leelg1e0y9.execute-api.eu-west-3.amazonaws.com\", \"Amir_RetailOrderDiscount_api_gateway, 0gdlkj8ceb.execute-api.eu-west-3.amazonaws.com\", ] retailorder_arns = [ \"John_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrder\", \"Sarah_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrder\", \"Amir_RetailOrder, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrder\", ] retailorder_invoke_url = [ \"John_RetailOrder_api_gateway, https://286kkhj9k1.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrder_api_gateway, https://rgv6lwdf81.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrder_api_gateway, https://5lnuuesqz3.execute-api.eu-west-3.amazonaws.com/default\", ] retailorderdiscount_arns = [ \"John_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderDiscount\", ] retailorderdiscount_path = [ \"John_RetailOrderDiscount, /default/John_RetailOrderDiscount\", \"Sarah_RetailOrderDiscount, /default/Sarah_RetailOrderDiscount\", \"Amir_RetailOrderDiscount, /default/Amir_RetailOrderDiscount\", ] retailorderline_arns = [ \"John_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderLine\", \"Sarah_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderLine\", \"Amir_RetailOrderLine, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderLine\", ] retailorderprice_arns = [ \"John_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:John_RetailOrderPrice\", \"Sarah_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Sarah_RetailOrderPrice\", \"Amir_RetailOrderPrice, arn:aws:lambda:eu-west-3:527477237977:function:Amir_RetailOrderPrice\", ] retailorderprice_invoke_url = [ \"John_RetailOrderPrice_api_gateway, https://gohem8pwib.execute-api.eu-west-3.amazonaws.com/default\", \"Sarah_RetailOrderPrice_api_gateway, https://dv23py0xo0.execute-api.eu-west-3.amazonaws.com/default\", \"Amir_RetailOrderPrice_api_gateway, https://dv1rn91g83.execute-api.eu-west-3.amazonaws.com/default\", ]","title":"4.3 Terraform Apply"},{"location":"lambda/setup/#43-what-did-we-deploy","text":"Assuming a successful deployment, if you check in your AWS Console you should find the following (with multiple versions where UID matches the user names from quantity.auto.tfvars ): Lambda Functions UID_RetailOrder UID_RetailOrderDiscount UID_RetailOrderLine UID_RetailOrderPrice Lambda Layers request-opentracing_2_0 Instances UID_otc API Gateways UID_RetailOrder_api_gateway UID_RetailOrderDiscount_api_gateway UID_RetailOrderPrice_api_gateway IAM Roles splunk_lambda_role_xxxxxxxxxx IAM Policies lambda_initiate_lambda_policy_xxxxxxxxxxx","title":"4.3 What Did We Deploy?"},{"location":"lambda/springboot-apm-II/","text":"Enable APM for Mobile Shop Springboot App (Cont.) \u00b6 3. Run a case and find both the Service Dashboard and your trace \u00b6 Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this: 3.1 Find your Service Dashboard for the Springboot app in Splunk APM \u00b6 Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Springboot App will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes. 3.1 Look at trace info in splunk APM \u00b6 Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application. Lets start enable APM in our first Lambda function to enrich this view!","title":"Enable APM for Mobile Shop Springboot App (Cont.)"},{"location":"lambda/springboot-apm-II/#enable-apm-for-mobile-shop-springboot-app-cont","text":"","title":"Enable APM for Mobile Shop Springboot App (Cont.)"},{"location":"lambda/springboot-apm-II/#3-run-a-case-and-find-both-the-service-dashboard-and-your-trace","text":"Go back to the browser tab with your Phone App running that you opened earlier, if you have closed it you can open a new one by navigating to http:// [ec2_ip] :8080/order (where [ec2_ip] is the public ip address of your EC2 instance) Generate your first trace in your environment by typing a phone name, selecting a number greater then 0 and choosing a customer type: Name of a phone: For example Geoff's big pictures phone Quantity: 2 Select a customer type: Silver Click submit to run the application with the newly enabled FrameWork and Tracing settings, which will result in the generation of your first APM Trace. The result should be similar to this:","title":"3. Run a case and find both the Service Dashboard and your trace"},{"location":"lambda/springboot-apm-II/#31-find-your-service-dashboard-for-the-springboot-app-in-splunk-apm","text":"Right now your trace is being processed by the splunk APM back end, and since this is the first time this service is seen by Splunk APM, the corresponding service dashboard for the Springboot App will be generated. Return to the APM Tab you opened in Step 1 of this section which should be displaying the Splunk APM Monitoring Dashboard. Hover over Dashboards in the top menu, and then click on All Dashboards . A number of pre-built dashboards are provided for you in your default view. Here you should have a Dashboard Group called APM Services (If it is not present, wait for a minute or two and refresh the screen, If it has not appeared after a couple of minutes, reach out the the workshop leader) Select the Services Dashboard. From the Environment Drop down box select UID_ Retail_Demo , from the Service drop down box select uid -mobile-web-shop-base (where [UID] is your unique UID allocated to you for this Workshop. The following examples have a UID of ACME). This wil give you the automatically generated service dashboard for uid -mobile-web-shop-base . If you set the time to -5 minutes you can see the single invocation, the averages over time on the left side will become active after a few minutes.","title":"3.1 Find your Service Dashboard for the Springboot app in Splunk APM"},{"location":"lambda/springboot-apm-II/#31-look-at-trace-info-in-splunk-apm","text":"Now navigate back to the APM Tab: At the APM monitoring page, you should now have a single circle in the centre of the dashboard, this represents the UID_Retail_Demo Service. Over the next modules you will enable APM on additional services which will then also appear in this view. If you see more services please filter it down by selecting your environment UID_ Retail_Demo from the drop down list. You can also see the two endpoints used by the service. Now click on Troubleshooting to go to the Troubleshooting view. You should see your single service with on the right, two dashboards with a spike indicating the single invocation. Select the top of the spike of the top dashboard as shown. This will expand and show a list of traces matching the time frame selected. (In our case it should be our single trace.) Click on the blue Trace-id to see the waterfall view of the trace. (Clicking on the Span name uid -mobile-web-shop-base will expand it as shown.) In the expanded view you can see the default tags send by the application. Lets start enable APM in our first Lambda function to enrich this view!","title":"3.1 Look at trace info in splunk APM"},{"location":"lambda/springboot-apm/","text":"Enable APM for Mobile Shop Springboot App \u00b6 1. Validate APM Environment \u00b6 The first activity we are going to do is validate that we have access to the Splunk APM environment and establish our starting point. To do this please login to Splunk Infrastructure & APM and select APM . This will bring you to the APM monitoring page, depending how many services you are currently monitoring with Splunk APM, you may or may not see a list of services. Once you have run the applications with APM enabled, you can filter the environment by entering your unique id you have been provided. This should then show just your APM environment. So lets start enabling APM in our environment. 2. Update the settings in the spring boot app \u00b6 To enable APM on the Spring boot application we need to update the FrameWork (pom.xml), update the application settings en add enable some lines in the code. 2.1 Update the FrameWork by updating POM.XML \u00b6 Connect back into your EC2 instance and stop the running Spring boot application by pressing Ctrl + C . Once the application is stopped, open an editor and edit the pom.xml file Shell Command nano pom.xml Scroll down until you find the following section and remove the comment marks lines (<-- & -->) by placing the cursor on the line with the remark and press Ctrl + K Afterwards the section will look like this: Make sure the lines are properly aligned and save the file by pressing Ctrl + O followed by Enter to write the file pom.xml to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line. 2.2 Update the application property file \u00b6 Edit the application property file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/resources/application.properties Remove the comment marks ## on the following 2 lines: ##spring.sleuth.sampler.probability=1.0 ##spring.zipkin.baseUrl=http://localhost:9080 The first line tells the springboot app to send 100% of the traces to Splunk APM and the second line directs the traces and spans to the local local OpenTelemetry Collector that will forward them to splunk APM. Your file should now look like this: application.properties File spring.thymeleaf.cache=false spring.thymeleaf.enabled=true spring.thymeleaf.prefix=classpath:/templates/ spring.thymeleaf.suffix=.html server.port=8080 spring.application.name=ACME-Mobile-Web-Shop-Base # For Sleuth 2.1+ use this property ## Enable this for full fidelity tracing spring.sleuth.sampler.probability=1.0 # The base url with the endpoint (/api/v2/spans) excluded # OpenTelemetry Collector deployed in your TKE namespace ## Enable this to send Traces to SignalFX spring.zipkin.baseUrl=http://localhost:9080 Now save the file by pressing Ctrl + O followed by enter to write the file applications.properties file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line. 2.3 Update the source code by enabling APM \u00b6 Open the main java file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/java/com/sfx/JavaLambda/JavaLambdaController.java Remove the comment marks /* & */ around the following three lines: /* import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; */ And remove the comment marks // in front of the following two lines: //@Autowired Tracer tracer; //@Autowired SpanCustomizer span; So that your JavaLambdaController.java file now looks like this (note only the first 33 lines are shown below for brevity): JavaLambdaController.java package com.sfx.JavaLambda; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import com.fasterxml.jackson.databind.ObjectMapper; import com.fasterxml.jackson.databind.ObjectReader; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.http.*; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.client.RestTemplate; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ModelAttribute; import org.springframework.web.bind.annotation.PostMapping; import java.util.*; import java.io.IOException; import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; @Controller public class JavaLambdaController { // set up AutoWired sleuth for APM @Autowired Tracer tracer; @Autowired SpanCustomizer span; Now save the file by pressing Ctrl + O followed by enter to write the file Java file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line. 2.4 Run the App \u00b6 Run the application by issuing the following command again: Shell Command mvn spring-boot:run When you have updated the files correctly you should see the SpringBoot logo again with no errors . We are now ready to test the app and send our first Trace. Info If you get errors: Please make sure that all the files are properly aligned and replace any leading spaces with Tabs Continue to the next section.","title":"Enable APM for Mobile Shop Springboot App"},{"location":"lambda/springboot-apm/#enable-apm-for-mobile-shop-springboot-app","text":"","title":"Enable APM for Mobile Shop Springboot App"},{"location":"lambda/springboot-apm/#1-validate-apm-environment","text":"The first activity we are going to do is validate that we have access to the Splunk APM environment and establish our starting point. To do this please login to Splunk Infrastructure & APM and select APM . This will bring you to the APM monitoring page, depending how many services you are currently monitoring with Splunk APM, you may or may not see a list of services. Once you have run the applications with APM enabled, you can filter the environment by entering your unique id you have been provided. This should then show just your APM environment. So lets start enabling APM in our environment.","title":"1. Validate APM Environment"},{"location":"lambda/springboot-apm/#2-update-the-settings-in-the-spring-boot-app","text":"To enable APM on the Spring boot application we need to update the FrameWork (pom.xml), update the application settings en add enable some lines in the code.","title":"2. Update the settings in the spring boot app"},{"location":"lambda/springboot-apm/#21-update-the-framework-by-updating-pomxml","text":"Connect back into your EC2 instance and stop the running Spring boot application by pressing Ctrl + C . Once the application is stopped, open an editor and edit the pom.xml file Shell Command nano pom.xml Scroll down until you find the following section and remove the comment marks lines (<-- & -->) by placing the cursor on the line with the remark and press Ctrl + K Afterwards the section will look like this: Make sure the lines are properly aligned and save the file by pressing Ctrl + O followed by Enter to write the file pom.xml to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line.","title":"2.1 Update the FrameWork by updating POM.XML"},{"location":"lambda/springboot-apm/#22-update-the-application-property-file","text":"Edit the application property file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/resources/application.properties Remove the comment marks ## on the following 2 lines: ##spring.sleuth.sampler.probability=1.0 ##spring.zipkin.baseUrl=http://localhost:9080 The first line tells the springboot app to send 100% of the traces to Splunk APM and the second line directs the traces and spans to the local local OpenTelemetry Collector that will forward them to splunk APM. Your file should now look like this: application.properties File spring.thymeleaf.cache=false spring.thymeleaf.enabled=true spring.thymeleaf.prefix=classpath:/templates/ spring.thymeleaf.suffix=.html server.port=8080 spring.application.name=ACME-Mobile-Web-Shop-Base # For Sleuth 2.1+ use this property ## Enable this for full fidelity tracing spring.sleuth.sampler.probability=1.0 # The base url with the endpoint (/api/v2/spans) excluded # OpenTelemetry Collector deployed in your TKE namespace ## Enable this to send Traces to SignalFX spring.zipkin.baseUrl=http://localhost:9080 Now save the file by pressing Ctrl + O followed by enter to write the file applications.properties file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line.","title":"2.2 Update the application property file"},{"location":"lambda/springboot-apm/#23-update-the-source-code-by-enabling-apm","text":"Open the main java file of the springboot application in the nano editor by enter in the following command: Shell Command nano src/main/java/com/sfx/JavaLambda/JavaLambdaController.java Remove the comment marks /* & */ around the following three lines: /* import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; */ And remove the comment marks // in front of the following two lines: //@Autowired Tracer tracer; //@Autowired SpanCustomizer span; So that your JavaLambdaController.java file now looks like this (note only the first 33 lines are shown below for brevity): JavaLambdaController.java package com.sfx.JavaLambda; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import com.fasterxml.jackson.databind.ObjectMapper; import com.fasterxml.jackson.databind.ObjectReader; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.http.*; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.client.RestTemplate; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ModelAttribute; import org.springframework.web.bind.annotation.PostMapping; import java.util.*; import java.io.IOException; import brave.sampler.Sampler; import brave.SpanCustomizer; import brave.Tracer; @Controller public class JavaLambdaController { // set up AutoWired sleuth for APM @Autowired Tracer tracer; @Autowired SpanCustomizer span; Now save the file by pressing Ctrl + O followed by enter to write the file Java file to disk. You can now leave the nano editor by pressing Ctrl + X . This should bring you back to the command line.","title":"2.3 Update the source code by enabling APM"},{"location":"lambda/springboot-apm/#24-run-the-app","text":"Run the application by issuing the following command again: Shell Command mvn spring-boot:run When you have updated the files correctly you should see the SpringBoot logo again with no errors . We are now ready to test the app and send our first Trace. Info If you get errors: Please make sure that all the files are properly aligned and replace any leading spaces with Tabs Continue to the next section.","title":"2.4 Run the App"},{"location":"lambda/wip/","text":"This section is still under construction \u00b6","title":"This section is still under construction"},{"location":"lambda/wip/#this-section-is-still-under-construction","text":"","title":"This section is still under construction"},{"location":"module-support/cleanup/","text":"Post Workshop Clean Up \u00b6 Multipass Once you have finished with this Workshop exit from the Multipass instance to get back to your system command prompt. Enter the following to delete and purge the Multipass instance: multipass delete --purge ${INSTANCE} AWS/EC2 Once you have finished with this Workshop exit from the AWS/EC2 instance to get back to your system command prompt. We will use Terraform to destroy the instance with the parameters you used in Smart Agent module: cd ~/workshop/ec2 terraform destroy -var=\"aws_instance_count=1\" -var=\"instance_type=1\" Enter Instance Count , Provide the desired region and Select instance type required . When prompted type yes to confirm you want to destroy, this will take a while to complete. aws_instance.observability-instance[0]: Destroying... [id=i-088560a5f6e2bbdbb] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 10s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 20s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 30s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 40s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 50s elapsed] aws_instance.observability-instance[0]: Destruction complete after 56s aws_security_group.instance: Destroying... [id=sg-0d6841fbeef022a9f] aws_security_group.instance: Destruction complete after 2s","title":"Post Workshop Clean Up"},{"location":"module-support/cleanup/#post-workshop-clean-up","text":"Multipass Once you have finished with this Workshop exit from the Multipass instance to get back to your system command prompt. Enter the following to delete and purge the Multipass instance: multipass delete --purge ${INSTANCE} AWS/EC2 Once you have finished with this Workshop exit from the AWS/EC2 instance to get back to your system command prompt. We will use Terraform to destroy the instance with the parameters you used in Smart Agent module: cd ~/workshop/ec2 terraform destroy -var=\"aws_instance_count=1\" -var=\"instance_type=1\" Enter Instance Count , Provide the desired region and Select instance type required . When prompted type yes to confirm you want to destroy, this will take a while to complete. aws_instance.observability-instance[0]: Destroying... [id=i-088560a5f6e2bbdbb] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 10s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 20s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 30s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 40s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 50s elapsed] aws_instance.observability-instance[0]: Destruction complete after 56s aws_security_group.instance: Destroying... [id=sg-0d6841fbeef022a9f] aws_security_group.instance: Destruction complete after 2s","title":"Post Workshop Clean Up"},{"location":"module-support/hotrod-eks/","text":"Deploying Hot R.O.D. in AWS/EKS \u00b6 Enabling APM An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from SignalFx to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to SignalFx and check that you have the APM tab on the top navbar next to Dashboards. 1. Launch the Multipass instance \u00b6 If you have not already completed the Smart Agent Preparation , then please do so, otherwise jump to Step #2 2. Create environment variables \u00b6 Create the following environment variables for SignalFx and AWS to use in the proceeding steps: SignalFx export ACCESS_TOKEN=[ACCESS_TOKEN] export REALM=[REALM e.g. us1] AWS export AWS_ACCESS_KEY_ID=[AWS Access Key] export AWS_SECRET_ACCESS_KEY=[AWS Secret Access Key] export AWS_DEFAULT_REGION=[e.g. us-east-1] export AWS_DEFAULT_OUTPUT=json export EKS_CLUSTER_NAME=$(hostname)-APP-DEV You can check for the latest SignalFx Smart Agent release on Github . 3. Configure AWS CLI for your account \u00b6 Use the AWS CLI to configure access to your AWS environment. The environment variables configured above mean you can just hit enter on each of the prompts to accept the values: Shell Command aws configure Output AWS Access Key ID [****************TVAQ]: AWS Secret Access Key [****************MkB4]: Default region name [us-east-1]: Default output format [json]: 4. Create a cluster running Amazon Elastic Kubernetes Service (EKS) \u00b6 Shell Command eksctl create cluster \\ --name $EKS_CLUSTER_NAME \\ --region $AWS_DEFAULT_REGION \\ --node-type t3.medium \\ --nodes-min 3 \\ --nodes-max 7 \\ --version=1.15 Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] setting availability zones to [us-east-1a us-east-1f] [\u2139] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19 [\u2139] subnets for us-east-1f - public:192.168.32.0/19 private:192.168.96.0/19 [\u2139] nodegroup \"ng-371a784a\" will use \"ami-0e5bb2367e692b807\" [AmazonLinux2/1.15] [\u2139] using Kubernetes version 1.15 [\u2139] creating EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region with un-managed nodes [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] CloudWatch logging will not be enabled for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"EKS-APP-DEV\", create nodegroup \"ng-371a784a\" } [\u2139] building cluster stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] building nodegroup stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2714] all EKS cluster resources for \"EKS-APP-DEV\" have been created [!] unable to write kubeconfig , please retry with 'eksctl utils write-kubeconfig -n EKS-APP-DEV': unable to modify kubeconfig /home/ubuntu/.kube/config: open /etc/rancher/k3s/k3s.yaml.lock: permission denied [\u2139] adding identity \"arn:aws:iam::327192335161:role/eksctl-EKS-APP-DEV-nodegroup-ng-3-NodeInstanceRole-2RMH7RBODD62\" to auth ConfigMap [\u2139] nodegroup \"ng-371a784a\" has 0 node(s) [\u2139] waiting for at least 3 node(s) to become ready in \"ng-371a784a\" [\u2139] nodegroup \"ng-371a784a\" has 3 node(s) [\u2139] node \"ip-192-168-35-104.ec2.internal\" is ready [\u2139] node \"ip-192-168-52-88.ec2.internal\" is ready [\u2139] node \"ip-192-168-8-236.ec2.internal\" is ready [\u2714] EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region is ready This may take some time (10-15 minutes). Ensure you see your cluster active in AWS EKS console before proceeding. Note You can ignore the error about unable to write kubeconfig as we address this below. Once complete update your kubeconfig to allow sudo kubectl access to the cluster: Shell Command sudo eksctl utils write-kubeconfig -n $EKS_CLUSTER_NAME 5. Deploy SignalFx SmartAgent to your EKS Cluster \u00b6 Add the SignalFx Helm chart repository to Helm: Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo Output \"signalfx\" has been added to your repositories Ensure the latest state of the SignalFx Helm repository: Shell Command helm repo update Output Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"signalfx\" chart repository Install the Smart Agent Helm chart with the following commands: Shell Command helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$EKS_CLUSTER_NAME \\ --set signalFxRealm=$REALM \\ --set kubeletAPI.url=https://localhost:10250 \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ signalfx-agent signalfx/signalfx-agent \\ -f workshop/k3s/values.yaml Output NAME: signalfx-agent LAST DEPLOYED: Mon Apr 13 14:23:19 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The SignalFx agent is being deployed in your Kubernetes cluster. You should see metrics flowing once the agent image is downloaded and started (this may take a few minutes since it has to download the agent container image). Assuming you are logged into SignalFx in your browser, visit https://app.us0.signalfx.com/#/navigator/kubernetes%20pods/kubernetes%20pods to see all of the pods in your cluster. Validate cluster looks healthy in SignalFx Kubernetes Navigator dashboard 6. Deploy Hot R.O.D. Application to EKS \u00b6 Shell Command sudo kubectl apply -f ~/workshop/apm/hotrod/k8s/deployment.yaml To ensure the Hot R.O.D. application is running see examples below: Shell Command sudo kubectl get pods Output NAME READY STATUS RESTARTS AGE hotrod-7564774bf5-vjpfw 1/1 Running 0 47h signalfx-agent-jmq4f 1/1 Running 0 138m signalfx-agent-nk8p9 1/1 Running 0 138m signalfx-agent-q5tzh 1/1 Running 0 138m You then need find the IP address assigned to the Hot R.O.D. service: Shell Command sudo kubectl get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hotrod LoadBalancer 10.100.188.249 af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com 8080:32521/TCP 47h kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 3d1h Create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(sudo kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') You can view / exercise Hot R.O.D. yourself in a browser by opening the EXTERNAL-IP:PORT as shown above e.g. Example URL https://af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com:8080 7. Generate some traffic to the application using Siege Benchmark \u00b6 Shell Command siege -r10 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number Shell Command siege -r10 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=391&nonse=0.17041229755366172\" You should now be able to exercise SignalFx APM dashboards. 8. Cleaning up \u00b6 To delete entire EKS cluster: Shell Command eksctl delete cluster $EKS_CLUSTER_NAME Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] deleting EKS cluster \"RWC-APP-DEV\" [\u2139] deleted 0 Fargate profile(s) [\u2139] cleaning up LoadBalancer services [\u2139] 2 sequential tasks: { delete nodegroup \"ng-371a784a\", delete cluster control plane \"EKS-APP-DEV\" [async] } [\u2139] will delete stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] waiting for stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" to get deleted [\u2139] will delete stack \"eksctl-EKS-APP-DEV-cluster\" [\u2714] all cluster resources were deleted Or to delete individual components: Shell Command sudo kubectl delete deploy/hotrod svc/hotrod helm delete signalfx-agent To switch back to using the local K3s cluster: Shell Command sudo sudo kubectl config use-context default","title":"Deploying Hot R.O.D. in AWS/EKS"},{"location":"module-support/hotrod-eks/#deploying-hot-rod-in-awseks","text":"Enabling APM An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from SignalFx to get a trial instance with APM enabled if you don\u2019t have one already. To check if you have an Organization with APM enabled, just login to SignalFx and check that you have the APM tab on the top navbar next to Dashboards.","title":"Deploying Hot R.O.D. in AWS/EKS"},{"location":"module-support/hotrod-eks/#1-launch-the-multipass-instance","text":"If you have not already completed the Smart Agent Preparation , then please do so, otherwise jump to Step #2","title":"1. Launch the Multipass instance"},{"location":"module-support/hotrod-eks/#2-create-environment-variables","text":"Create the following environment variables for SignalFx and AWS to use in the proceeding steps: SignalFx export ACCESS_TOKEN=[ACCESS_TOKEN] export REALM=[REALM e.g. us1] AWS export AWS_ACCESS_KEY_ID=[AWS Access Key] export AWS_SECRET_ACCESS_KEY=[AWS Secret Access Key] export AWS_DEFAULT_REGION=[e.g. us-east-1] export AWS_DEFAULT_OUTPUT=json export EKS_CLUSTER_NAME=$(hostname)-APP-DEV You can check for the latest SignalFx Smart Agent release on Github .","title":"2. Create environment variables"},{"location":"module-support/hotrod-eks/#3-configure-aws-cli-for-your-account","text":"Use the AWS CLI to configure access to your AWS environment. The environment variables configured above mean you can just hit enter on each of the prompts to accept the values: Shell Command aws configure Output AWS Access Key ID [****************TVAQ]: AWS Secret Access Key [****************MkB4]: Default region name [us-east-1]: Default output format [json]:","title":"3. Configure AWS CLI for your account"},{"location":"module-support/hotrod-eks/#4-create-a-cluster-running-amazon-elastic-kubernetes-service-eks","text":"Shell Command eksctl create cluster \\ --name $EKS_CLUSTER_NAME \\ --region $AWS_DEFAULT_REGION \\ --node-type t3.medium \\ --nodes-min 3 \\ --nodes-max 7 \\ --version=1.15 Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] setting availability zones to [us-east-1a us-east-1f] [\u2139] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19 [\u2139] subnets for us-east-1f - public:192.168.32.0/19 private:192.168.96.0/19 [\u2139] nodegroup \"ng-371a784a\" will use \"ami-0e5bb2367e692b807\" [AmazonLinux2/1.15] [\u2139] using Kubernetes version 1.15 [\u2139] creating EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region with un-managed nodes [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] CloudWatch logging will not be enabled for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"EKS-APP-DEV\", create nodegroup \"ng-371a784a\" } [\u2139] building cluster stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] building nodegroup stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2714] all EKS cluster resources for \"EKS-APP-DEV\" have been created [!] unable to write kubeconfig , please retry with 'eksctl utils write-kubeconfig -n EKS-APP-DEV': unable to modify kubeconfig /home/ubuntu/.kube/config: open /etc/rancher/k3s/k3s.yaml.lock: permission denied [\u2139] adding identity \"arn:aws:iam::327192335161:role/eksctl-EKS-APP-DEV-nodegroup-ng-3-NodeInstanceRole-2RMH7RBODD62\" to auth ConfigMap [\u2139] nodegroup \"ng-371a784a\" has 0 node(s) [\u2139] waiting for at least 3 node(s) to become ready in \"ng-371a784a\" [\u2139] nodegroup \"ng-371a784a\" has 3 node(s) [\u2139] node \"ip-192-168-35-104.ec2.internal\" is ready [\u2139] node \"ip-192-168-52-88.ec2.internal\" is ready [\u2139] node \"ip-192-168-8-236.ec2.internal\" is ready [\u2714] EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region is ready This may take some time (10-15 minutes). Ensure you see your cluster active in AWS EKS console before proceeding. Note You can ignore the error about unable to write kubeconfig as we address this below. Once complete update your kubeconfig to allow sudo kubectl access to the cluster: Shell Command sudo eksctl utils write-kubeconfig -n $EKS_CLUSTER_NAME","title":"4. Create a cluster running Amazon Elastic Kubernetes Service (EKS)"},{"location":"module-support/hotrod-eks/#5-deploy-signalfx-smartagent-to-your-eks-cluster","text":"Add the SignalFx Helm chart repository to Helm: Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo Output \"signalfx\" has been added to your repositories Ensure the latest state of the SignalFx Helm repository: Shell Command helm repo update Output Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"signalfx\" chart repository Install the Smart Agent Helm chart with the following commands: Shell Command helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$EKS_CLUSTER_NAME \\ --set signalFxRealm=$REALM \\ --set kubeletAPI.url=https://localhost:10250 \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ signalfx-agent signalfx/signalfx-agent \\ -f workshop/k3s/values.yaml Output NAME: signalfx-agent LAST DEPLOYED: Mon Apr 13 14:23:19 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The SignalFx agent is being deployed in your Kubernetes cluster. You should see metrics flowing once the agent image is downloaded and started (this may take a few minutes since it has to download the agent container image). Assuming you are logged into SignalFx in your browser, visit https://app.us0.signalfx.com/#/navigator/kubernetes%20pods/kubernetes%20pods to see all of the pods in your cluster. Validate cluster looks healthy in SignalFx Kubernetes Navigator dashboard","title":"5. Deploy SignalFx SmartAgent to your EKS Cluster"},{"location":"module-support/hotrod-eks/#6-deploy-hot-rod-application-to-eks","text":"Shell Command sudo kubectl apply -f ~/workshop/apm/hotrod/k8s/deployment.yaml To ensure the Hot R.O.D. application is running see examples below: Shell Command sudo kubectl get pods Output NAME READY STATUS RESTARTS AGE hotrod-7564774bf5-vjpfw 1/1 Running 0 47h signalfx-agent-jmq4f 1/1 Running 0 138m signalfx-agent-nk8p9 1/1 Running 0 138m signalfx-agent-q5tzh 1/1 Running 0 138m You then need find the IP address assigned to the Hot R.O.D. service: Shell Command sudo kubectl get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hotrod LoadBalancer 10.100.188.249 af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com 8080:32521/TCP 47h kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 3d1h Create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(sudo kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') You can view / exercise Hot R.O.D. yourself in a browser by opening the EXTERNAL-IP:PORT as shown above e.g. Example URL https://af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com:8080","title":"6. Deploy Hot R.O.D. Application to EKS"},{"location":"module-support/hotrod-eks/#7-generate-some-traffic-to-the-application-using-siege-benchmark","text":"Shell Command siege -r10 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number Shell Command siege -r10 -c10 \"http://$HOTROD_ENDPOINT/dispatch?customer=391&nonse=0.17041229755366172\" You should now be able to exercise SignalFx APM dashboards.","title":"7. Generate some traffic to the application using Siege Benchmark"},{"location":"module-support/hotrod-eks/#8-cleaning-up","text":"To delete entire EKS cluster: Shell Command eksctl delete cluster $EKS_CLUSTER_NAME Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] deleting EKS cluster \"RWC-APP-DEV\" [\u2139] deleted 0 Fargate profile(s) [\u2139] cleaning up LoadBalancer services [\u2139] 2 sequential tasks: { delete nodegroup \"ng-371a784a\", delete cluster control plane \"EKS-APP-DEV\" [async] } [\u2139] will delete stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] waiting for stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" to get deleted [\u2139] will delete stack \"eksctl-EKS-APP-DEV-cluster\" [\u2714] all cluster resources were deleted Or to delete individual components: Shell Command sudo kubectl delete deploy/hotrod svc/hotrod helm delete signalfx-agent To switch back to using the local K3s cluster: Shell Command sudo sudo kubectl config use-context default","title":"8. Cleaning up"},{"location":"module-support/vm/","text":"Lab Summary \u00b6 Deploy SignalFx Smart Agent via install script on a VM Confirm the Smart Agent is working and sending data Use multipass to create a vanilla Ubuntu VM and shell into it. You can also use a Linux-based VM with your cloud provider of choice. Replace [INITIALS] with your actual initials. Shell Command multipass launch [INITIALS]-vm multipass shell [INITIALs]-vm 1. Deploy SignalFx Smart Agent via install script on a VM \u00b6 You will need to obtain your Access Token from the SignalFx UI. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Later in the lab you can come back here and click the Copy button which will copy it to your clipboard so you can paste it when you need to provide an access token in the lab. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select 'My Profile'. The Ream can be found in the middle of the page within the Organizations section. In this example it is us1 . SignalFx maintains a shell script to install on supported distributions. Copy the script below and replace $REALM and $ACCESS_TOKEN with the values found in previous screen: Shell Command curl -sSL https://dl.signalfx.com/signalfx-agent.sh > /tmp/signalfx-agent.sh sudo sh /tmp/signalfx-agent.sh --realm $REALM -- $ACCESS_TOKEN Once the installation is complete check the status of the agent. Shell Command signalfx-agent status Output SignalFx Agent version: 5.1.2 Agent uptime: 4s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 16 Bad Monitor Config: None Global Dimensions: {host: as-k3s} GlobalSpanTags: map[] Datapoints sent (last minute): 0 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 0 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything Important Make a note of the value displayed for host in the Global Dimensions section of the output, as you need this later! 2. Confirm the Smart Agent is working and sending data \u00b6 To see the Metrics that the Smart Agent is sending to SignalFx, please goto the SignalFX UI, and select Infrastructure \u2192 Hosts to see the lists of hosts. Here you see a list of the Nodes that have an Smart Agent installed and are reporting into SignalFx. Make sure you see your Multipass or AWS/EC2 instance in the list of hosts. (The hostname from the previous section) You can also set a filter for just your instance by selecting the host: attribute, followed by picking the name of your host from the drop down list. Click on the link to your host from the list, this wil take you to the overview page of your host. Make sure you have the SYSTEM METRIC tab selected. Here you can see various charts that relate to the health of your host, like CPU & Memory Used%, Disk I/O and many more. You can also see the list of services running on your host by selecting the PROCESSES tab. Take a moment to explore the various charts and the Processes list.","title":"Lab Summary"},{"location":"module-support/vm/#lab-summary","text":"Deploy SignalFx Smart Agent via install script on a VM Confirm the Smart Agent is working and sending data Use multipass to create a vanilla Ubuntu VM and shell into it. You can also use a Linux-based VM with your cloud provider of choice. Replace [INITIALS] with your actual initials. Shell Command multipass launch [INITIALS]-vm multipass shell [INITIALs]-vm","title":"Lab Summary"},{"location":"module-support/vm/#1-deploy-signalfx-smart-agent-via-install-script-on-a-vm","text":"You will need to obtain your Access Token from the SignalFx UI. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Later in the lab you can come back here and click the Copy button which will copy it to your clipboard so you can paste it when you need to provide an access token in the lab. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select 'My Profile'. The Ream can be found in the middle of the page within the Organizations section. In this example it is us1 . SignalFx maintains a shell script to install on supported distributions. Copy the script below and replace $REALM and $ACCESS_TOKEN with the values found in previous screen: Shell Command curl -sSL https://dl.signalfx.com/signalfx-agent.sh > /tmp/signalfx-agent.sh sudo sh /tmp/signalfx-agent.sh --realm $REALM -- $ACCESS_TOKEN Once the installation is complete check the status of the agent. Shell Command signalfx-agent status Output SignalFx Agent version: 5.1.2 Agent uptime: 4s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 16 Bad Monitor Config: None Global Dimensions: {host: as-k3s} GlobalSpanTags: map[] Datapoints sent (last minute): 0 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 0 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything Important Make a note of the value displayed for host in the Global Dimensions section of the output, as you need this later!","title":"1. Deploy SignalFx Smart Agent via install script on a VM"},{"location":"module-support/vm/#2-confirm-the-smart-agent-is-working-and-sending-data","text":"To see the Metrics that the Smart Agent is sending to SignalFx, please goto the SignalFX UI, and select Infrastructure \u2192 Hosts to see the lists of hosts. Here you see a list of the Nodes that have an Smart Agent installed and are reporting into SignalFx. Make sure you see your Multipass or AWS/EC2 instance in the list of hosts. (The hostname from the previous section) You can also set a filter for just your instance by selecting the host: attribute, followed by picking the name of your host from the drop down list. Click on the link to your host from the list, this wil take you to the overview page of your host. Make sure you have the SYSTEM METRIC tab selected. Here you can see various charts that relate to the health of your host, like CPU & Memory Used%, Disk I/O and many more. You can also see the list of services running on your host by selecting the PROCESSES tab. Take a moment to explore the various charts and the Processes list.","title":"2. Confirm the Smart Agent is working and sending data"},{"location":"monitoring-as-code/","text":"Monitoring as Code - Lab Summary \u00b6 Use Terraform 1 to manage Observability Suite Dashboards and Detectors Initialize the Terraform SignalFx Provider 2 . Run Terraform to create detectors and dashboards from code using the SignalFx Terraform Provider. See how Terraform can also delete detectors and dashboards. 1. Initial setup \u00b6 Remaining in your Multipass or AWS/EC2 instance from the Smart Agent module, change into the signalfx-jumpstart directory Shell Command cd ~/signalfx-jumpstart The environment variables needed should already be set from Deploy the Smart Agent in K3s . If not, create the following environment variables to use in the Terraform steps below Shell Command export ACCESS_TOKEN= ACCESS TOKEN, from organisation page export REALM= REALM e.g. us1 Initialize Terraform and upgrade to the latest version of the SignalFx Terraform Provider Upgrading the SignalFx Terraform Provider You will need to run this command each time a new version of the SignalFx Terraform Provider is released. You can track the releases on GitHub . Shell Command terraform init -upgrade Output Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.18.6... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.18\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. \u21a9 A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. SignalFx, Terraform Cloud, DNSimple, Cloudflare). \u21a9","title":"Initial Setup"},{"location":"monitoring-as-code/#monitoring-as-code-lab-summary","text":"Use Terraform 1 to manage Observability Suite Dashboards and Detectors Initialize the Terraform SignalFx Provider 2 . Run Terraform to create detectors and dashboards from code using the SignalFx Terraform Provider. See how Terraform can also delete detectors and dashboards.","title":"Monitoring as Code - Lab Summary"},{"location":"monitoring-as-code/#1-initial-setup","text":"Remaining in your Multipass or AWS/EC2 instance from the Smart Agent module, change into the signalfx-jumpstart directory Shell Command cd ~/signalfx-jumpstart The environment variables needed should already be set from Deploy the Smart Agent in K3s . If not, create the following environment variables to use in the Terraform steps below Shell Command export ACCESS_TOKEN= ACCESS TOKEN, from organisation page export REALM= REALM e.g. us1 Initialize Terraform and upgrade to the latest version of the SignalFx Terraform Provider Upgrading the SignalFx Terraform Provider You will need to run this command each time a new version of the SignalFx Terraform Provider is released. You can track the releases on GitHub . Shell Command terraform init -upgrade Output Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.18.6... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.18\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. \u21a9 A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. SignalFx, Terraform Cloud, DNSimple, Cloudflare). \u21a9","title":"1. Initial setup"},{"location":"monitoring-as-code/plan-and-apply/","text":"Plan and Apply Terraform \u00b6 1. Create an execution plan \u00b6 Review the execution plan. Shell Command terraform plan -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" If the plan executes successfully, we can go ahead and apply: 2. Apply actions from execution plan \u00b6 Shell Command terraform apply -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Validate that the detectors were created, under the Alerts \u2192 Detectors . They will be prefixed by the hostname of your instance. To check the prefix value run: Shell Command echo $(hostname) You will see a list of the new detectors and you can search for the prefix that was output from above. 3. Destroy all your hard work \u00b6 Destroy all Detectors and Dashboards that were previously applied. Shell Command terraform destroy -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" Validate all the detectors have been removed by navigating to Alerts \u2192 Detectors","title":"Plan, Apply and Destroy"},{"location":"monitoring-as-code/plan-and-apply/#plan-and-apply-terraform","text":"","title":"Plan and Apply Terraform"},{"location":"monitoring-as-code/plan-and-apply/#1-create-an-execution-plan","text":"Review the execution plan. Shell Command terraform plan -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" If the plan executes successfully, we can go ahead and apply:","title":"1. Create an execution plan"},{"location":"monitoring-as-code/plan-and-apply/#2-apply-actions-from-execution-plan","text":"Shell Command terraform apply -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Validate that the detectors were created, under the Alerts \u2192 Detectors . They will be prefixed by the hostname of your instance. To check the prefix value run: Shell Command echo $(hostname) You will see a list of the new detectors and you can search for the prefix that was output from above.","title":"2. Apply actions from execution plan"},{"location":"monitoring-as-code/plan-and-apply/#3-destroy-all-your-hard-work","text":"Destroy all Detectors and Dashboards that were previously applied. Shell Command terraform destroy -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" Validate all the detectors have been removed by navigating to Alerts \u2192 Detectors","title":"3. Destroy all your hard work"},{"location":"oncall/getting_started/","text":"Initial Setup \u00b6 Aim \u00b6 This module is simply to ensure you have access to the Splunk On-Call UI (formerly known as VictorOps), Splunk Infrastructure Monitoring UI (formerly known as SignalFx) and the EC2 Instance which has been allocated to you. Once you have access to each platform, keep them open for the duration of the workshop as you will be switching between them and the workshop instructions. 1. Activate your Splunk On-Call Login \u00b6 You should have received an invitation to Activate your Splunk On-Call account via e-mail, if you have not already done so, click the Activate Account link and follow the prompts. If you did not receive an invitation it is probably because you already have a Splunk On-Call login, linked to a different organisation. If so login to to that Org, then use the organisation dropdown next to your username in the top left to switch to the Observability Workshop Org. Note If you do not see the Organisation dropdown menu item next to your name with Observability Workshop EMEA that is OK, it simply means you only have access to a single Org so that menu is not visible to you. If you have forgotten your password go to the sign-in page and use the forgotten password link to reset your password. 2. Activate your Splunk Infrastructure Monitoring Login \u00b6 You should have received an invitation to join the Splunk Infrastructure Monitoring - Observability Workshop. If you have not already done so click the JOIN NOW button and follow the prompts to set a password and activate your login. 3. Access your EC2 Instance \u00b6 Splunk has provided you with a dedicated EC2 Instance which you can use during this workshop for triggering Incidents the same way the instructor did during the introductory demo. This VM has Splunk Infrastructure Monitoring deployed and has an associated Detector configured. The Detector will pass Alerts to Splunk On-Call which will then create Incidents and page the on-call user. The welcome e-mail you received providing you all the details for this Workshop contain the instructions for accessing your allocated EC2 Instance. SSH (Mac OS/Linux) \u00b6 Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in your welcome e-mail). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the welcome e-mail. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue with the workshop when instructed to do so by the instructor Putty (Windows users only) \u00b6 If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find the downloads here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and in the Host Name (or IP address) field enter the IP address provided in the welcome e-mail. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your EC2 instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu using the password provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue with the workshop when instructed to do so by the instructor Web Browser (All) \u00b6 If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the welcome e-mail). Once connected, login in as ubuntu and the password is the one provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below: Copy & Paste in browser \u00b6 Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop asks you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue with the workshop when instructed to do so by the instructor Download Putty \u21a9","title":"Initial Setup"},{"location":"oncall/getting_started/#initial-setup","text":"","title":"Initial Setup"},{"location":"oncall/getting_started/#aim","text":"This module is simply to ensure you have access to the Splunk On-Call UI (formerly known as VictorOps), Splunk Infrastructure Monitoring UI (formerly known as SignalFx) and the EC2 Instance which has been allocated to you. Once you have access to each platform, keep them open for the duration of the workshop as you will be switching between them and the workshop instructions.","title":"Aim"},{"location":"oncall/getting_started/#1-activate-your-splunk-on-call-login","text":"You should have received an invitation to Activate your Splunk On-Call account via e-mail, if you have not already done so, click the Activate Account link and follow the prompts. If you did not receive an invitation it is probably because you already have a Splunk On-Call login, linked to a different organisation. If so login to to that Org, then use the organisation dropdown next to your username in the top left to switch to the Observability Workshop Org. Note If you do not see the Organisation dropdown menu item next to your name with Observability Workshop EMEA that is OK, it simply means you only have access to a single Org so that menu is not visible to you. If you have forgotten your password go to the sign-in page and use the forgotten password link to reset your password.","title":"1. Activate your Splunk On-Call Login"},{"location":"oncall/getting_started/#2-activate-your-splunk-infrastructure-monitoring-login","text":"You should have received an invitation to join the Splunk Infrastructure Monitoring - Observability Workshop. If you have not already done so click the JOIN NOW button and follow the prompts to set a password and activate your login.","title":"2. Activate your Splunk Infrastructure Monitoring Login"},{"location":"oncall/getting_started/#3-access-your-ec2-instance","text":"Splunk has provided you with a dedicated EC2 Instance which you can use during this workshop for triggering Incidents the same way the instructor did during the introductory demo. This VM has Splunk Infrastructure Monitoring deployed and has an associated Detector configured. The Detector will pass Alerts to Splunk On-Call which will then create Incidents and page the on-call user. The welcome e-mail you received providing you all the details for this Workshop contain the instructions for accessing your allocated EC2 Instance.","title":"3. Access your EC2 Instance"},{"location":"oncall/getting_started/#ssh-mac-oslinux","text":"Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in your welcome e-mail). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the welcome e-mail. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue with the workshop when instructed to do so by the instructor","title":"SSH (Mac OS/Linux)"},{"location":"oncall/getting_started/#putty-windows-users-only","text":"If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find the downloads here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and in the Host Name (or IP address) field enter the IP address provided in the welcome e-mail. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your EC2 instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu using the password provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue with the workshop when instructed to do so by the instructor","title":"Putty (Windows users only)"},{"location":"oncall/getting_started/#web-browser-all","text":"If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the welcome e-mail). Once connected, login in as ubuntu and the password is the one provided in the welcome e-mail. Once you are connected successfully you should see a screen similar to the one below:","title":"Web Browser (All)"},{"location":"oncall/getting_started/#copy-paste-in-browser","text":"Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop asks you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue with the workshop when instructed to do so by the instructor Download Putty \u21a9","title":"Copy &amp; Paste in browser"},{"location":"oncall/getting_started/escalation/","text":"Configure Escalation Policies \u00b6 Aim \u00b6 Escalation policies determine who is actually on-call for a given team and are the link to utilizing any rotations that have been created. The aim of this module is for you to create three different Escalation Policies to demonstrate a number of different features and operating models. The instructor will start by explaining the concepts before you proceed with the configuration. Navigate to the Escalation Polices tab on the Teams sub menu, you should have no existing Polices so we need to create some. We are going to create the following Polices to cover off three typical use cases. 1. 24/7 Policy \u00b6 Click Add Escalation Policy Policy Name: 24/7 Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Senior SRE Escalation Click Save 2. Primary Policy \u00b6 Click Add Escalation Policy Policy Name: Primary Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Follow the Sun Support - Business Hours Click Add Step Step 2 If still unacked after 15 minutes Notify the next user(s) in the current on-duty shift \u2192 Follow the Sun Support - Business Hours Click Add Step Step 3 If still unacked after 15 more minutes Execute Policy \u2192 [Your Team Name] : 24/7 Click Save 3. Waiting Room Policy \u00b6 Click Add Escalation Policy Policy Name: Waiting Room Step 1 If still unacked after 10 more minutes Execute Policy \u2192 [Your Team Name] : Primary Click Save You should now have the following three escalation polices: You may have noticed that when we created each policy there was the following warning message: Warning There are no routing keys for this policy - it will only receive incidents via manual reroute or when on another escalation policy This is because there are no Routing Keys linked to these Escalation Polices, so now that we have these polices configured we can create the Routing Keys and link them to our Polices.. Continue and also complete the Creating Routing Keys module.","title":"Configure Escalation Policies"},{"location":"oncall/getting_started/escalation/#configure-escalation-policies","text":"","title":"Configure Escalation Policies"},{"location":"oncall/getting_started/escalation/#aim","text":"Escalation policies determine who is actually on-call for a given team and are the link to utilizing any rotations that have been created. The aim of this module is for you to create three different Escalation Policies to demonstrate a number of different features and operating models. The instructor will start by explaining the concepts before you proceed with the configuration. Navigate to the Escalation Polices tab on the Teams sub menu, you should have no existing Polices so we need to create some. We are going to create the following Polices to cover off three typical use cases.","title":"Aim"},{"location":"oncall/getting_started/escalation/#1-247-policy","text":"Click Add Escalation Policy Policy Name: 24/7 Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Senior SRE Escalation Click Save","title":"1. 24/7 Policy"},{"location":"oncall/getting_started/escalation/#2-primary-policy","text":"Click Add Escalation Policy Policy Name: Primary Step 1 Immediately Notify the on-duty user(s) in rotation \u2192 Follow the Sun Support - Business Hours Click Add Step Step 2 If still unacked after 15 minutes Notify the next user(s) in the current on-duty shift \u2192 Follow the Sun Support - Business Hours Click Add Step Step 3 If still unacked after 15 more minutes Execute Policy \u2192 [Your Team Name] : 24/7 Click Save","title":"2. Primary Policy"},{"location":"oncall/getting_started/escalation/#3-waiting-room-policy","text":"Click Add Escalation Policy Policy Name: Waiting Room Step 1 If still unacked after 10 more minutes Execute Policy \u2192 [Your Team Name] : Primary Click Save You should now have the following three escalation polices: You may have noticed that when we created each policy there was the following warning message: Warning There are no routing keys for this policy - it will only receive incidents via manual reroute or when on another escalation policy This is because there are no Routing Keys linked to these Escalation Polices, so now that we have these polices configured we can create the Routing Keys and link them to our Polices.. Continue and also complete the Creating Routing Keys module.","title":"3. Waiting Room Policy"},{"location":"oncall/getting_started/rotations/","text":"Configure Rotations \u00b6 Aim \u00b6 A rotation is a recurring schedule, that consists of one or more shifts, with members who rotate through a shift. The aim of this module is for you to configure two example Rotations, and assign Team Members to the Rotations. Navigate to the Rotations tab on the Teams sub menu, you should have no existing Rotations so we need to create some. The 1st Rotation you will create is for a follow the sun support pattern where the members of each shift provide cover during their normal working hours within their time zone. The 2nd will be a Rotation used to provide escalation support by more experienced senior members of the team, based on a 24/7, 1 week shift pattern. 1. Follow the Sun Support - Business Hours \u00b6 Click Add Rotation Enter a name of \" Follow the Sun Support - Business Hours \" and Select Partial day from the three available shift templates. Enter a Shift name of \" Asia \" Time Zone set to \" Asia/Tokyo \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Rotation You will now be prompted to add Members to this shift; add the Asia members who are jimhalpert, lydia and marie , but only if you're using the Splunk provided environment for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add an 2nd shift for Europe by again clicking +Add a shift \u2192 Partial Day Enter a Shift name of \" Europe \" Time Zone set to \" Europe/London \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the Europe members who are duanechow, gomez and heisenberg , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add a 3rd shift for West Coast USA by again clicking +Add a shift - Partial Day Enter a Shift name of \" West Coast \" Time Zone set to \" US/Pacific \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the West Coast members who are maximo, michaelscott and tuco , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. The first user added will be the 'current' user for that shift. You can re-order the shifts by simply dragging the users up and down, and you can change the current user by clicking Set Current on an alternate user You will now have three different Shift patterns, that provide cover 24hr hours, Mon - Fri, but with no cover at weekends. We will now add another Rotation for our Senior SRE Escalation cover. 2. Senior SRE Escalation \u00b6 Click Add Rotation Enter a name of \" Senior SRE Escalation \" Select 24/7 from the three available shift templates Enter a Shift name of \" Senior SRE Escalation \" Time Zone set to \" Asia/Tokyo \" Handoff happens every \" 7 days at 9.00am \" The next handoff happens [select the next Monday from the date picker] Click Save Rotation You will again be prompted to add Members to this shift; add the 24/7 members who are jackwelker, hank and pambeesly , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Please wait for the instructor before proceeding to the Configuring Escalation Policies module.","title":"Configure Rotations"},{"location":"oncall/getting_started/rotations/#configure-rotations","text":"","title":"Configure Rotations"},{"location":"oncall/getting_started/rotations/#aim","text":"A rotation is a recurring schedule, that consists of one or more shifts, with members who rotate through a shift. The aim of this module is for you to configure two example Rotations, and assign Team Members to the Rotations. Navigate to the Rotations tab on the Teams sub menu, you should have no existing Rotations so we need to create some. The 1st Rotation you will create is for a follow the sun support pattern where the members of each shift provide cover during their normal working hours within their time zone. The 2nd will be a Rotation used to provide escalation support by more experienced senior members of the team, based on a 24/7, 1 week shift pattern.","title":"Aim"},{"location":"oncall/getting_started/rotations/#1-follow-the-sun-support-business-hours","text":"Click Add Rotation Enter a name of \" Follow the Sun Support - Business Hours \" and Select Partial day from the three available shift templates. Enter a Shift name of \" Asia \" Time Zone set to \" Asia/Tokyo \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Rotation You will now be prompted to add Members to this shift; add the Asia members who are jimhalpert, lydia and marie , but only if you're using the Splunk provided environment for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add an 2nd shift for Europe by again clicking +Add a shift \u2192 Partial Day Enter a Shift name of \" Europe \" Time Zone set to \" Europe/London \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the Europe members who are duanechow, gomez and heisenberg , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Now add a 3rd shift for West Coast USA by again clicking +Add a shift - Partial Day Enter a Shift name of \" West Coast \" Time Zone set to \" US/Pacific \" Each user is on duty from \" Monday through Friday from 9.00am to 5.00pm \" Handoff happens every \" 5 days \" The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the West Coast members who are maximo, michaelscott and tuco , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. The first user added will be the 'current' user for that shift. You can re-order the shifts by simply dragging the users up and down, and you can change the current user by clicking Set Current on an alternate user You will now have three different Shift patterns, that provide cover 24hr hours, Mon - Fri, but with no cover at weekends. We will now add another Rotation for our Senior SRE Escalation cover.","title":"1. Follow the Sun Support - Business Hours"},{"location":"oncall/getting_started/rotations/#2-senior-sre-escalation","text":"Click Add Rotation Enter a name of \" Senior SRE Escalation \" Select 24/7 from the three available shift templates Enter a Shift name of \" Senior SRE Escalation \" Time Zone set to \" Asia/Tokyo \" Handoff happens every \" 7 days at 9.00am \" The next handoff happens [select the next Monday from the date picker] Click Save Rotation You will again be prompted to add Members to this shift; add the 24/7 members who are jackwelker, hank and pambeesly , but only if you're using the Observability Workshop Org for this workshop. If you're using your own Organisation refer to the specific list provided separately. Please wait for the instructor before proceeding to the Configuring Escalation Policies module.","title":"2. Senior SRE Escalation"},{"location":"oncall/getting_started/routing/","text":"Create Routing Keys \u00b6 Aim \u00b6 Routing Keys map the incoming alert messages from your monitoring system to an Escalation Policy which in turn sends the notifications to the appropriate team. Note that routing keys are case insensitive and should only be composed of letters, numbers, hyphens, and underscores. The aim of this module is for you to create some routing keys and then link them to your Escalation Policies you have created in the previous exercise. 1. Instance ID \u00b6 Each participant requires a unique Routing Key so we use the Hostname of the EC2 Instance you were allocated. We are only doing this to ensure your Routing Key is unique and we know all Hostnames are unique. In a production deployment the Routing Key would typically reflect the name of a System or Service being monitored, or a Team such as 1st Line Support etc. Your welcome e-mail informed you of the details of your EC2 Instance that has been provided for you to use during this workshop and you should have logged into this as part of the 1st exercise. The e-mail also contained the Hostname of the Instance, but you can also obtain it from the Instance directly. To get your Hostname from within the shell session connected to your Instance run the following command: Shell Command echo ${HOSTNAME} Example Output zevn It is very important that when creating the Routing Keys you use the 4 letter hostname allocated to you as a Detector has been configured within Splunk Infrastructure Monitoring using this hostname, so any deviation will cause future exercises to fail. 2 Create Routing Keys \u00b6 Navigate to Settings on the main menu bar, you should now be at the Routing Keys page. You are going to create the following two Routing Keys using the naming conventions listed in the following table, but replacing HOSTNAME with the value from above and replace TEAM_NAME with the team you were allocated or created earlier. Routing Key Escalation Policies HOSTNAME _PRI TEAM_NAME : Primary HOSTNAME _WR TEAM_NAME : Waiting Room There will probably already be a number of Routing Keys configured, but to add a new one simply scroll to the bottom of the page and then click Add Key In the left hand box, enter the name for the key as per the table above. In the Routing Key column, select your Teams Primary policy from the drop down in the Escalation Polices column. You can start typing your Team Name to filter the results. Note If there are a large number of participants on the workshop, resulting in an unusually large number of Escalation Policies sometimes the search filter does not list all the Policies under your Team Name. If this happens instead of using the search feature, simply scroll down to your team name, all the policies will then be listed. Repeat the above steps for both Keys, xxxx_PRI and xxxx_WR , mapping them to your Teams Primary and Waiting Room policies. You should now have two Routing Keys configured, similar to the following: Tip You can assign a Routing Key to multiple Escalation Policies if required by simply selecting more from the list If you now navigate back to Teams \u2192 [Your Team Name] \u2192 Escalation Policies and look at the settings for your Primary and Waiting Room polices you will see that these now have Routes assigned to them. The 24/7 policy does not have a Route assigned as this will only be triggered via an Execute Policy escalation from the Primary policy. Please wait for the instructor before proceeding to the Incident Lifecycle/Overview module.","title":"Creating Routing keys"},{"location":"oncall/getting_started/routing/#create-routing-keys","text":"","title":"Create Routing Keys"},{"location":"oncall/getting_started/routing/#aim","text":"Routing Keys map the incoming alert messages from your monitoring system to an Escalation Policy which in turn sends the notifications to the appropriate team. Note that routing keys are case insensitive and should only be composed of letters, numbers, hyphens, and underscores. The aim of this module is for you to create some routing keys and then link them to your Escalation Policies you have created in the previous exercise.","title":"Aim"},{"location":"oncall/getting_started/routing/#1-instance-id","text":"Each participant requires a unique Routing Key so we use the Hostname of the EC2 Instance you were allocated. We are only doing this to ensure your Routing Key is unique and we know all Hostnames are unique. In a production deployment the Routing Key would typically reflect the name of a System or Service being monitored, or a Team such as 1st Line Support etc. Your welcome e-mail informed you of the details of your EC2 Instance that has been provided for you to use during this workshop and you should have logged into this as part of the 1st exercise. The e-mail also contained the Hostname of the Instance, but you can also obtain it from the Instance directly. To get your Hostname from within the shell session connected to your Instance run the following command: Shell Command echo ${HOSTNAME} Example Output zevn It is very important that when creating the Routing Keys you use the 4 letter hostname allocated to you as a Detector has been configured within Splunk Infrastructure Monitoring using this hostname, so any deviation will cause future exercises to fail.","title":"1. Instance ID"},{"location":"oncall/getting_started/routing/#2-create-routing-keys","text":"Navigate to Settings on the main menu bar, you should now be at the Routing Keys page. You are going to create the following two Routing Keys using the naming conventions listed in the following table, but replacing HOSTNAME with the value from above and replace TEAM_NAME with the team you were allocated or created earlier. Routing Key Escalation Policies HOSTNAME _PRI TEAM_NAME : Primary HOSTNAME _WR TEAM_NAME : Waiting Room There will probably already be a number of Routing Keys configured, but to add a new one simply scroll to the bottom of the page and then click Add Key In the left hand box, enter the name for the key as per the table above. In the Routing Key column, select your Teams Primary policy from the drop down in the Escalation Polices column. You can start typing your Team Name to filter the results. Note If there are a large number of participants on the workshop, resulting in an unusually large number of Escalation Policies sometimes the search filter does not list all the Policies under your Team Name. If this happens instead of using the search feature, simply scroll down to your team name, all the policies will then be listed. Repeat the above steps for both Keys, xxxx_PRI and xxxx_WR , mapping them to your Teams Primary and Waiting Room policies. You should now have two Routing Keys configured, similar to the following: Tip You can assign a Routing Key to multiple Escalation Policies if required by simply selecting more from the list If you now navigate back to Teams \u2192 [Your Team Name] \u2192 Escalation Policies and look at the settings for your Primary and Waiting Room polices you will see that these now have Routes assigned to them. The 24/7 policy does not have a Route assigned as this will only be triggered via an Execute Policy escalation from the Primary policy. Please wait for the instructor before proceeding to the Incident Lifecycle/Overview module.","title":"2 Create Routing Keys"},{"location":"oncall/getting_started/team/","text":"Teams \u00b6 Aim \u00b6 The aim of this module is for you to complete the first step of Team configuration by adding users to your Team. 1. Find your Team \u00b6 Navigate to the Teams tab on the main toolbar, you should find you that a Team has been created for you as part of the workshop pre-setup and you would have been informed of your Team Name via e-mail. If you have found your pre-configured Team, skip Step 2. and proceed to Step 3. Configure Your Team However if you cannot find you allocated Team, you will need to create a new one, so proceed with Step 2. Create Team 2. Create Team \u00b6 Only complete this step if you cannot find your pre-allocated Team as detailed in your workshop e-mail. Select Add Team , then enter your allocated team name, this will typically be in the format of \"AttendeeID Workshop\" and then save by clicking the Add Team button. 3. Configure Your Team \u00b6 You now need to add other users to your team. If you are running this workshop using the Splunk provided environment, the following accounts are available for testing. If you are running this lab in your own environment, you will have been provided a list of usernames you can use in place of the table below. These users are dummy accounts who will not receive notifications when they are on call. Name Username Shift Duane Chow duanechow Europe Steven Gomez gomez Europe Walter White heisenberg Europe Jim Halpert jimhalpert Asia Lydia Rodarte-Quayle lydia Asia Marie Schrader marie Asia Maximo Arciniega maximo West Coast Michael Scott michaelscott West Coast Tuco Salamanca tuco West Coast Jack Welker jackwelker 24/7 Hank Schrader hank 24/7 Pam Beesly pambeesly 24/7 Add the users to your team, using either the above list or the alternate one provided to you. The value in the Shift column can be ignored for now, but will be required for a later step. Click the Invite User button on the right hand side, then either start typing the usernames (this will filter the list), or copy and paste them into the dialogue box. Once all users are added to the list click the Add User button. To make a team member a Team Admin, simply click the icon in the right hand column, pick any user and make them an Admin. Tip For large team management you can use the APIs to streamline this process Continue and also complete the Configure Rotations module.","title":"Teams"},{"location":"oncall/getting_started/team/#teams","text":"","title":"Teams"},{"location":"oncall/getting_started/team/#aim","text":"The aim of this module is for you to complete the first step of Team configuration by adding users to your Team.","title":"Aim"},{"location":"oncall/getting_started/team/#1-find-your-team","text":"Navigate to the Teams tab on the main toolbar, you should find you that a Team has been created for you as part of the workshop pre-setup and you would have been informed of your Team Name via e-mail. If you have found your pre-configured Team, skip Step 2. and proceed to Step 3. Configure Your Team However if you cannot find you allocated Team, you will need to create a new one, so proceed with Step 2. Create Team","title":"1. Find your Team"},{"location":"oncall/getting_started/team/#2-create-team","text":"Only complete this step if you cannot find your pre-allocated Team as detailed in your workshop e-mail. Select Add Team , then enter your allocated team name, this will typically be in the format of \"AttendeeID Workshop\" and then save by clicking the Add Team button.","title":"2. Create Team"},{"location":"oncall/getting_started/team/#3-configure-your-team","text":"You now need to add other users to your team. If you are running this workshop using the Splunk provided environment, the following accounts are available for testing. If you are running this lab in your own environment, you will have been provided a list of usernames you can use in place of the table below. These users are dummy accounts who will not receive notifications when they are on call. Name Username Shift Duane Chow duanechow Europe Steven Gomez gomez Europe Walter White heisenberg Europe Jim Halpert jimhalpert Asia Lydia Rodarte-Quayle lydia Asia Marie Schrader marie Asia Maximo Arciniega maximo West Coast Michael Scott michaelscott West Coast Tuco Salamanca tuco West Coast Jack Welker jackwelker 24/7 Hank Schrader hank 24/7 Pam Beesly pambeesly 24/7 Add the users to your team, using either the above list or the alternate one provided to you. The value in the Shift column can be ignored for now, but will be required for a later step. Click the Invite User button on the right hand side, then either start typing the usernames (this will filter the list), or copy and paste them into the dialogue box. Once all users are added to the list click the Add User button. To make a team member a Team Admin, simply click the icon in the right hand column, pick any user and make them an Admin. Tip For large team management you can use the APIs to streamline this process Continue and also complete the Configure Rotations module.","title":"3. Configure Your Team"},{"location":"oncall/getting_started/user_profile/","text":"User Profile \u00b6 Aim \u00b6 The aim of this module is for you to configure your personal profile which controls how you will be notified by Splunk On-Call whenever you get paged. 1. Contact Methods \u00b6 Switch to the Splunk On-Call UI and click on your login name in the top right hand corner and chose Profile from the drop down. Confirm your contact methods are listed correctly and add any additional phone numbers and e-mail address you wish to use. 2. Mobile Devices \u00b6 To install the Splunk On-Call app for your smartphone search your phones App Store for Splunk On-Call to find the appropriate version of the app. The publisher should be listed as VictorOps Inc. Configuration help guides are available: Apple Android Install the App and login, then refresh the Profile page and your device should now be listed under the devices section. Click the Test push notification button and confirm you receive the test message. 3. Personal Calendar \u00b6 This link will enable you to sync your on-call schedule with your calendar, however as you do not have any allocated shifts yet this will currently be empty. You can add it to your calendar by copying the link into your preferred application and setting it up as a new subscription. 4. Paging Policies \u00b6 Paging Polices specify how you will be contacted when on-call. The Primary Paging Policy will have defaulted to sending you an SMS assuming you added your phone number when activating your account. We will now configure this policy into a three tier multi-stage policy similar to the image below. Step 1: Send a push notification \u00b6 Click the edit policy button in the top right corner for the Primary Paging Policy. Send a push notification to all my devices Execute the next step if I have not responded within 5 minutes Click Add a Step Step 2: Send an e-mail \u00b6 Send an e-mail to [your email address] Execute the next step if I have not responded within 5 minutes Click Add a Step Step 3: Call your number \u00b6 Every 5 minutes until we have reached you Make a phone call to [your phone number] Click Save to save the policy. When you are on-call or in the escalation path of an incident, you will receive notifications in this order following these time delays. To cease the paging you must acknowledge the incident. Acknowledgements can occur in one of the following ways: Expanding the Push Notification on your device and selecting Acknowledge Responding to the SMS with the 5 digit code included Pressing 4 during the Phone Call Slack Button For more information on Notification Types, see here . 5. Custom Paging Policies \u00b6 Custom paging polices enable you to override the primary policy based on the time and day of the week. A good example would be get the system to immediately phone you whenever you get a page during the evening or weekends as this is more likely to get your attention than a push notification. Create a new Custom Policy by clicking Add a Policy and configure with the following settings: 5.1 Custom evening policy \u00b6 Policy Name: Evening Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: All 7 Days Timezone Between 7pm and 9am Click Save to save the policy then add one more. 5.2 Custom weekend policy \u00b6 Policy Name: Weekend Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: Sat & Sun Timezone Between 9am and 7pm Click Save to save the policy. These custom paging policies will be used during the specified times in place of the Primary Policy. However admins do have the ability to ignore these custom policies, and we will highlight how this is achieved in a later module. The final option here is the setting for Recovery Notifications. As these are typically low priority simply sending you an email or a push notification are the typical settings used. Your profile is now fully configured using these example configurations. Organizations will have different views on how profiles should be configured and will typically issue guidelines for paging policies and times between escalations etc. Please wait for the instructor before proceeding to the Teams module.","title":"User Profile"},{"location":"oncall/getting_started/user_profile/#user-profile","text":"","title":"User Profile"},{"location":"oncall/getting_started/user_profile/#aim","text":"The aim of this module is for you to configure your personal profile which controls how you will be notified by Splunk On-Call whenever you get paged.","title":"Aim"},{"location":"oncall/getting_started/user_profile/#1-contact-methods","text":"Switch to the Splunk On-Call UI and click on your login name in the top right hand corner and chose Profile from the drop down. Confirm your contact methods are listed correctly and add any additional phone numbers and e-mail address you wish to use.","title":"1. Contact Methods"},{"location":"oncall/getting_started/user_profile/#2-mobile-devices","text":"To install the Splunk On-Call app for your smartphone search your phones App Store for Splunk On-Call to find the appropriate version of the app. The publisher should be listed as VictorOps Inc. Configuration help guides are available: Apple Android Install the App and login, then refresh the Profile page and your device should now be listed under the devices section. Click the Test push notification button and confirm you receive the test message.","title":"2. Mobile Devices"},{"location":"oncall/getting_started/user_profile/#3-personal-calendar","text":"This link will enable you to sync your on-call schedule with your calendar, however as you do not have any allocated shifts yet this will currently be empty. You can add it to your calendar by copying the link into your preferred application and setting it up as a new subscription.","title":"3. Personal Calendar"},{"location":"oncall/getting_started/user_profile/#4-paging-policies","text":"Paging Polices specify how you will be contacted when on-call. The Primary Paging Policy will have defaulted to sending you an SMS assuming you added your phone number when activating your account. We will now configure this policy into a three tier multi-stage policy similar to the image below.","title":"4. Paging Policies"},{"location":"oncall/getting_started/user_profile/#step-1-send-a-push-notification","text":"Click the edit policy button in the top right corner for the Primary Paging Policy. Send a push notification to all my devices Execute the next step if I have not responded within 5 minutes Click Add a Step","title":"Step 1: Send a push notification"},{"location":"oncall/getting_started/user_profile/#step-2-send-an-e-mail","text":"Send an e-mail to [your email address] Execute the next step if I have not responded within 5 minutes Click Add a Step","title":"Step 2: Send an e-mail"},{"location":"oncall/getting_started/user_profile/#step-3-call-your-number","text":"Every 5 minutes until we have reached you Make a phone call to [your phone number] Click Save to save the policy. When you are on-call or in the escalation path of an incident, you will receive notifications in this order following these time delays. To cease the paging you must acknowledge the incident. Acknowledgements can occur in one of the following ways: Expanding the Push Notification on your device and selecting Acknowledge Responding to the SMS with the 5 digit code included Pressing 4 during the Phone Call Slack Button For more information on Notification Types, see here .","title":"Step 3: Call your number"},{"location":"oncall/getting_started/user_profile/#5-custom-paging-policies","text":"Custom paging polices enable you to override the primary policy based on the time and day of the week. A good example would be get the system to immediately phone you whenever you get a page during the evening or weekends as this is more likely to get your attention than a push notification. Create a new Custom Policy by clicking Add a Policy and configure with the following settings:","title":"5. Custom Paging Policies"},{"location":"oncall/getting_started/user_profile/#51-custom-evening-policy","text":"Policy Name: Evening Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: All 7 Days Timezone Between 7pm and 9am Click Save to save the policy then add one more.","title":"5.1 Custom evening policy"},{"location":"oncall/getting_started/user_profile/#52-custom-weekend-policy","text":"Policy Name: Weekend Every 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: Sat & Sun Timezone Between 9am and 7pm Click Save to save the policy. These custom paging policies will be used during the specified times in place of the Primary Policy. However admins do have the ability to ignore these custom policies, and we will highlight how this is achieved in a later module. The final option here is the setting for Recovery Notifications. As these are typically low priority simply sending you an email or a push notification are the typical settings used. Your profile is now fully configured using these example configurations. Organizations will have different views on how profiles should be configured and will typically issue guidelines for paging policies and times between escalations etc. Please wait for the instructor before proceeding to the Teams module.","title":"5.2 Custom weekend policy"},{"location":"oncall/incident_lifecycle/","text":"UI Overview \u00b6 Aim \u00b6 The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features. 1. Timeline \u00b6 The aim of Splunk On-Call is to \"Make On Call Suck Less\" , and it does this by getting the critical data, to the right people, at the right time. The key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting. Login to the Splunk On-Call UI and select the Timeline tab on the main menu bar, you should have a screen similar to the following image: 2. People \u00b6 On the left we have the People section with the Teams and Users sub tabs. On the Teams tab, click on All Teams then expand [Your Teamname] . Users with the Splunk On-Call Logo against their name are currently on call. Here you can see who is on call within a particular Team, or across all Teams via Users \u2192 On-Call . If you click into one of the currently on call users, you can see their status. It shows which Rotation they are on call for, when their current Shift ends and their next Shift starts (times are displayed in your timezone), what contact methods they have and which Teams they belong to (dummy users such as Hank do not have Contact Methods configured). 3. Timeline \u00b6 In the centre Timeline section you get a realtime view of what is happening within your environment with the newest messages at the top. Here you can quickly post update messages to make your colleagues aware of important developments etc. You can filter the view using the buttons on the top toolbar showing only update messages, GitHub integrations, or apply more advanced filters. Lets change the Filters settings to streamline your view. Click the Filters button then within the Routing Keys tab change the Show setting from all routing keys to selected routing keys . Change the My Keys value to all and the Other Keys value to selected and deselect all keys under the Other Keys section. Click anywhere outside of the dialogue box to close it. You will probably now have a much simpler view as you will not currently have Incidents created using your Routing Keys, so you are left with the other types of messages that the Timeline can display. Click on Filters again, but this time switch to the Message Types tab. Here you control the types of messages that are displayed. For example, deselect On-call Changes and Escalations , this will reduce the amount of messages displayed. 4. Incidents \u00b6 On the right we have the Incidents section. Here we get a list of all the incidents within the platform, or we view a more specific list such as incidents you are specifically assigned to, or for any of the Teams you are a member of. Select the Team Incidents tab you should find that the Triggered , Acknowledged & Resolved tabs are currently all empty as you have had no incidents logged. Let's change that by generating your first incident! Continue with the Create Incidents module.","title":"Overview"},{"location":"oncall/incident_lifecycle/#ui-overview","text":"","title":"UI Overview"},{"location":"oncall/incident_lifecycle/#aim","text":"The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features.","title":"Aim"},{"location":"oncall/incident_lifecycle/#1-timeline","text":"The aim of Splunk On-Call is to \"Make On Call Suck Less\" , and it does this by getting the critical data, to the right people, at the right time. The key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting. Login to the Splunk On-Call UI and select the Timeline tab on the main menu bar, you should have a screen similar to the following image:","title":"1. Timeline"},{"location":"oncall/incident_lifecycle/#2-people","text":"On the left we have the People section with the Teams and Users sub tabs. On the Teams tab, click on All Teams then expand [Your Teamname] . Users with the Splunk On-Call Logo against their name are currently on call. Here you can see who is on call within a particular Team, or across all Teams via Users \u2192 On-Call . If you click into one of the currently on call users, you can see their status. It shows which Rotation they are on call for, when their current Shift ends and their next Shift starts (times are displayed in your timezone), what contact methods they have and which Teams they belong to (dummy users such as Hank do not have Contact Methods configured).","title":"2. People"},{"location":"oncall/incident_lifecycle/#3-timeline","text":"In the centre Timeline section you get a realtime view of what is happening within your environment with the newest messages at the top. Here you can quickly post update messages to make your colleagues aware of important developments etc. You can filter the view using the buttons on the top toolbar showing only update messages, GitHub integrations, or apply more advanced filters. Lets change the Filters settings to streamline your view. Click the Filters button then within the Routing Keys tab change the Show setting from all routing keys to selected routing keys . Change the My Keys value to all and the Other Keys value to selected and deselect all keys under the Other Keys section. Click anywhere outside of the dialogue box to close it. You will probably now have a much simpler view as you will not currently have Incidents created using your Routing Keys, so you are left with the other types of messages that the Timeline can display. Click on Filters again, but this time switch to the Message Types tab. Here you control the types of messages that are displayed. For example, deselect On-call Changes and Escalations , this will reduce the amount of messages displayed.","title":"3. Timeline"},{"location":"oncall/incident_lifecycle/#4-incidents","text":"On the right we have the Incidents section. Here we get a list of all the incidents within the platform, or we view a more specific list such as incidents you are specifically assigned to, or for any of the Teams you are a member of. Select the Team Incidents tab you should find that the Triggered , Acknowledged & Resolved tabs are currently all empty as you have had no incidents logged. Let's change that by generating your first incident! Continue with the Create Incidents module.","title":"4. Incidents"},{"location":"oncall/incident_lifecycle/create_incidents/","text":"Create Incidents \u00b6 Aim \u00b6 The aim of this module is for you to place yourself 'On-Call' then generate an Incident using the supplied EC2 Instance so you can then work through the lifecycle of an Incident. 1. On-Call \u00b6 Before generating any incidents you should assign yourself to the current Shift within your Follow the Sun Support - Business Hours Rotation and also place yourself On-Call . Click on the Schedule link within your Team in the People section on the left Or navigate to Teams \u2192 [Your Team] \u2192 Rotations Expand the Follow the Sun Support - Business Hours Rotation Click on the Manage members icon (the figures) for the current active shift depending on your timezone Use the Select a user to add... dropdown to add yourself to the shift Then click on Set Current to make yourself the current on-call user within the shift You should now get a Push Notification to your phone informing you that You Are Now On-Call 2. Trigger Alert \u00b6 Switch back to your shell session connected to your EC2 Instance; all of the following commands will be executed from your Instance. Force the CPU to spike to 100% by running the following command: Shell Command openssl speed -multi $(grep -ci processor /proc/cpuinfo) Output Forked child 0 +DT:md4:3:16 +R:19357020:md4:3.000000 +DT:md4:3:64 +R:14706608:md4:3.010000 +DT:md4:3:256 +R:8262960:md4:3.000000 +DT:md4:3:1024 This will result in an Alert being generated by Splunk Infrastructure Monitoring which in turn will generate an Incident within Splunk On-Call within a maximum of 10 seconds. This is the default polling time for the SignalFx Agent installed on your Instance (note it can be reduced to 1 second). Continue with the Manage Incidents module.","title":"Create Incidents"},{"location":"oncall/incident_lifecycle/create_incidents/#create-incidents","text":"","title":"Create Incidents"},{"location":"oncall/incident_lifecycle/create_incidents/#aim","text":"The aim of this module is for you to place yourself 'On-Call' then generate an Incident using the supplied EC2 Instance so you can then work through the lifecycle of an Incident.","title":"Aim"},{"location":"oncall/incident_lifecycle/create_incidents/#1-on-call","text":"Before generating any incidents you should assign yourself to the current Shift within your Follow the Sun Support - Business Hours Rotation and also place yourself On-Call . Click on the Schedule link within your Team in the People section on the left Or navigate to Teams \u2192 [Your Team] \u2192 Rotations Expand the Follow the Sun Support - Business Hours Rotation Click on the Manage members icon (the figures) for the current active shift depending on your timezone Use the Select a user to add... dropdown to add yourself to the shift Then click on Set Current to make yourself the current on-call user within the shift You should now get a Push Notification to your phone informing you that You Are Now On-Call","title":"1. On-Call"},{"location":"oncall/incident_lifecycle/create_incidents/#2-trigger-alert","text":"Switch back to your shell session connected to your EC2 Instance; all of the following commands will be executed from your Instance. Force the CPU to spike to 100% by running the following command: Shell Command openssl speed -multi $(grep -ci processor /proc/cpuinfo) Output Forked child 0 +DT:md4:3:16 +R:19357020:md4:3.000000 +DT:md4:3:64 +R:14706608:md4:3.010000 +DT:md4:3:256 +R:8262960:md4:3.000000 +DT:md4:3:1024 This will result in an Alert being generated by Splunk Infrastructure Monitoring which in turn will generate an Incident within Splunk On-Call within a maximum of 10 seconds. This is the default polling time for the SignalFx Agent installed on your Instance (note it can be reduced to 1 second). Continue with the Manage Incidents module.","title":"2. Trigger Alert"},{"location":"oncall/incident_lifecycle/manage_incidents/","text":"Managing Incidents \u00b6 1. Acknowledge \u00b6 Use your Splunk On-Call App on your phone to acknowledge the Incident by clicking on the push notification ... ...to open the alert in the Splunk On-Call mobile app, then clicking on either the single tick in the top right hand corner, or the Acknowledge link to acknowledge the incident and stop the escalation process. The will then transform into a , and the status will change from TRIGGERED to ACKNOWLEDGED . Triggered Incident Acknowledge Incident 2. Details and Annotations \u00b6 Still on your phone, select the Alert Details tab. Then on the Web UI, navigate back to Timeline , select Team Incidents on the right, then select Acknowledged and click into the new Incident . You should now have the Details tab displayed on both your Phone and the Web UI. Notice how they both show the exact same information. Now select the Annotations tab on both the Phone and the Web UI, you should have a Graph displayed in the UI which is generated by SignalFx. On your phone you should get the same image displayed (sometimes it's a simple hyperlink depending on the image size) Splunk On-Call is a 'Mobile First' platform meaning the phone app is full functionality and you can manage an incident directly from your phone. For the remainder of this module we will focus on the Web UI however please spend some time later exploring the phone app features. 3. Link to Alerting System \u00b6 Sticking with the Web UI, click the 2. Alert Details in SignalFx link. This will open a new browser tab and take you directly to the Alert within Splunk Infrastructure Monitoring where you could then progress your troubleshooting using the powerful tools built into its UI. However, we are focussing on Splunk On-Call so close this tab and return to the Splunk On-Call UI. 4. Similar Incidents \u00b6 What if Splunk On-Call could identify previous incidents within the system which may give you a clue to the best way to tackle this incident. The Similar Incidents tab does exactly that, surfacing previous incidents allowing you to look at them and see what actions were taken to resolve them, actions which could be easily repeated for this incident. 5. War Room Dashboard \u00b6 At the top right in the UI are a number of icons that allow quick access to various actions, click on the far right one which will open this Incident in a War Room Dashboard (opens in new tab). 5.1 Timeline \u00b6 With the Incident expanded, you can see on the right we have a Time Line view where you can add messages and see the history of previous alerts and interactions. 5.2 Add Responders \u00b6 On the far left you have the option of allocating additional resources to this incident by clicking on the Add Responders link. This allows you build a virtual team specific to this incident by adding other Teams or individual Users, and also share details of a Conference Bridge where you can all get together and collaborate. Once the system has built up some incident data history, it will use Machine Learning to suggest Teams and Users who have historically worked on similar incidents, as they may be best placed to help resolve this incident quickly. You can select different Teams and/or Users and also choose from a pre-configured conference bridge, or populate the details of a new bridge from your preferred provider. We do not need to add any Responders in this exercise so close the Add Responders dialogue by clicking Cancel . 5.3 Reroute \u00b6 If it's decided that maybe the incident could be better dealt with by a different Team, the call can be Rerouted by clicking the Reroute Button at the top of the left hand panel. In a similar method to that used in the Add Responders dialogue, you can select Teams or Users to Reroute the Incident to. We do not need to actually Reroute in this exercise so close the Reroute Incident dialogue by clicking Cancel . 5.4 Snooze \u00b6 You can also snooze this incident by clicking on the alarm clock Button at the top of the left hand panel. You can enter an amount of time upto 24 hours to snooze the incident. This action will be tracked in the Timeline, and when the time expires the paging will restart. This is useful for low priority incidents, enabling you to put them on a back burner for a few hours, but it ensures they do not get forgotten thanks to the paging process starting again. We do not need to actually Snooze in this exercise so close the Snooze Incident dialogue by clicking Cancel . 5.5 Action Tracking \u00b6 Now lets fix this issue and update the Incident with what we did. Add a new message at the top of the right hand panel such as Discovered rogue process, terminated it . All the actions related to the Incident will be recorded here, and can then be summarized is a Post Incident Review Report available from the Reports tab 5.6 Resolution \u00b6 Now kill off the process we started in the VM to max out the CPU by switching back the Shell session for the VM and pressing ctrl+c Within no more than 10 seconds SignalFx should detect the new CPU value, clear the alert state in SignalFx, then automatically update the Incident in VictorOps marking it as Resolved . As we have two way integration between Splunk Infrastructure Monitoring and Splunk On-Call we could have also marked the incident as Resolved in Splunk On-Call, and this would have resulted in the alert in Splunk Infrastructure Monitoring being resolved as well. That completes this introduction to Splunk On-Call, but feel free to checkout the more advanced modules which will be published in the coming weeks in the Optional Modules section. These will cover topics such as: Reporting Using the API Webhooks Alert Rules Engine Maintenance Mode","title":"Manage Incidents"},{"location":"oncall/incident_lifecycle/manage_incidents/#managing-incidents","text":"","title":"Managing Incidents"},{"location":"oncall/incident_lifecycle/manage_incidents/#1-acknowledge","text":"Use your Splunk On-Call App on your phone to acknowledge the Incident by clicking on the push notification ... ...to open the alert in the Splunk On-Call mobile app, then clicking on either the single tick in the top right hand corner, or the Acknowledge link to acknowledge the incident and stop the escalation process. The will then transform into a , and the status will change from TRIGGERED to ACKNOWLEDGED . Triggered Incident Acknowledge Incident","title":"1. Acknowledge"},{"location":"oncall/incident_lifecycle/manage_incidents/#2-details-and-annotations","text":"Still on your phone, select the Alert Details tab. Then on the Web UI, navigate back to Timeline , select Team Incidents on the right, then select Acknowledged and click into the new Incident . You should now have the Details tab displayed on both your Phone and the Web UI. Notice how they both show the exact same information. Now select the Annotations tab on both the Phone and the Web UI, you should have a Graph displayed in the UI which is generated by SignalFx. On your phone you should get the same image displayed (sometimes it's a simple hyperlink depending on the image size) Splunk On-Call is a 'Mobile First' platform meaning the phone app is full functionality and you can manage an incident directly from your phone. For the remainder of this module we will focus on the Web UI however please spend some time later exploring the phone app features.","title":"2. Details and Annotations"},{"location":"oncall/incident_lifecycle/manage_incidents/#3-link-to-alerting-system","text":"Sticking with the Web UI, click the 2. Alert Details in SignalFx link. This will open a new browser tab and take you directly to the Alert within Splunk Infrastructure Monitoring where you could then progress your troubleshooting using the powerful tools built into its UI. However, we are focussing on Splunk On-Call so close this tab and return to the Splunk On-Call UI.","title":"3. Link to Alerting System"},{"location":"oncall/incident_lifecycle/manage_incidents/#4-similar-incidents","text":"What if Splunk On-Call could identify previous incidents within the system which may give you a clue to the best way to tackle this incident. The Similar Incidents tab does exactly that, surfacing previous incidents allowing you to look at them and see what actions were taken to resolve them, actions which could be easily repeated for this incident.","title":"4. Similar Incidents"},{"location":"oncall/incident_lifecycle/manage_incidents/#5-war-room-dashboard","text":"At the top right in the UI are a number of icons that allow quick access to various actions, click on the far right one which will open this Incident in a War Room Dashboard (opens in new tab).","title":"5. War Room Dashboard"},{"location":"oncall/incident_lifecycle/manage_incidents/#51-timeline","text":"With the Incident expanded, you can see on the right we have a Time Line view where you can add messages and see the history of previous alerts and interactions.","title":"5.1 Timeline"},{"location":"oncall/incident_lifecycle/manage_incidents/#52-add-responders","text":"On the far left you have the option of allocating additional resources to this incident by clicking on the Add Responders link. This allows you build a virtual team specific to this incident by adding other Teams or individual Users, and also share details of a Conference Bridge where you can all get together and collaborate. Once the system has built up some incident data history, it will use Machine Learning to suggest Teams and Users who have historically worked on similar incidents, as they may be best placed to help resolve this incident quickly. You can select different Teams and/or Users and also choose from a pre-configured conference bridge, or populate the details of a new bridge from your preferred provider. We do not need to add any Responders in this exercise so close the Add Responders dialogue by clicking Cancel .","title":"5.2 Add Responders"},{"location":"oncall/incident_lifecycle/manage_incidents/#53-reroute","text":"If it's decided that maybe the incident could be better dealt with by a different Team, the call can be Rerouted by clicking the Reroute Button at the top of the left hand panel. In a similar method to that used in the Add Responders dialogue, you can select Teams or Users to Reroute the Incident to. We do not need to actually Reroute in this exercise so close the Reroute Incident dialogue by clicking Cancel .","title":"5.3 Reroute"},{"location":"oncall/incident_lifecycle/manage_incidents/#54-snooze","text":"You can also snooze this incident by clicking on the alarm clock Button at the top of the left hand panel. You can enter an amount of time upto 24 hours to snooze the incident. This action will be tracked in the Timeline, and when the time expires the paging will restart. This is useful for low priority incidents, enabling you to put them on a back burner for a few hours, but it ensures they do not get forgotten thanks to the paging process starting again. We do not need to actually Snooze in this exercise so close the Snooze Incident dialogue by clicking Cancel .","title":"5.4 Snooze"},{"location":"oncall/incident_lifecycle/manage_incidents/#55-action-tracking","text":"Now lets fix this issue and update the Incident with what we did. Add a new message at the top of the right hand panel such as Discovered rogue process, terminated it . All the actions related to the Incident will be recorded here, and can then be summarized is a Post Incident Review Report available from the Reports tab","title":"5.5 Action Tracking"},{"location":"oncall/incident_lifecycle/manage_incidents/#56-resolution","text":"Now kill off the process we started in the VM to max out the CPU by switching back the Shell session for the VM and pressing ctrl+c Within no more than 10 seconds SignalFx should detect the new CPU value, clear the alert state in SignalFx, then automatically update the Incident in VictorOps marking it as Resolved . As we have two way integration between Splunk Infrastructure Monitoring and Splunk On-Call we could have also marked the incident as Resolved in Splunk On-Call, and this would have resulted in the alert in Splunk Infrastructure Monitoring being resolved as well. That completes this introduction to Splunk On-Call, but feel free to checkout the more advanced modules which will be published in the coming weeks in the Optional Modules section. These will cover topics such as: Reporting Using the API Webhooks Alert Rules Engine Maintenance Mode","title":"5.6 Resolution"},{"location":"oncall/optional/detector/","text":"Create a SignalFx Detector \u00b6 Aim \u00b6 We need to create a new Detector within SignalFx which will use VictorOps as the target to send alerts to. We will use Terraform installed within the VM to create the Detector, but first we need to obtain some values required for Terraform to run. 1. Preparation \u00b6 The presenter will typically share these values with you at the start of the module to save time, but the following instructions explain how to get them for yourself. 1.1 Create a variables document \u00b6 We suggest you create a variables document using your preferred text editor as you will be gathering three different values in the next few steps which you need to use in the last step of this module. Add the following lines to your variables document, then as you gather the values you can add them to the appropriate lines: variables.txt export ACCESS_TOKEN= export REALM= export SFXVOPSID= 1.2 Obtain SignalFx Access Token \u00b6 In the SignalFx UI you can find your Access Token by clicking on the Settings icon on the top right of the SignalFx UI, select Organization Settings \u2192 Access Tokens , expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy it you your clipboard, then paste it into the ACCESS_TOKEN line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= export SFXVOPSID= 1.3 Obtain SignalFx Realm \u00b6 Still in the SignalFx UI, click on the Settings icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us1 , but yours may be eu0 or one of the many other SignalFx Realms. Copy it to the REALM line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID= 1.4. Obtain VictorOps Integration ID \u00b6 In SignalFx UI navigate to Integrations and use the search feature to find the VictorOps Integration. Expand the VictorOps-xxxx configuration; if there are more than one you will be informed which one to copy by the presenter. Copy it to the SFXVOPSID line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID= xxxx 2. Create environment variables \u00b6 2.1 Copy variables to VM \u00b6 With all the required values now safely copied into your variables document you can use them to compile the commands which we will run in your VM in the next step. Example export SFXVOPSID=EYierbGA4AA export ACCESS_TOKEN=by78voyt7b..... export REALM=us1 Switch back to your shell session connected to the VM you created in the Getting Started/Create a Test Environment module, all of the following commands will be executed within this instance: Past the three commands from your variables document into the shell session of your VM. 3. Initialize and apply Terraform \u00b6 Still within your VM, switch to the victorops folder where the Terraform config files are located (you should still be logged in as Ubuntu and should not have elevated to root) Change Directory cd ~/workshop/victorops Now we can initialize Terraform: Shell Command terraform init -upgrade Example Output Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.21.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.21\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. You can now copy and the paste the following code block to run Terraform using the Variables you created in the VM. Check the plan output for errors before typing yes to commit the apply. Shell Command terraform apply \\ -var=\"access_token=$ACCESS_TOKEN\" \\ -var=\"realm=$REALM\" \\ -var=\"sfx_prefix=${HOSTNAME}\" \\ -var=\"sfx_vo_id=$SFXVOPSID\" \\ -var=\"routing_key=${HOSTNAME}_PRI\" Example Output An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # signalfx_detector.cpu_greater_90 will be created + resource \"signalfx_detector\" \"cpu_greater_90\" { + description = \"Alerts when CPU usage is greater than 90%\" + id = (known after apply) + max_delay = 0 + name = \"vmpe CPU greater than 90%\" + program_text = <<~EOT from signalfx.detectors.against_recent import against_recent A = data('cpu.utilization', filter=filter('host', 'vmpe*')).publish(label='A') detect(when(A > threshold(90))).publish('CPU utilization is greater than 90%') EOT + show_data_markers = true + time_range = 3600 + url = (known after apply) + rule { + detect_label = \"CPU utilization is greater than 90%\" + disabled = false + notifications = [ + \"VictorOps,xxx,vmpe_pri\", ] + parameterized_body = <<~EOT {{#if anomalous}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" triggered at {{timestamp}}. {{else}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" cleared at {{timestamp}}. {{/if}} {{#if anomalous}} Triggering condition: {{{readableRule}}} {{/if}} {{#if anomalous}} Signal value: {{inputs.A.value}} {{else}} Current signal value: {{inputs.A.value}} {{/if}} {{#notEmpty dimensions}} Signal details: {{{dimensions}}} {{/notEmpty}} {{#if anomalous}} {{#if runbookUrl}} Runbook: {{{runbookUrl}}} {{/if}} {{#if tip}} Tip: {{{tip}}} {{/if}} {{/if}} EOT + parameterized_subject = \"{{ruleSeverity}} Alert: {{{ruleName}}} ({{{detectorName}}})\" + severity = \"Critical\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions in workspace \"Workshop\"? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes signalfx_detector.cpu_greater_90: Creating... signalfx_detector.cpu_greater_90: Creation complete after 2s [id=EWHU-YAAAAA] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. 4. Summary \u00b6 By running Terraform within the VM you have just created a new Detector within SignalFx which will send alerts to VictorOps if the CPU utilization of your specific VM goes above 90%. In the SignalFx UI go to Alerts \u2192 Detectors to show all the Detectors and find the one matching your INSTANCE value (the first four letters of the name of your VM). Optionally - Click on CPU Utilization is greater than 90% to open the Alert Rule Editor to view its settings. A filter has been used to specifically monitor your VM using the 1st 4 characters of its name, which were randomly assigned when you created the VM. A Recipient has been configured using the VictorOps Integration and your Routing Key has been specified. This is how a monitoring system such as SignalFx knows to route Alerts into VictorOps, and ensure they get routed to the correct team. You have now configured the Integrations between VictorOps and SignalFx! The final part of this module is to test the flow of alerts from SignalFx into VictorOps and see how you can manage the incident with both the VictorOps UI and Mobile App.","title":"Create a Detector"},{"location":"oncall/optional/detector/#create-a-signalfx-detector","text":"","title":"Create a SignalFx Detector"},{"location":"oncall/optional/detector/#aim","text":"We need to create a new Detector within SignalFx which will use VictorOps as the target to send alerts to. We will use Terraform installed within the VM to create the Detector, but first we need to obtain some values required for Terraform to run.","title":"Aim"},{"location":"oncall/optional/detector/#1-preparation","text":"The presenter will typically share these values with you at the start of the module to save time, but the following instructions explain how to get them for yourself.","title":"1. Preparation"},{"location":"oncall/optional/detector/#11-create-a-variables-document","text":"We suggest you create a variables document using your preferred text editor as you will be gathering three different values in the next few steps which you need to use in the last step of this module. Add the following lines to your variables document, then as you gather the values you can add them to the appropriate lines: variables.txt export ACCESS_TOKEN= export REALM= export SFXVOPSID=","title":"1.1 Create a variables document"},{"location":"oncall/optional/detector/#12-obtain-signalfx-access-token","text":"In the SignalFx UI you can find your Access Token by clicking on the Settings icon on the top right of the SignalFx UI, select Organization Settings \u2192 Access Tokens , expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy it you your clipboard, then paste it into the ACCESS_TOKEN line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= export SFXVOPSID=","title":"1.2 Obtain SignalFx Access Token"},{"location":"oncall/optional/detector/#13-obtain-signalfx-realm","text":"Still in the SignalFx UI, click on the Settings icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us1 , but yours may be eu0 or one of the many other SignalFx Realms. Copy it to the REALM line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID=","title":"1.3 Obtain SignalFx Realm"},{"location":"oncall/optional/detector/#14-obtain-victorops-integration-id","text":"In SignalFx UI navigate to Integrations and use the search feature to find the VictorOps Integration. Expand the VictorOps-xxxx configuration; if there are more than one you will be informed which one to copy by the presenter. Copy it to the SFXVOPSID line of your variables document. variables.txt export ACCESS_TOKEN= xxxx export REALM= xxxx export SFXVOPSID= xxxx","title":"1.4. Obtain VictorOps Integration ID"},{"location":"oncall/optional/detector/#2-create-environment-variables","text":"","title":"2. Create environment variables"},{"location":"oncall/optional/detector/#21-copy-variables-to-vm","text":"With all the required values now safely copied into your variables document you can use them to compile the commands which we will run in your VM in the next step. Example export SFXVOPSID=EYierbGA4AA export ACCESS_TOKEN=by78voyt7b..... export REALM=us1 Switch back to your shell session connected to the VM you created in the Getting Started/Create a Test Environment module, all of the following commands will be executed within this instance: Past the three commands from your variables document into the shell session of your VM.","title":"2.1 Copy variables to VM"},{"location":"oncall/optional/detector/#3-initialize-and-apply-terraform","text":"Still within your VM, switch to the victorops folder where the Terraform config files are located (you should still be logged in as Ubuntu and should not have elevated to root) Change Directory cd ~/workshop/victorops Now we can initialize Terraform: Shell Command terraform init -upgrade Example Output Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.21.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.21\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. You can now copy and the paste the following code block to run Terraform using the Variables you created in the VM. Check the plan output for errors before typing yes to commit the apply. Shell Command terraform apply \\ -var=\"access_token=$ACCESS_TOKEN\" \\ -var=\"realm=$REALM\" \\ -var=\"sfx_prefix=${HOSTNAME}\" \\ -var=\"sfx_vo_id=$SFXVOPSID\" \\ -var=\"routing_key=${HOSTNAME}_PRI\" Example Output An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # signalfx_detector.cpu_greater_90 will be created + resource \"signalfx_detector\" \"cpu_greater_90\" { + description = \"Alerts when CPU usage is greater than 90%\" + id = (known after apply) + max_delay = 0 + name = \"vmpe CPU greater than 90%\" + program_text = <<~EOT from signalfx.detectors.against_recent import against_recent A = data('cpu.utilization', filter=filter('host', 'vmpe*')).publish(label='A') detect(when(A > threshold(90))).publish('CPU utilization is greater than 90%') EOT + show_data_markers = true + time_range = 3600 + url = (known after apply) + rule { + detect_label = \"CPU utilization is greater than 90%\" + disabled = false + notifications = [ + \"VictorOps,xxx,vmpe_pri\", ] + parameterized_body = <<~EOT {{#if anomalous}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" triggered at {{timestamp}}. {{else}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" cleared at {{timestamp}}. {{/if}} {{#if anomalous}} Triggering condition: {{{readableRule}}} {{/if}} {{#if anomalous}} Signal value: {{inputs.A.value}} {{else}} Current signal value: {{inputs.A.value}} {{/if}} {{#notEmpty dimensions}} Signal details: {{{dimensions}}} {{/notEmpty}} {{#if anomalous}} {{#if runbookUrl}} Runbook: {{{runbookUrl}}} {{/if}} {{#if tip}} Tip: {{{tip}}} {{/if}} {{/if}} EOT + parameterized_subject = \"{{ruleSeverity}} Alert: {{{ruleName}}} ({{{detectorName}}})\" + severity = \"Critical\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions in workspace \"Workshop\"? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes signalfx_detector.cpu_greater_90: Creating... signalfx_detector.cpu_greater_90: Creation complete after 2s [id=EWHU-YAAAAA] Apply complete! Resources: 1 added, 0 changed, 0 destroyed.","title":"3. Initialize and apply Terraform"},{"location":"oncall/optional/detector/#4-summary","text":"By running Terraform within the VM you have just created a new Detector within SignalFx which will send alerts to VictorOps if the CPU utilization of your specific VM goes above 90%. In the SignalFx UI go to Alerts \u2192 Detectors to show all the Detectors and find the one matching your INSTANCE value (the first four letters of the name of your VM). Optionally - Click on CPU Utilization is greater than 90% to open the Alert Rule Editor to view its settings. A filter has been used to specifically monitor your VM using the 1st 4 characters of its name, which were randomly assigned when you created the VM. A Recipient has been configured using the VictorOps Integration and your Routing Key has been specified. This is how a monitoring system such as SignalFx knows to route Alerts into VictorOps, and ensure they get routed to the correct team. You have now configured the Integrations between VictorOps and SignalFx! The final part of this module is to test the flow of alerts from SignalFx into VictorOps and see how you can manage the incident with both the VictorOps UI and Mobile App.","title":"4. Summary"},{"location":"oncall/optional/multipass/","text":"Creating a Test VM Using Multipass \u00b6 Aim \u00b6 The aim of this module is to guide you through the process of creating a VM locally using Multipass. Once the configuration of VictorOps is complete you will use this VM to trigger an Alert from SignalFx which in turn will create an Incident within VictorOps, resulting in you getting paged. 1. Install Multipass \u00b6 If you do not already have Multipass installed you can download the installer from here . Users running macOS can install it using Homebrew by running: Shell Command brew cask install multipass 2. Create VM using Multipass \u00b6 2.1 Cloud-init \u00b6 The first step is to pull down the cloud-init file to launch a pre-configured VM. Shell Command WSVERSION=1.57 curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/victorops.yaml \\ -o victorops.yaml 2.2 Launch VM \u00b6 Remaining in the same directory where you downloaded victorops.yaml , run the following commands to create your VM. The first command will generate a random unique 4 character string. This will prevent clashes in the SignalFx UI. Shell Command export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) multipass launch \\ --name ${INSTANCE} \\ --cloud-init victorops.yaml Example Output Launched: zevn Make a note of your VMs Hostname as you will need it in later steps. 2.3 Connect to VM \u00b6 Once the VM has deployed successfully, in a new shell session connect to the VM using the following command. Shell Command multipass shell ${INSTANCE} Example Input multipass shell zevn Example Output Last login: Tue Jun 9 15:10:19 2020 from 192.168.64.1 ubuntu@zevn:~$ 3. Install SignalFx Agent \u00b6 An easy way to install the SignalFx Agent into your VM is to copy the install commands from the SignalFx UI, then run them directly within your VM. 3.1 SignalFx UI \u00b6 Navigate to the Integrations tab within the SignalFx UI, where you will find the SignalFx SmartAgent tile on the top row. Click on the SmartAgent tile to open it... ...then select the Setup tab... ...then scroll down to 'Step 1' where you will find the commands for installing the agent for both Linux and Windows. You need to copy the commands for Linux, so click the top copy button to place these commands on your clipboard ready for the next step. 3.2 Install Agent \u00b6 Now paste the linux install commands into your VM Shell, the SignalFx Agent will install and after approx 1 min you should have the following result. Example Output The SignalFx Agent has been successfully installed. Make sure that your system's time is relatively accurate or else datapoints may not be accepted. The agent's main configuration file is located at /etc/signalfx/agent.yaml. 4. Check SignalFx Agent \u00b6 4.1 Agent Status \u00b6 Once the agent has completed installing run the following command to check the status Shell Command sudo signalfx-agent status Example Output SignalFx Agent version: 5.3.0 Agent uptime: 2m7s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 6 Bad Monitor Config: None Global Dimensions: {host: zevn} GlobalSpanTags: map[] Datapoints sent (last minute): 237 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 18 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything 4.2 Check the SignalFx UI \u00b6 Navigate to the SignalFx UI and click on the Infrastructure tab. The click on Hosts (Smart Agent / collectd) under the Hosts section. Find your VM and confirm it is reporting in correctly; allow a few minutes for it to appear. If it fails to appear after 3 mins, please let the Splunk Team know so they can help troubleshoot. Because you are using a Multipass VM instead of a Splunk Provided EC2 Instance, you will also need to complete the optional module \"Create a Detector\". This will ensure you receive incident notifications for this VM.","title":"Using Multipass VM"},{"location":"oncall/optional/multipass/#creating-a-test-vm-using-multipass","text":"","title":"Creating a Test VM Using Multipass"},{"location":"oncall/optional/multipass/#aim","text":"The aim of this module is to guide you through the process of creating a VM locally using Multipass. Once the configuration of VictorOps is complete you will use this VM to trigger an Alert from SignalFx which in turn will create an Incident within VictorOps, resulting in you getting paged.","title":"Aim"},{"location":"oncall/optional/multipass/#1-install-multipass","text":"If you do not already have Multipass installed you can download the installer from here . Users running macOS can install it using Homebrew by running: Shell Command brew cask install multipass","title":"1. Install Multipass"},{"location":"oncall/optional/multipass/#2-create-vm-using-multipass","text":"","title":"2. Create VM using Multipass"},{"location":"oncall/optional/multipass/#21-cloud-init","text":"The first step is to pull down the cloud-init file to launch a pre-configured VM. Shell Command WSVERSION=1.57 curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/victorops.yaml \\ -o victorops.yaml","title":"2.1 Cloud-init"},{"location":"oncall/optional/multipass/#22-launch-vm","text":"Remaining in the same directory where you downloaded victorops.yaml , run the following commands to create your VM. The first command will generate a random unique 4 character string. This will prevent clashes in the SignalFx UI. Shell Command export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) multipass launch \\ --name ${INSTANCE} \\ --cloud-init victorops.yaml Example Output Launched: zevn Make a note of your VMs Hostname as you will need it in later steps.","title":"2.2 Launch VM"},{"location":"oncall/optional/multipass/#23-connect-to-vm","text":"Once the VM has deployed successfully, in a new shell session connect to the VM using the following command. Shell Command multipass shell ${INSTANCE} Example Input multipass shell zevn Example Output Last login: Tue Jun 9 15:10:19 2020 from 192.168.64.1 ubuntu@zevn:~$","title":"2.3 Connect to VM"},{"location":"oncall/optional/multipass/#3-install-signalfx-agent","text":"An easy way to install the SignalFx Agent into your VM is to copy the install commands from the SignalFx UI, then run them directly within your VM.","title":"3. Install SignalFx Agent"},{"location":"oncall/optional/multipass/#31-signalfx-ui","text":"Navigate to the Integrations tab within the SignalFx UI, where you will find the SignalFx SmartAgent tile on the top row. Click on the SmartAgent tile to open it... ...then select the Setup tab... ...then scroll down to 'Step 1' where you will find the commands for installing the agent for both Linux and Windows. You need to copy the commands for Linux, so click the top copy button to place these commands on your clipboard ready for the next step.","title":"3.1 SignalFx UI"},{"location":"oncall/optional/multipass/#32-install-agent","text":"Now paste the linux install commands into your VM Shell, the SignalFx Agent will install and after approx 1 min you should have the following result. Example Output The SignalFx Agent has been successfully installed. Make sure that your system's time is relatively accurate or else datapoints may not be accepted. The agent's main configuration file is located at /etc/signalfx/agent.yaml.","title":"3.2 Install Agent"},{"location":"oncall/optional/multipass/#4-check-signalfx-agent","text":"","title":"4. Check SignalFx Agent"},{"location":"oncall/optional/multipass/#41-agent-status","text":"Once the agent has completed installing run the following command to check the status Shell Command sudo signalfx-agent status Example Output SignalFx Agent version: 5.3.0 Agent uptime: 2m7s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 6 Bad Monitor Config: None Global Dimensions: {host: zevn} GlobalSpanTags: map[] Datapoints sent (last minute): 237 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 18 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything","title":"4.1 Agent Status"},{"location":"oncall/optional/multipass/#42-check-the-signalfx-ui","text":"Navigate to the SignalFx UI and click on the Infrastructure tab. The click on Hosts (Smart Agent / collectd) under the Hosts section. Find your VM and confirm it is reporting in correctly; allow a few minutes for it to appear. If it fails to appear after 3 mins, please let the Splunk Team know so they can help troubleshoot. Because you are using a Multipass VM instead of a Splunk Provided EC2 Instance, you will also need to complete the optional module \"Create a Detector\". This will ensure you receive incident notifications for this VM.","title":"4.2 Check the SignalFx UI"},{"location":"oncall/optional/sfx_integrations/","text":"VictorOps Integrations - Lab Summary \u00b6 This module covers configuring the Integrations between SignalFx and VictorOps. Whilst the detailed steps below walk you through the process, you will find that the intergrations are already active within the Splunk systems being used for this workhop so Steps 1 & 2 are for info only. You only need to complete Step 3. Copy ID 1. VictorOps Service API Endpoint \u00b6 Warning The SignalFx Integration only needs to be enabled once per VictorOps instance, so you will probably find it has already been enabled, please DO NOT disable an already active integration when completing this lab. This is for info only as the Integration has already been enabled In order to integrate SignalFx with VictorOps we need to first obtain the Service API Endpoint for VictorOps. Within the VictorOps UI navigate to Integrations main tab and then use the search feature to find the SignalFx Integration. If it is not already enabled, click the Enable Integration button to activate it. This would be used when configuring the VictorOps Integration within the SignalFx UI if it had not already been enabled. 2. Enable VictorOps Integration within SignalFx \u00b6 In the SignalFx UI navigate to Integrations and use the search feature to find the VictorOps integration. Do not create a new integration! Please do not create additional VictorOps integrations if one already exists, it will not break anything but simply creates extra clean up work after the workshop has completed. The aim of this part of the lab was to show you how you would go about configuring the Integration if it was not already enabled. Assuming you are using the AppDev EMEA instance of VictorOps you will find the VictorOps Integration has already been configured so there is no need to create a new one. However the process of creating a new Integration is simply to click on Create New Integration like in the image below, or if there are existing integrations and you want to add another one you would click New Integration . Enter a descriptive Name then paste the Service_API_Endpoint value you copied in the previous step into the Post URL field, then save it. Handling multiple VictorOps integrations SignalFx can integrate with multiple VictorOps accounts so it is important when creating one to use a descriptive name and to not simply call it VictorOps. This name will be used within the SignalFx UI when selecting this integration, so ensure it is unambiguous 3. Copy ID \u00b6 In SignalFx UI navigate to Integrations and use the search feature to find the VictorOps Integration. Copy the ID field and save it for use in the next steps. We suggest you create a notepad document or similar as you will be gathering some additional values in the next steps.","title":"SignalFx Integration"},{"location":"oncall/optional/sfx_integrations/#victorops-integrations-lab-summary","text":"This module covers configuring the Integrations between SignalFx and VictorOps. Whilst the detailed steps below walk you through the process, you will find that the intergrations are already active within the Splunk systems being used for this workhop so Steps 1 & 2 are for info only. You only need to complete Step 3. Copy ID","title":"VictorOps Integrations - Lab Summary"},{"location":"oncall/optional/sfx_integrations/#1-victorops-service-api-endpoint","text":"Warning The SignalFx Integration only needs to be enabled once per VictorOps instance, so you will probably find it has already been enabled, please DO NOT disable an already active integration when completing this lab. This is for info only as the Integration has already been enabled In order to integrate SignalFx with VictorOps we need to first obtain the Service API Endpoint for VictorOps. Within the VictorOps UI navigate to Integrations main tab and then use the search feature to find the SignalFx Integration. If it is not already enabled, click the Enable Integration button to activate it. This would be used when configuring the VictorOps Integration within the SignalFx UI if it had not already been enabled.","title":"1. VictorOps Service API Endpoint"},{"location":"oncall/optional/sfx_integrations/#2-enable-victorops-integration-within-signalfx","text":"In the SignalFx UI navigate to Integrations and use the search feature to find the VictorOps integration. Do not create a new integration! Please do not create additional VictorOps integrations if one already exists, it will not break anything but simply creates extra clean up work after the workshop has completed. The aim of this part of the lab was to show you how you would go about configuring the Integration if it was not already enabled. Assuming you are using the AppDev EMEA instance of VictorOps you will find the VictorOps Integration has already been configured so there is no need to create a new one. However the process of creating a new Integration is simply to click on Create New Integration like in the image below, or if there are existing integrations and you want to add another one you would click New Integration . Enter a descriptive Name then paste the Service_API_Endpoint value you copied in the previous step into the Post URL field, then save it. Handling multiple VictorOps integrations SignalFx can integrate with multiple VictorOps accounts so it is important when creating one to use a descriptive name and to not simply call it VictorOps. This name will be used within the SignalFx UI when selecting this integration, so ensure it is unambiguous","title":"2. Enable VictorOps Integration within SignalFx"},{"location":"oncall/optional/sfx_integrations/#3-copy-id","text":"In SignalFx UI navigate to Integrations and use the search feature to find the VictorOps Integration. Copy the ID field and save it for use in the next steps. We suggest you create a notepad document or similar as you will be gathering some additional values in the next steps.","title":"3. Copy ID"},{"location":"resources/","text":"Additional Splunk for DevOps Resources \u00b6 Below are helpful resources about Splunk and the DevOps use case. Topics covered are SignalFx, VictorOps, OpenTelemetry, Observability and Incident Response. Documentation \u00b6 APM O11y Platform Docs APM O11y API Docs On-Call On-Call Platform Docs On-Call On-Call API Docs Blog Posts \u00b6 Observability \"TraceInvaders - an OpenTelemetry Video Game\" Observability \"Observability Microsite\" APM \"How Splunk Does Site Reliability Engineering (SRE)\" OTEL \"OpenTelemetry for Business Continuity\" APM \"In Observability RED is the new Black\" On-Call \"Alerts to Incident Response in Three Easy Steps\" APM \"Application Performance Redefined: Meet the New Splunk's Microservices APM\" Splunk \"Splunk is Lambda Ready: Announcing a New Partnership with AWS\" APM \"Supporting APM for .NET Applications\" OTEL \"OpenTelemetry Consolidates Data for Observability\" Observability \"Using Observability as a Proxy for User Happiness\" Webinars & Podcast \u00b6 APM \"Future of Microservices and APM\" DevOps \"DevOps from the Top Podcast\" Observability \"Oh my! SRE, AIOps and Observe-a-what?\" Observability \"On-Demand Observability Demo\" DevOps \"Dissecting DevOps PlayList\" DevOps \"DevOps Open Source Innovation and Insights On-Demand Virtual Event\" DevOps \"Observability to Enable Your Digital Initiatives On-Demand Virtual Event\"","title":"Links"},{"location":"resources/#additional-splunk-for-devops-resources","text":"Below are helpful resources about Splunk and the DevOps use case. Topics covered are SignalFx, VictorOps, OpenTelemetry, Observability and Incident Response.","title":"Additional Splunk for DevOps Resources"},{"location":"resources/#documentation","text":"APM O11y Platform Docs APM O11y API Docs On-Call On-Call Platform Docs On-Call On-Call API Docs","title":"Documentation"},{"location":"resources/#blog-posts","text":"Observability \"TraceInvaders - an OpenTelemetry Video Game\" Observability \"Observability Microsite\" APM \"How Splunk Does Site Reliability Engineering (SRE)\" OTEL \"OpenTelemetry for Business Continuity\" APM \"In Observability RED is the new Black\" On-Call \"Alerts to Incident Response in Three Easy Steps\" APM \"Application Performance Redefined: Meet the New Splunk's Microservices APM\" Splunk \"Splunk is Lambda Ready: Announcing a New Partnership with AWS\" APM \"Supporting APM for .NET Applications\" OTEL \"OpenTelemetry Consolidates Data for Observability\" Observability \"Using Observability as a Proxy for User Happiness\"","title":"Blog Posts"},{"location":"resources/#webinars-podcast","text":"APM \"Future of Microservices and APM\" DevOps \"DevOps from the Top Podcast\" Observability \"Oh my! SRE, AIOps and Observe-a-what?\" Observability \"On-Demand Observability Demo\" DevOps \"Dissecting DevOps PlayList\" DevOps \"DevOps Open Source Innovation and Insights On-Demand Virtual Event\" DevOps \"Observability to Enable Your Digital Initiatives On-Demand Virtual Event\"","title":"Webinars &amp; Podcast"},{"location":"resources/faq/","text":"Frequently Asked Questions \u00b6 A collection of the common questions and their answers associated with Observability, DevOps, Incident Response, SignalFx and VictorOps. Q: Alerts v. Incident Response v. Incident Management \u00b6 A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process. Monitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents. Those incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident. Incidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved. All these components are necessary for successful incident response and management practices. On-Call Q: Is Observability Monitoring \u00b6 A: The key difference between Monitoring and Observability is the difference between \u201cknown knowns\u201d and \u201cunknown knowns\u201d respectively. In monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed. Observability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited. Observability is a set of practices and technology, which include traditional monitoring metrics. These practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static. Observability Q: What are Traces and Spans \u00b6 A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together. Because microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures. APM Q: What is the Sidecar Pattern \u00b6 A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents associated with the management plan along with the application service they support. In Observability the sidecar services are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle. Observability","title":"FAQ"},{"location":"resources/faq/#frequently-asked-questions","text":"A collection of the common questions and their answers associated with Observability, DevOps, Incident Response, SignalFx and VictorOps.","title":"Frequently Asked Questions"},{"location":"resources/faq/#q-alerts-v-incident-response-v-incident-management","text":"A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process. Monitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents. Those incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident. Incidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved. All these components are necessary for successful incident response and management practices. On-Call","title":"Q: Alerts v. Incident Response v. Incident Management"},{"location":"resources/faq/#q-is-observability-monitoring","text":"A: The key difference between Monitoring and Observability is the difference between \u201cknown knowns\u201d and \u201cunknown knowns\u201d respectively. In monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed. Observability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited. Observability is a set of practices and technology, which include traditional monitoring metrics. These practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static. Observability","title":"Q: Is Observability Monitoring"},{"location":"resources/faq/#q-what-are-traces-and-spans","text":"A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together. Because microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures. APM","title":"Q: What are Traces and Spans"},{"location":"resources/faq/#q-what-is-the-sidecar-pattern","text":"A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents associated with the management plan along with the application service they support. In Observability the sidecar services are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle. Observability","title":"Q: What is the Sidecar Pattern"},{"location":"servicebureau/billing-and-usage/","text":"Service Bureau - Lab Summary \u00b6 How to keep track of the usage of SignalFx in your organization Learn how to keep track of spend by exploring the Billing and Usage interface Creating Teams Adding notification rules to Teams Controlling usage 1. Understanding SignalFx engagement \u00b6 To fully understand SignalFx engagement inside your organization, click on the Settings icon on the top right of the SignalFx UI. From the drop down, select the Organizations Settings \u2192 Organization Overview , this will provide you with the following dashboard that shows you how your SignalFx organization is being used: On the left hand menu (not shown in the above screenshot) you will see a list of members, and in the centre, various charts that show you the number of users, teams, charts, dashboards, and dashboard groups created, as well as various growth trends. The screenshot is taken from an demonstration organization, the Workshop organization you're looking at may have less data to work with as this is cleared down after each workshop. Take a minute to explore the various charts in the Organization Overview of this Workshop instance. 2. Usage and Billing \u00b6 If you want to see what your usage is against your contract you can select the Organizations Settings \u2192 Billing and Usage from your profile icon top right of the SignalFx UI. Or the faster way is to select the Billing and Usage item from the left hand pane. This screen may take a few seconds to load whilst it calculates and pulls in the usage. 3. Understanding usage \u00b6 You will see a screen similar like the one below that will give you an overview of the current usage, the average usage and your entitlement per category : Hosts, Containers, Custom Metrics and High Resolution Metrics. For more information about about these categories please refer to Billing and Usage information . 4. Examine usage in detail \u00b6 The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below). Also, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart). In this example you can see that there are 18 Hosts, 0 Containers and 1038 Custom Metrics and 0 High Resolution Metrics. In the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart). The blue line marked Average Usage indicates what SignalFx will use to calculate your average usage for the current billing period. Info As you can see from the screenshot, SignalFx does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges. To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the the Billing Period with the drop down on the right. Please take a minute to explore the different time periods & categories and their views. Finally, the pane on the right shows you information about your Subscription.","title":"Billing and Usage"},{"location":"servicebureau/billing-and-usage/#service-bureau-lab-summary","text":"How to keep track of the usage of SignalFx in your organization Learn how to keep track of spend by exploring the Billing and Usage interface Creating Teams Adding notification rules to Teams Controlling usage","title":"Service Bureau - Lab Summary"},{"location":"servicebureau/billing-and-usage/#1-understanding-signalfx-engagement","text":"To fully understand SignalFx engagement inside your organization, click on the Settings icon on the top right of the SignalFx UI. From the drop down, select the Organizations Settings \u2192 Organization Overview , this will provide you with the following dashboard that shows you how your SignalFx organization is being used: On the left hand menu (not shown in the above screenshot) you will see a list of members, and in the centre, various charts that show you the number of users, teams, charts, dashboards, and dashboard groups created, as well as various growth trends. The screenshot is taken from an demonstration organization, the Workshop organization you're looking at may have less data to work with as this is cleared down after each workshop. Take a minute to explore the various charts in the Organization Overview of this Workshop instance.","title":"1. Understanding SignalFx engagement"},{"location":"servicebureau/billing-and-usage/#2-usage-and-billing","text":"If you want to see what your usage is against your contract you can select the Organizations Settings \u2192 Billing and Usage from your profile icon top right of the SignalFx UI. Or the faster way is to select the Billing and Usage item from the left hand pane. This screen may take a few seconds to load whilst it calculates and pulls in the usage.","title":"2. Usage and Billing"},{"location":"servicebureau/billing-and-usage/#3-understanding-usage","text":"You will see a screen similar like the one below that will give you an overview of the current usage, the average usage and your entitlement per category : Hosts, Containers, Custom Metrics and High Resolution Metrics. For more information about about these categories please refer to Billing and Usage information .","title":"3. Understanding usage"},{"location":"servicebureau/billing-and-usage/#4-examine-usage-in-detail","text":"The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below). Also, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart). In this example you can see that there are 18 Hosts, 0 Containers and 1038 Custom Metrics and 0 High Resolution Metrics. In the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart). The blue line marked Average Usage indicates what SignalFx will use to calculate your average usage for the current billing period. Info As you can see from the screenshot, SignalFx does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges. To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the the Billing Period with the drop down on the right. Please take a minute to explore the different time periods & categories and their views. Finally, the pane on the right shows you information about your Subscription.","title":"4. Examine usage in detail"},{"location":"servicebureau/teams/","text":"Teams - Lab Summary \u00b6 Introduction to Teams Create a Team and add members to Team 1. Introduction to Teams \u00b6 To make sure that users see the dashboards and alerts that are relevant to them when using SignalFX, most organizations will use SignalFx's Teams feature to assign a member to one or more Teams. Ideally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in SignalFx. When a user logs into SignalFx, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role. In the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team. This Dashboard has specific Dashboard Groups for NGINX, Infra and K8s assigned but any Dashboard Group can be linked to a Teams Dashboard. They can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL Dashboards using the adjacent link. Alerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert. The Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown). 2. Creating a new Team \u00b6 To work with to SignalFx's Team UI click on the Settings icon on the top right of the SignalFx UI. Select the Organizations Settings \u2192 Teams tab, or select the Teams tab from the left pane. When the Team UI is selected you will be presented with the list of current Teams. To add a new Team click on the Create New Team button. This will present you with the Create New Team dialog. Create your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below: You can remove selected users by pressing Remove or the small x . Make sure you have your group created with your initials and with yourself added as a member, then click Done . This will bring you back to the Teams list that will now show your Team and the one's created by others. Note The Teams(s) you are a member of have a grey Member icon in front of it. If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself. This is the same dialog you get when pressing the 3 dots ... at the end of the line with your Team and selecting Edit Team The ... menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member). 3. Adding Notification Rules \u00b6 You can set up specific Notification rules per team, click on the NOTIFICATION POLICY tab, this will open the notification edit menu. By default the system offers you the ability to set up a general notification rule for your team. Note The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type. 3.1 Adding recipients \u00b6 You can add other recipients, by clicking Add Recipient . These recipients do not need to be SignalFx users. However if you click on the link Configure a single policy for alerts of any severity you can configure every alert level independently. Different alert rules for the different alert levels can be configured, as shown in the above image. Critical and Major are using Splunk's VictorOps Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email. 3.2 Notification Integrations \u00b6 In addition to sending alert notifications via email, you can configure SignalFx to send alert notifications to the services shown below. Take a moment to create some notification rules for you Team.","title":"Teams"},{"location":"servicebureau/teams/#teams-lab-summary","text":"Introduction to Teams Create a Team and add members to Team","title":"Teams - Lab Summary"},{"location":"servicebureau/teams/#1-introduction-to-teams","text":"To make sure that users see the dashboards and alerts that are relevant to them when using SignalFX, most organizations will use SignalFx's Teams feature to assign a member to one or more Teams. Ideally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in SignalFx. When a user logs into SignalFx, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role. In the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team. This Dashboard has specific Dashboard Groups for NGINX, Infra and K8s assigned but any Dashboard Group can be linked to a Teams Dashboard. They can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL Dashboards using the adjacent link. Alerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert. The Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown).","title":"1. Introduction to Teams"},{"location":"servicebureau/teams/#2-creating-a-new-team","text":"To work with to SignalFx's Team UI click on the Settings icon on the top right of the SignalFx UI. Select the Organizations Settings \u2192 Teams tab, or select the Teams tab from the left pane. When the Team UI is selected you will be presented with the list of current Teams. To add a new Team click on the Create New Team button. This will present you with the Create New Team dialog. Create your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below: You can remove selected users by pressing Remove or the small x . Make sure you have your group created with your initials and with yourself added as a member, then click Done . This will bring you back to the Teams list that will now show your Team and the one's created by others. Note The Teams(s) you are a member of have a grey Member icon in front of it. If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself. This is the same dialog you get when pressing the 3 dots ... at the end of the line with your Team and selecting Edit Team The ... menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member).","title":"2. Creating a new Team"},{"location":"servicebureau/teams/#3-adding-notification-rules","text":"You can set up specific Notification rules per team, click on the NOTIFICATION POLICY tab, this will open the notification edit menu. By default the system offers you the ability to set up a general notification rule for your team. Note The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type.","title":"3. Adding Notification Rules"},{"location":"servicebureau/teams/#31-adding-recipients","text":"You can add other recipients, by clicking Add Recipient . These recipients do not need to be SignalFx users. However if you click on the link Configure a single policy for alerts of any severity you can configure every alert level independently. Different alert rules for the different alert levels can be configured, as shown in the above image. Critical and Major are using Splunk's VictorOps Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email.","title":"3.1 Adding recipients"},{"location":"servicebureau/teams/#32-notification-integrations","text":"In addition to sending alert notifications via email, you can configure SignalFx to send alert notifications to the services shown below. Take a moment to create some notification rules for you Team.","title":"3.2 Notification Integrations"},{"location":"servicebureau/tokens/","text":"Controlling Usage - Lab Summary \u00b6 Discover how you can restrict usage by creating separate access tokens and set limits. 1. Access Tokens \u00b6 If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization. Assuming you are still within the Organization Overview section, simply select the Access Tokens tab from the left pane. However to get to this section from anywhere click on Settings (icon at the top right top of the page) and select Organizations Settings \u2192 Access tokens The Access Tokens Interface provides an overview of your Allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured. Each Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume. The Usage Status Column quickly shows if a token is above or below its assigned limits. 1.1 Creating a new token \u00b6 Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog. Enter the new name of the new Token by using your Initials e.g. RWC-Token After you press Ok, you will be taken back to the Access Token UI, here your new token should be present, among the ones created by others. If you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis ( ... ) menu button behind a token limit to open the manage token menu. If you made a typo you can use the Rename Token option to correct the name of your token. 1.2 Disabling a token \u00b6 If you need to make sure a token cannot be used to send Metrics in you can Disable a token. Click on the Disable button to Disable the token, this means the token cannot be used for sending in data to SignalFX. The line with Your Token should become greyed out to indicate that is has been Disabled as you can see in the screenshot below. Go ahead and click on the ellipsis ( ... ) menu button to Disable and Enable your token. 1.3 Manage token usage limits \u00b6 Now Lets start limiting usage by clicking on Manage Token Limit in the 3 ... menu. This will show the Manage Token Limit Dialog: In this Dialog you can set the limits per category. Please go ahead and specify the limits as follows for each usage metric: Limit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above. Token limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes. To specify the recipients, click Add Recipient , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended). The severity for token alerts is always Critical. Click on Update to save your Access Tokens limits and The Alert Settings. Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by SignalFx. This will make sure you there will be no unexpected cost due to a team sending in data without restriction. Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved. In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to SignalFx. This will allow you to fine tune the way you consume your SignalFx allotment and prevent overages from happening. Congratulations! You have now have completed the Service Bureau module.","title":"Control Usage"},{"location":"servicebureau/tokens/#controlling-usage-lab-summary","text":"Discover how you can restrict usage by creating separate access tokens and set limits.","title":"Controlling Usage - Lab Summary"},{"location":"servicebureau/tokens/#1-access-tokens","text":"If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization. Assuming you are still within the Organization Overview section, simply select the Access Tokens tab from the left pane. However to get to this section from anywhere click on Settings (icon at the top right top of the page) and select Organizations Settings \u2192 Access tokens The Access Tokens Interface provides an overview of your Allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured. Each Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume. The Usage Status Column quickly shows if a token is above or below its assigned limits.","title":"1. Access Tokens"},{"location":"servicebureau/tokens/#11-creating-a-new-token","text":"Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog. Enter the new name of the new Token by using your Initials e.g. RWC-Token After you press Ok, you will be taken back to the Access Token UI, here your new token should be present, among the ones created by others. If you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis ( ... ) menu button behind a token limit to open the manage token menu. If you made a typo you can use the Rename Token option to correct the name of your token.","title":"1.1 Creating a new token"},{"location":"servicebureau/tokens/#12-disabling-a-token","text":"If you need to make sure a token cannot be used to send Metrics in you can Disable a token. Click on the Disable button to Disable the token, this means the token cannot be used for sending in data to SignalFX. The line with Your Token should become greyed out to indicate that is has been Disabled as you can see in the screenshot below. Go ahead and click on the ellipsis ( ... ) menu button to Disable and Enable your token.","title":"1.2 Disabling a token"},{"location":"servicebureau/tokens/#13-manage-token-usage-limits","text":"Now Lets start limiting usage by clicking on Manage Token Limit in the 3 ... menu. This will show the Manage Token Limit Dialog: In this Dialog you can set the limits per category. Please go ahead and specify the limits as follows for each usage metric: Limit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above. Token limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes. To specify the recipients, click Add Recipient , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended). The severity for token alerts is always Critical. Click on Update to save your Access Tokens limits and The Alert Settings. Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by SignalFx. This will make sure you there will be no unexpected cost due to a team sending in data without restriction. Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved. In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to SignalFx. This will allow you to fine tune the way you consume your SignalFx allotment and prevent overages from happening. Congratulations! You have now have completed the Service Bureau module.","title":"1.3 Manage token usage limits"},{"location":"smartagent/","text":"Introduction \u00b6 During this technical workshop you will build out an environment based on a lightweight Kubernetes 1 deployment. In order to simplify the workshop modules, a pre-configured AWS/EC2 instance is provided. The instance is pre-configured with all the software required to install the Smart Agent 2 in Kubernetes, deploy a NGINX 3 ReplicaSet 4 and finally deploy a microservices application which has been instrumented to send Traces and Spans using Jaeger 5 . The workshop also introduces you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code 6 and the Service Bureau 6 By the end of this technical workshop you will have a good understanding of some of the key features and capabilities of the Observability Suite. Here are the instructions how to access you pre-configured AWS/EC2 instance Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. \u21a9 The SignalFx Smart Agent gathers host performance, application, and service-level metrics from both containerized and non-container environments. The Smart Agent installs with more than 100 bundled monitors for gathering data, including Python-based plug-ins such as Mongo, Redis, and Docker. \u21a9 NGINX is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. \u21a9 Kubernetes ReplicaSet \u21a9 Jaeger, inspired by Dapper and OpenZipkin, is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems \u21a9 Monitoring as Code and Service Bureau \u21a9 \u21a9","title":"Workshop Introduction"},{"location":"smartagent/#introduction","text":"During this technical workshop you will build out an environment based on a lightweight Kubernetes 1 deployment. In order to simplify the workshop modules, a pre-configured AWS/EC2 instance is provided. The instance is pre-configured with all the software required to install the Smart Agent 2 in Kubernetes, deploy a NGINX 3 ReplicaSet 4 and finally deploy a microservices application which has been instrumented to send Traces and Spans using Jaeger 5 . The workshop also introduces you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code 6 and the Service Bureau 6 By the end of this technical workshop you will have a good understanding of some of the key features and capabilities of the Observability Suite. Here are the instructions how to access you pre-configured AWS/EC2 instance Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. \u21a9 The SignalFx Smart Agent gathers host performance, application, and service-level metrics from both containerized and non-container environments. The Smart Agent installs with more than 100 bundled monitors for gathering data, including Python-based plug-ins such as Mongo, Redis, and Docker. \u21a9 NGINX is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. \u21a9 Kubernetes ReplicaSet \u21a9 Jaeger, inspired by Dapper and OpenZipkin, is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems \u21a9 Monitoring as Code and Service Bureau \u21a9 \u21a9","title":"Introduction"},{"location":"smartagent/connect-info/","text":"How to connect to your workshop environment \u00b6 How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty 1 or your web browser. Verify your connection to your AWS/EC2 cloud instance. Find your AWS/EC2 IP Address \u00b6 In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2. To get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader. Search for your AWS/EC2 instance by entering your first name, as provided during registration for this workshop. The search result will provide you with the IP address, the SSH command (for Mac OS and Linux) and the password to use to connect to the workshop instance. Important Please make a note of the IP address as you will need this during the workshop. SSH (Mac OS/Linux) \u00b6 Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the Google Sheet from Step #1. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue and start the workshop Putty (Windows users only) \u00b6 If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and enter the in Host Name (or IP address) field the IP address provided in the Google Sheet. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue and start the workshop Web Browser (All) \u00b6 If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the Google Sheet). Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: Copy & Paste in browser \u00b6 Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop ask you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue and start the workshop Multipass (final option) \u00b6 If you are unable to access AWS, but you can install software locally, follow the instructions for using Multipass . Download Putty \u21a9","title":"Access AWS/EC2 instance"},{"location":"smartagent/connect-info/#how-to-connect-to-your-workshop-environment","text":"How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty 1 or your web browser. Verify your connection to your AWS/EC2 cloud instance.","title":"How to connect to your workshop environment"},{"location":"smartagent/connect-info/#find-your-awsec2-ip-address","text":"In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2. To get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader. Search for your AWS/EC2 instance by entering your first name, as provided during registration for this workshop. The search result will provide you with the IP address, the SSH command (for Mac OS and Linux) and the password to use to connect to the workshop instance. Important Please make a note of the IP address as you will need this during the workshop.","title":"Find your AWS/EC2 IP Address"},{"location":"smartagent/connect-info/#ssh-mac-oslinux","text":"Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device. To use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1). When prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes . Enter the password provided in the Google Sheet from Step #1. Upon successful login you will be presented with the Splunk logo and the Linux prompt. At this point you are ready to continue and start the workshop","title":"SSH (Mac OS/Linux)"},{"location":"smartagent/connect-info/#putty-windows-users-only","text":"If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find here . Important If you cannot install Putty, please go to Web Browser (All) . Open Putty and enter the in Host Name (or IP address) field the IP address provided in the Google Sheet. You can optionally save your settings by providing a name and pressing Save . To then login to your instance click on the Open button as shown above. If this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialog, please click Yes . Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below: At this point you are ready to continue and start the workshop","title":"Putty (Windows users only)"},{"location":"smartagent/connect-info/#web-browser-all","text":"If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser. Note This assumes that access to port 6501 is not restricted by your company's firewall. Open your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the Google Sheet). Once connected, login in as ubuntu and the password is the one provided in the Google Sheet. Once you are connected successfully you should see a screen similar to the one below:","title":"Web Browser (All)"},{"location":"smartagent/connect-info/#copy-paste-in-browser","text":"Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions. When the workshop ask you to copy instructions into your terminal, please do the following: Copy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below: This will open a dialog box asking for the text to be pasted into the web terminal: Paste the text in the text box as show, then press OK to complete the copy and paste process. Note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal. Simply click the Connect button and you will be reconnected and will be able to continue. At this point you are ready to continue and start the workshop","title":"Copy &amp; Paste in browser"},{"location":"smartagent/connect-info/#multipass-final-option","text":"If you are unable to access AWS, but you can install software locally, follow the instructions for using Multipass . Download Putty \u21a9","title":"Multipass (final option)"},{"location":"smartagent/k3s/","text":"Deploying the Smart Agent in Kubernetes (K3s) \u00b6 Use the SignalFx Helm chart to install the Smart Agent in K3s Explore your cluster in the Kubernetes Navigator 1. Obtain SignalFx Access Token \u00b6 You will need to obtain your Access Token 1 from the SignalFx UI once Kubernetes is running. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy to clipboard. You will also need to obtain the name of the Realm 2 for your SignalFx account. Click on the profile icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us1 . 2. Use Helm to deploy agent \u00b6 Create the following variables to use in the proceeding helm install command, replacing the highlighted VARIABLE with the appropriate values. For instance, if your realm is us1 , you would run export REALM=us1 and for eu0 run export REALM=eu0 . Shell Command export ACCESS_TOKEN= Replace with your ACCESS_TOKEN export REALM= Replace with your REALM Install the agent using the SignalFx Helm chart. Firstly, add the SignalFx Helm chart repository to Helm. Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo && helm repo update Install the Smart Agent Helm chart with the following commands, do NOT edit this: Shell Command helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$(hostname)-k3s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml You can monitor the progress of the deployment by running sudo kubectl get pods which should typically report a new pod is up and running after about 30 seconds. Ensure the status is reported as Running before continuing. Info If you make an error installing the Smart Agent you can start over by deleting the erroneous install using: helm delete signalfx-agent Shell Command sudo kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-66tvr 1/1 Running 0 7s Ensure there are no errors by tailing the logs from the Smart Agent Pod. Output should look similar to the log output shown in the Output tab below. Use the label set by the helm install to tail logs (You will need to press Ctrl + C to exit). Or use the installed k9s terminal UI for bonus points! Shell Command sudo kubectl logs -l app=signalfx-agent -f Output signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Starting up agent version 5.2.1\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Watching for config file changes\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"New config loaded\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using log level info\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Fetching host id dimensions\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Trying to get fully qualified hostname\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using hostname sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Using host id dimensions map[host:sedj kubernetes_node_uid:ea3bf9ff-3f04-4485-9702-6e7097b261dd]\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending datapoints to https://ingest.us0.signalfx.com/v2/datapoint\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending events to https://ingest.us0.signalfx.com/v2/event\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending trace spans to https://ingest.us0.signalfx.com/v2/trace\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Setting cluster:sedj-k3s-cluster property on host:sedj dimension\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=1 monitorType=cpu \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=2 monitorType=filesystems \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=3 monitorType=disk-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=4 monitorType=net-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=5 monitorType=load \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=6 monitorType=memory \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=7 monitorType=host-metadata \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=8 monitorType=processlist \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=9 monitorType=vmem \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=10 monitorType=kubelet-stats \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=11 monitorType=kubernetes-cluster \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=12 monitorType=signalfx-forwarder \u2502 signalfx-agent I0527 20:52:12.796150 1 leaderelection.go:242] attempting to acquire leader lease default/signalfx-agent-leader... \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=13 monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Done configuring agent\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Serving internal metrics at localhost:8095\" \u2502 signalfx-agent I0527 20:52:12.813288 1 leaderelection.go:252] successfully acquired lease default/signalfx-agent-leader \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"K8s leader is now node sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"This instance is now the leader and will send events\" monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Starting K8s API resource sync\" \u2502 3. Validate metrics in the UI \u00b6 In the SignalFx UI, goto Infrastructure \u2192 Kubernetes Navigator \u2192 Cluster Map and open the Kubernetes Navigator Cluster Map to ensure metrics are being sent. Validate that your cluster is discovered and reporting by finding your cluster (in the workshop you will see many other clusters). To find your cluster name run the following command and copy the output to your clipboard: Shell Command echo $(hostname)-k3s-cluster To examine the health of your node, first click on the blue cross on your cluster. This will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar. Once it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc. Various instructions in this Workshop include a REALM placeholder that will need to be replaced with the actual name of your realm. This realm name is shown on your profile page in SignalFx. If you do not include the realm name when specifying an endpoint, SignalFx will interpret it as pointing to the us0 realm. Access Tokens (sometimes called Org Tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the SignalFx API. \u21a9 A realm is a self-contained deployment of SignalFx in which your Organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). \u21a9","title":"Deploy the Smart Agent in K3s"},{"location":"smartagent/k3s/#deploying-the-smart-agent-in-kubernetes-k3s","text":"Use the SignalFx Helm chart to install the Smart Agent in K3s Explore your cluster in the Kubernetes Navigator","title":"Deploying the Smart Agent in Kubernetes (K3s)"},{"location":"smartagent/k3s/#1-obtain-signalfx-access-token","text":"You will need to obtain your Access Token 1 from the SignalFx UI once Kubernetes is running. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy to clipboard. You will also need to obtain the name of the Realm 2 for your SignalFx account. Click on the profile icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us1 .","title":"1. Obtain SignalFx Access Token"},{"location":"smartagent/k3s/#2-use-helm-to-deploy-agent","text":"Create the following variables to use in the proceeding helm install command, replacing the highlighted VARIABLE with the appropriate values. For instance, if your realm is us1 , you would run export REALM=us1 and for eu0 run export REALM=eu0 . Shell Command export ACCESS_TOKEN= Replace with your ACCESS_TOKEN export REALM= Replace with your REALM Install the agent using the SignalFx Helm chart. Firstly, add the SignalFx Helm chart repository to Helm. Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo && helm repo update Install the Smart Agent Helm chart with the following commands, do NOT edit this: Shell Command helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$(hostname)-k3s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml You can monitor the progress of the deployment by running sudo kubectl get pods which should typically report a new pod is up and running after about 30 seconds. Ensure the status is reported as Running before continuing. Info If you make an error installing the Smart Agent you can start over by deleting the erroneous install using: helm delete signalfx-agent Shell Command sudo kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-66tvr 1/1 Running 0 7s Ensure there are no errors by tailing the logs from the Smart Agent Pod. Output should look similar to the log output shown in the Output tab below. Use the label set by the helm install to tail logs (You will need to press Ctrl + C to exit). Or use the installed k9s terminal UI for bonus points! Shell Command sudo kubectl logs -l app=signalfx-agent -f Output signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Starting up agent version 5.2.1\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Watching for config file changes\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"New config loaded\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using log level info\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Fetching host id dimensions\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Trying to get fully qualified hostname\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using hostname sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Using host id dimensions map[host:sedj kubernetes_node_uid:ea3bf9ff-3f04-4485-9702-6e7097b261dd]\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending datapoints to https://ingest.us0.signalfx.com/v2/datapoint\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending events to https://ingest.us0.signalfx.com/v2/event\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending trace spans to https://ingest.us0.signalfx.com/v2/trace\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Setting cluster:sedj-k3s-cluster property on host:sedj dimension\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=1 monitorType=cpu \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=2 monitorType=filesystems \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=3 monitorType=disk-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=4 monitorType=net-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=5 monitorType=load \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=6 monitorType=memory \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=7 monitorType=host-metadata \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=8 monitorType=processlist \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=9 monitorType=vmem \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=10 monitorType=kubelet-stats \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=11 monitorType=kubernetes-cluster \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=12 monitorType=signalfx-forwarder \u2502 signalfx-agent I0527 20:52:12.796150 1 leaderelection.go:242] attempting to acquire leader lease default/signalfx-agent-leader... \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=13 monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Done configuring agent\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Serving internal metrics at localhost:8095\" \u2502 signalfx-agent I0527 20:52:12.813288 1 leaderelection.go:252] successfully acquired lease default/signalfx-agent-leader \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"K8s leader is now node sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"This instance is now the leader and will send events\" monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Starting K8s API resource sync\" \u2502","title":"2. Use Helm to deploy agent"},{"location":"smartagent/k3s/#3-validate-metrics-in-the-ui","text":"In the SignalFx UI, goto Infrastructure \u2192 Kubernetes Navigator \u2192 Cluster Map and open the Kubernetes Navigator Cluster Map to ensure metrics are being sent. Validate that your cluster is discovered and reporting by finding your cluster (in the workshop you will see many other clusters). To find your cluster name run the following command and copy the output to your clipboard: Shell Command echo $(hostname)-k3s-cluster To examine the health of your node, first click on the blue cross on your cluster. This will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar. Once it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc. Various instructions in this Workshop include a REALM placeholder that will need to be replaced with the actual name of your realm. This realm name is shown on your profile page in SignalFx. If you do not include the realm name when specifying an endpoint, SignalFx will interpret it as pointing to the us0 realm. Access Tokens (sometimes called Org Tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the SignalFx API. \u21a9 A realm is a self-contained deployment of SignalFx in which your Organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). \u21a9","title":"3. Validate metrics in the UI"},{"location":"smartagent/multipass/","text":"Launch a Multipass instance \u00b6 1. Pre-requisites \u00b6 Install Multipass 1 for your operating system. Make sure you are using at least version 1.5.0 . On a Mac you can also install via Homebrew e.g. brew cask install multipass 2. Download cloud-init YAML \u00b6 Linux/Mac OS WSVERSION=1.57 mkdir cloud-init curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/k3s.yaml \\ -o cloud-init/k3s.yaml export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) Windows Info Download the zip by clicking on the following URL https://github.com/signalfx/observability-workshop/archive/v1.57.zip . Once downloaded, unzip the the file and rename it to workshop . Then, from the command prompt change into that directory and run $INSTANCE = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\".tochararray() | sort {Get-Random})[0..3] -join '' 3. Launch Multipass instance \u00b6 In this section you will build and launch the Multipass instance which will run the Kubernetes (K3s) environment that you will use in multiple labs. Shell Command multipass launch \\ --name ${INSTANCE} \\ --cloud-init cloud-init/k3s.yaml Once the instance has been successfully created (this can take several minutes), shell into it. Shell Command multipass shell ${INSTANCE} Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@vmpe-k3s:~$ Once your instance presents you with the Splunk logo, you have completed the preparation for your Multipass instance and can go directly to the next lab Deploy the Smart Agent in K3s . Multipass is a lightweight VM manager for Linux, Windows and macOS. It's designed for developers who want a fresh Ubuntu environment with a single command. It uses KVM on Linux, Hyper-V on Windows and HyperKit on macOS to run the VM with minimal overhead. It can also use VirtualBox on Windows and macOS. Multipass will fetch images for you and keep them up to date. \u21a9","title":"Launch a Multipass instance"},{"location":"smartagent/multipass/#launch-a-multipass-instance","text":"","title":"Launch a Multipass instance"},{"location":"smartagent/multipass/#1-pre-requisites","text":"Install Multipass 1 for your operating system. Make sure you are using at least version 1.5.0 . On a Mac you can also install via Homebrew e.g. brew cask install multipass","title":"1. Pre-requisites"},{"location":"smartagent/multipass/#2-download-cloud-init-yaml","text":"Linux/Mac OS WSVERSION=1.57 mkdir cloud-init curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/cloud-init/k3s.yaml \\ -o cloud-init/k3s.yaml export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) Windows Info Download the zip by clicking on the following URL https://github.com/signalfx/observability-workshop/archive/v1.57.zip . Once downloaded, unzip the the file and rename it to workshop . Then, from the command prompt change into that directory and run $INSTANCE = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\".tochararray() | sort {Get-Random})[0..3] -join ''","title":"2. Download cloud-init YAML"},{"location":"smartagent/multipass/#3-launch-multipass-instance","text":"In this section you will build and launch the Multipass instance which will run the Kubernetes (K3s) environment that you will use in multiple labs. Shell Command multipass launch \\ --name ${INSTANCE} \\ --cloud-init cloud-init/k3s.yaml Once the instance has been successfully created (this can take several minutes), shell into it. Shell Command multipass shell ${INSTANCE} Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@vmpe-k3s:~$ Once your instance presents you with the Splunk logo, you have completed the preparation for your Multipass instance and can go directly to the next lab Deploy the Smart Agent in K3s . Multipass is a lightweight VM manager for Linux, Windows and macOS. It's designed for developers who want a fresh Ubuntu environment with a single command. It uses KVM on Linux, Hyper-V on Windows and HyperKit on macOS to run the VM with minimal overhead. It can also use VirtualBox on Windows and macOS. Multipass will fetch images for you and keep them up to date. \u21a9","title":"3. Launch Multipass instance"},{"location":"smartagent/nginx/","text":"Deploying NGINX in K3s - Lab Summary \u00b6 Deploy a NGINX ReplicaSet into your K3s cluster and confirm the auto discovery of your NGINX deployment. Run a benchmark test to create metrics and confirm them streaming into SignalFX! 1. Start your NGINX \u00b6 This deployment of NGINX has been configured to use Kubernetes pod annotations to tell the Smart Agent how to monitor the service. This is achieved by defining the port and monitor type to use for monitoring the NGINX service e.g. agent.signalfx.com/monitorType.80: \"collectd/nginx\" Verify the number of pods running in the SignalFx UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster. Note the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node! Now switch back to the default cluster node view by selecting the MAP tab and select your cluster again. In the Multipass or AWS/EC2 shell session and change into the nginx directory: Shell Command cd ~/workshop/k3s/nginx 2. Create NGINX deployment \u00b6 Create the NGINX configmap 1 using the nginx.conf file: Shell Command sudo kubectl create configmap nginxconfig --from-file=nginx.conf Output configmap/nginxconfig created Then create the deployment: Shell Command sudo kubectl create -f nginx-deployment.yaml Output deployment.apps/nginx created service/nginx created Validate the deployment has been successful and that the NGINX pods are running. If you have the SignalFx UI open you should see new Pods being started and containers being deployed. It should only take around 20 seconds for the pods to transition into a Running state. In the SignalFx UI you will have a cluster that looks like below: If you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX: Let's validate this in your shell as well: Shell Command sudo kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-7mljv 1/1 Running 0 87m nginx-7554f6c668-pdjkp 1/1 Running 0 65s nginx-7554f6c668-pddfj 1/1 Running 0 65s nginx-7554f6c668-ggblg 1/1 Running 0 65s nginx-7554f6c668-mjtsh 1/1 Running 0 65s Next create an environment variable containing the CLUSTER_IP of NGINX: Shell Command CLUSTER_IP=$(sudo kubectl get svc nginx -n default -o jsonpath='{.spec.clusterIP}') Confirm the environment variable has been set correctly: Shell Command curl ${CLUSTER_IP} Output <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > < style > body { width : 35 em ; margin : 0 auto ; font-family : Tahoma , Verdana , Arial , sans-serif ; } </ style > </ head > < body > < h1 > Welcome to nginx! </ h1 > < p > If you see this page, the nginx web server is successfully installed and working. Further configuration is required. </ p > < p > For online documentation and support please refer to < a href = \"http://nginx.org/\" > nginx.org </ a > . < br /> Commercial support is available at < a href = \"http://nginx.com/\" > nginx.com </ a > . </ p > < p >< em > Thank you for using nginx. </ em ></ p > </ body > </ html > 3. Run Siege Benchmark \u00b6 Use the Siege 2 Load Testing command to generate some traffic to light up your SignalFx NGINX dashboards. Run this a couple of times! Shell Command siege -b -r 50 -c 20 --no-parser http://${CLUSTER_IP}/ 1>/dev/null Output ** SIEGE 4.0.5 ** Preparing 20 concurrent users for battle. The server is now under siege... Transactions: 1000 hits Availability: 100.00 % Elapsed time: 1.17 secs Data transferred: 20.05 MB Response time: 0.02 secs Transaction rate: 854.70 trans/sec Throughput: 17.14 MB/sec Concurrency: 19.77 Successful transactions: 1000 Failed transactions: 0 Longest transaction: 0.16 Shortest transaction: 0.01 Validate you are seeing metrics in the UI by going to Dashboards \u2192 NGINX \u2192 NGINX Servers . Using the Overrides filter on kubernetes_cluster: , find the name of your cluster as returned by echo $(hostname)-k3s-cluster in the terminal. A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. \u21a9 What is Siege? \u21a9","title":"Deploy NGINX in K3s"},{"location":"smartagent/nginx/#deploying-nginx-in-k3s-lab-summary","text":"Deploy a NGINX ReplicaSet into your K3s cluster and confirm the auto discovery of your NGINX deployment. Run a benchmark test to create metrics and confirm them streaming into SignalFX!","title":"Deploying NGINX in K3s - Lab Summary"},{"location":"smartagent/nginx/#1-start-your-nginx","text":"This deployment of NGINX has been configured to use Kubernetes pod annotations to tell the Smart Agent how to monitor the service. This is achieved by defining the port and monitor type to use for monitoring the NGINX service e.g. agent.signalfx.com/monitorType.80: \"collectd/nginx\" Verify the number of pods running in the SignalFx UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster. Note the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node! Now switch back to the default cluster node view by selecting the MAP tab and select your cluster again. In the Multipass or AWS/EC2 shell session and change into the nginx directory: Shell Command cd ~/workshop/k3s/nginx","title":"1. Start your NGINX"},{"location":"smartagent/nginx/#2-create-nginx-deployment","text":"Create the NGINX configmap 1 using the nginx.conf file: Shell Command sudo kubectl create configmap nginxconfig --from-file=nginx.conf Output configmap/nginxconfig created Then create the deployment: Shell Command sudo kubectl create -f nginx-deployment.yaml Output deployment.apps/nginx created service/nginx created Validate the deployment has been successful and that the NGINX pods are running. If you have the SignalFx UI open you should see new Pods being started and containers being deployed. It should only take around 20 seconds for the pods to transition into a Running state. In the SignalFx UI you will have a cluster that looks like below: If you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX: Let's validate this in your shell as well: Shell Command sudo kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-7mljv 1/1 Running 0 87m nginx-7554f6c668-pdjkp 1/1 Running 0 65s nginx-7554f6c668-pddfj 1/1 Running 0 65s nginx-7554f6c668-ggblg 1/1 Running 0 65s nginx-7554f6c668-mjtsh 1/1 Running 0 65s Next create an environment variable containing the CLUSTER_IP of NGINX: Shell Command CLUSTER_IP=$(sudo kubectl get svc nginx -n default -o jsonpath='{.spec.clusterIP}') Confirm the environment variable has been set correctly: Shell Command curl ${CLUSTER_IP} Output <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > < style > body { width : 35 em ; margin : 0 auto ; font-family : Tahoma , Verdana , Arial , sans-serif ; } </ style > </ head > < body > < h1 > Welcome to nginx! </ h1 > < p > If you see this page, the nginx web server is successfully installed and working. Further configuration is required. </ p > < p > For online documentation and support please refer to < a href = \"http://nginx.org/\" > nginx.org </ a > . < br /> Commercial support is available at < a href = \"http://nginx.com/\" > nginx.com </ a > . </ p > < p >< em > Thank you for using nginx. </ em ></ p > </ body > </ html >","title":"2. Create NGINX deployment"},{"location":"smartagent/nginx/#3-run-siege-benchmark","text":"Use the Siege 2 Load Testing command to generate some traffic to light up your SignalFx NGINX dashboards. Run this a couple of times! Shell Command siege -b -r 50 -c 20 --no-parser http://${CLUSTER_IP}/ 1>/dev/null Output ** SIEGE 4.0.5 ** Preparing 20 concurrent users for battle. The server is now under siege... Transactions: 1000 hits Availability: 100.00 % Elapsed time: 1.17 secs Data transferred: 20.05 MB Response time: 0.02 secs Transaction rate: 854.70 trans/sec Throughput: 17.14 MB/sec Concurrency: 19.77 Successful transactions: 1000 Failed transactions: 0 Longest transaction: 0.16 Shortest transaction: 0.01 Validate you are seeing metrics in the UI by going to Dashboards \u2192 NGINX \u2192 NGINX Servers . Using the Overrides filter on kubernetes_cluster: , find the name of your cluster as returned by echo $(hostname)-k3s-cluster in the terminal. A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. \u21a9 What is Siege? \u21a9","title":"3. Run Siege Benchmark"}]}